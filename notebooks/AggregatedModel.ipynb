{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8404124,"sourceType":"datasetVersion","datasetId":5000806}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Utils ","metadata":{}},{"cell_type":"code","source":"!pip install editdistance torchmetrics pytorch_lightning","metadata":{"execution":{"iopub.status.busy":"2024-05-13T20:39:41.667491Z","iopub.execute_input":"2024-05-13T20:39:41.668132Z","iopub.status.idle":"2024-05-13T20:39:56.620016Z","shell.execute_reply.started":"2024-05-13T20:39:41.668090Z","shell.execute_reply":"2024-05-13T20:39:56.618731Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting editdistance\n  Downloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.66.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (6.0.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.0)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.9.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\nDownloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: editdistance\nSuccessfully installed editdistance-0.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nimport math\nfrom random import random\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport albumentations as A\nimport cv2 as cv\nfrom dataclasses import dataclass, field\nfrom functools import partial\nfrom random import randint\nimport html\nimport random\nfrom pathlib import Path\nfrom typing import Union, Tuple, Dict, Sequence, Optional, List, Any, Callable, Optional\nimport pandas as pd\nfrom torch import Tensor, nn\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchmetrics import Metric\nimport torch\nfrom torchvision import models\nfrom torch.utils.data import Dataset\nimport editdistance\nimport wandb\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.optim import Optimizer\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nimport concurrent\nfrom pytorch_lightning.callbacks import TQDMProgressBar\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T20:39:56.622339Z","iopub.execute_input":"2024-05-13T20:39:56.622694Z","iopub.status.idle":"2024-05-13T20:40:07.897909Z","shell.execute_reply.started":"2024-05-13T20:39:56.622664Z","shell.execute_reply":"2024-05-13T20:40:07.896982Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class LabelParser:\n    def __init__(self):\n        self.classes = None\n        self.vocab_size = None\n        self.class_to_idx = None\n        self.idx_to_class = None\n\n    def fit(self, classes: Sequence[str]):\n        self.classes = list(classes)\n        self.vocab_size = len(classes)\n        self.idx_to_class = dict(enumerate(classes))\n        self.class_to_idx = {cls: i for i, cls in self.idx_to_class.items()}\n\n        return self\n\n    def addClasses(self, classes: List[str]):\n        all_classes = sorted(set(self.classes + classes))\n\n        self.fit(all_classes)\n\n    def encode_labels(self, sequence: Sequence[str]):\n        self._check_fitted()\n        return [self.class_to_idx[c] for c in sequence]\n\n    def decode_labels(self, sequence: Sequence[int]):\n        self._check_fitted()\n        return [self.idx_to_class[c] for c in sequence]\n\n    def _check_fitted(self):\n        if self.classes is None:\n            raise ValueError(\"LabelParser class was not fitted yet\")\n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-04-17T18:00:09.266332Z","start_time":"2024-04-17T18:00:09.262338Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-13T20:40:07.899204Z","iopub.execute_input":"2024-05-13T20:40:07.899536Z","iopub.status.idle":"2024-05-13T20:40:07.909765Z","shell.execute_reply.started":"2024-05-13T20:40:07.899510Z","shell.execute_reply":"2024-05-13T20:40:07.908631Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def pickle_load(file) -> Any:\n    with open(file, \"rb\") as f:\n        return pickle.load(f)\n\ndef pickle_save(obj, file):\n    with open(file, \"wb\") as f:\n        pickle.dump(obj, f)\n\ndef read_xml(file: Union[Path, str]) -> ET.Element:\n    tree = ET.parse(file)\n    root = tree.getroot()\n\n    return root\n\ndef find_child_by_tag(element: ET.Element, tag: str, value: str) -> Union[ET.Element, None]:\n    for child in element:\n        if child.get(tag) == value:\n            return child\n    return None\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef dpi_adjusting(img: np.ndarray, scale: float, **kwargs) -> np.ndarray:\n    height, width = img.shape[:2]\n    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n    return cv.resize(img, (new_width, new_height))\n\nclass LitProgressBar(TQDMProgressBar):\n    def get_metrics(self, trainer, model):\n        # don't show the version number\n        items = super().get_metrics(trainer, model)\n        for k in list(items.keys()):\n            if k.startswith(\"grad\"):\n                items.pop(k, None)\n        items.pop(\"v_num\", None)\n        return items\n    \ndef decode_prediction_and_target(\n    pred: Tensor, target: Tensor, label_encoder: LabelParser, eos_tkn_idx: int\n) -> Tuple[str, str]:\n    # Find padding and <EOS> positions in predictions and targets.\n    eos_idx_pred = (pred == eos_tkn_idx).float().argmax().item()\n    eos_idx_tgt = (target == eos_tkn_idx).float().argmax().item()\n\n    # Decode prediction and target.\n    p, t = pred.tolist(), target.tolist()\n    p = p[1:]  # skip the initial <SOS> token, which is added by default\n    p = p[:eos_idx_pred] if eos_idx_pred != 0 else p\n    t = t[:eos_idx_tgt] if eos_idx_tgt != 0 else t\n    pred_str = \"\".join(label_encoder.decode_labels(p))\n    target_str = \"\".join(label_encoder.decode_labels(t))\n    return pred_str, target_str\n\ndef matplotlib_imshow(\n    img: torch.Tensor, mean: float = 0.5, std: float = 0.5, one_channel=True\n):\n    assert img.device.type == \"cpu\"\n    if one_channel and img.ndim == 3:\n        img = img.mean(dim=0)\n    img = img * std + mean  # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-04-17T18:00:09.235185Z","start_time":"2024-04-17T18:00:09.229244Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-13T20:40:07.912994Z","iopub.execute_input":"2024-05-13T20:40:07.913455Z","iopub.status.idle":"2024-05-13T20:40:07.932041Z","shell.execute_reply.started":"2024-05-13T20:40:07.913419Z","shell.execute_reply":"2024-05-13T20:40:07.931066Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Image transformations","metadata":{}},{"cell_type":"code","source":"\n\nclass SafeRandomScale(A.RandomScale):\n    def apply(self, img, scale=0, interpolation=cv.INTER_LINEAR, **params):\n        height, width = img.shape[:2]\n        new_height, new_width = int(height * scale), int(width * scale)\n        if new_height <= 0 or new_width <= 0:\n            return img\n        return super().apply(img, scale, interpolation, **params)\n\ndef adjust_dpi(img: np.ndarray, scale: float, **kwargs):\n    height, width = img.shape\n    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n    return cv.resize(img, (new_width, new_height))\n\ndef randomly_displace_and_pad(\n    img: np.ndarray,\n    padded_size: Tuple[int, int],\n    crop_if_necessary: bool = False,\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"\n    Randomly displace an image within a frame, and pad zeros around the image.\n\n    Args:\n        img (np.ndarray): image to process\n        padded_size (Tuple[int, int]): (height, width) tuple indicating the size of the frame\n        crop_if_necessary (bool): whether to crop the image if its size exceeds that\n            of the frame\n    \"\"\"\n    frame_h, frame_w = padded_size\n    img_h, img_w = img.shape\n    if frame_h < img_h or frame_w < img_w:\n        if crop_if_necessary:\n            print(\n                \"WARNING (`randomly_displace_and_pad`): cropping input image before \"\n                \"padding because it exceeds the size of the frame.\"\n            )\n            img_h, img_w = min(img_h, frame_h), min(img_w, frame_w)\n            img = img[:img_h, :img_w]\n        else:\n            raise AssertionError(\n                f\"Frame is smaller than the image: ({frame_h}, {frame_w}) vs. ({img_h},\"\n                f\" {img_w})\"\n            )\n\n    res = np.zeros((frame_h, frame_w), dtype=img.dtype)\n\n    pad_top =  randint(0, frame_h - img_h)\n    pad_bottom = pad_top + img_h\n    pad_left = randint(0, frame_w - img_w)\n    pad_right = pad_left + img_w\n\n    res[pad_top:pad_bottom, pad_left:pad_right] = img\n    return res\n\n@dataclass\nclass ImageTransforms:\n    max_img_size: Tuple[int, int]  # (h, w)\n    normalize_params: Tuple[float, float]  # (mean, std)\n    scale: float = (\n        0.5\n    )\n    random_scale_limit: float = 0.1\n    random_rotate_limit: int = 10\n\n    train_trnsf: A.Compose = field(init=False)\n    test_trnsf: A.Compose = field(init=False)\n\n    def __post_init__(self):\n        scale, random_scale_limit, random_rotate_limit, normalize_params =(\n            self.scale,\n            self.random_scale_limit,\n            self.random_rotate_limit,\n            self.normalize_params\n        )\n\n        max_img_h, max_img_w = self.max_img_size\n        max_scale = scale + scale * random_scale_limit\n        padded_h, padded_w = math.ceil(max_scale * max_img_h), math.ceil(max_scale * max_img_w)\n\n        self.train_trnsf = A.Compose([\n            A.Lambda(partial(adjust_dpi, scale=scale)),\n            SafeRandomScale(scale_limit=random_scale_limit, p=0.5),\n            A.SafeRotate(\n                limit = random_rotate_limit,\n                border_mode = cv.BORDER_CONSTANT,\n                value = 0\n            ),\n            A.RandomBrightnessContrast(),\n            A.Perspective(scale=(0.01, 0.05)),\n            A.GaussNoise(),\n            A.Normalize(*normalize_params),\n            A.Lambda(\n                image=partial(\n                    randomly_displace_and_pad,\n                    padded_size=(padded_h, padded_w),\n                    crop_if_necessary=False,\n                )\n            )\n        ])\n\n        self.test_trnsf = A.Compose([\n            A.Lambda(partial(adjust_dpi, scale=scale)),\n            A.Normalize(*normalize_params),\n            A.PadIfNeeded(\n                max_img_h, max_img_w, border_mode=cv.BORDER_CONSTANT, value=0\n            )\n        ])\n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-04-17T18:00:09.286749Z","start_time":"2024-04-17T18:00:09.276042Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-13T20:40:07.933338Z","iopub.execute_input":"2024-05-13T20:40:07.933618Z","iopub.status.idle":"2024-05-13T20:40:07.957058Z","shell.execute_reply.started":"2024-05-13T20:40:07.933595Z","shell.execute_reply":"2024-05-13T20:40:07.956112Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## IAM Dataset and Synthetic Dataset","metadata":{}},{"cell_type":"code","source":"\nclass IAMDataset(Dataset):\n    MEAN = 0.8275\n    STD = 0.2314\n    MAX_FORM_HEIGHT = 3542\n    MAX_FORM_WIDTH = 2479\n\n    MAX_SEQ_LENS = {\n        \"word\": 55,\n        \"line\": 90,\n        \"form\": 700,\n    }  # based on the maximum seq lengths found in the dataset\n\n    _pad_token = \"<PAD>\"\n    _sos_token = \"<SOS>\"\n    _eos_token = \"<EOS>\"\n\n    root: Path\n    data: pd.DataFrame\n    label_enc: LabelParser\n    parse_method: str\n    only_lowercase: bool\n    transforms: Optional[A.Compose]\n    id_to_idx: Dict[str, int]\n    _split: str\n    _return_writer_id: Optional[bool]\n\n    def __init__(\n        self,\n        root: Union[Path, str],\n        parse_method: str,\n        split: str,\n        return_writer_id: bool = False,\n        only_lowercase: bool = False,\n        label_enc: Optional[LabelParser] = None,\n    ):\n        super().__init__()\n        _parse_methods = [\"form\", \"line\", \"word\"]\n        err_message = (\n            f\"{parse_method} is not a possible parsing method: {_parse_methods}\"\n        )\n        assert parse_method in _parse_methods, err_message\n\n        _splits = [\"train\", \"test\", \"val\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n\n        self._split = split\n        self._return_writer_id = return_writer_id\n        self.only_lowercase = only_lowercase\n        self.root = Path(root)\n        self.label_enc = label_enc\n        self.parse_method = parse_method\n\n        # Process the data.\n        if not hasattr(self, \"data\"):\n            if self.parse_method == \"form\":\n                self.data = self._get_forms()\n            elif self.parse_method == \"word\":\n                self.data = self._get_words(skip_bad_segmentation=True)\n            elif self.parse_method == \"line\":\n                self.data = self._get_lines()\n\n        # Create the label encoder.\n        if self.label_enc is None:\n            vocab = [self._pad_token, self._sos_token, self._eos_token]\n            s = \"\".join(self.data[\"target\"].tolist())\n            if self.only_lowercase:\n                s = s.lower()\n            vocab += sorted(list(set(s)))\n            self.label_enc = LabelParser().fit(vocab)\n        if not \"target_enc\" in self.data.columns:\n            self.data.insert(\n                2,\n                \"target_enc\",\n                self.data[\"target\"].apply(\n                    lambda s: np.array(\n                        self.label_enc.encode_labels(\n                            [c for c in (s.lower() if self.only_lowercase else s)]\n                        )\n                    )\n                ),\n            )\n\n        self.transforms = self._get_transforms(split)\n        self.id_to_idx = {\n            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n        }\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n            # Crop the image vertically.\n            img = img[data[\"bb_y_start\"] : data[\"bb_y_end\"], :]\n        assert isinstance(img, np.ndarray), (\n            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n            f\"Is there something wrong with this image?\"\n        )\n        if self.transforms is not None:\n            img = self.transforms(image=img)[\"image\"]\n        if self._return_writer_id:\n            return img, data[\"writer_id\"], data[\"target_enc\"]\n        \n        if img is None :\n            raise ValueError(\"Image is None in IAM.\")\n        if data[\"target_enc\"] is None:\n            raise ValueError(\"Image is None in IAM.\")\n            \n        return img, data[\"target_enc\"]\n\n    def get_max_height(self):\n        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n\n    @property\n    def vocab(self):\n        return self.label_enc.classes\n\n    @staticmethod\n    def collate_fn(\n        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n        pad_val: int,\n        eos_tkn_idx: int,\n        dataset_returns_writer_id: bool = False,\n    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n        if dataset_returns_writer_id:\n            imgs, writer_ids, targets = zip(*batch)\n        else:\n            imgs, targets = zip(*batch)\n\n        img_sizes = [im.shape for im in imgs]\n        if (\n            not len(set(img_sizes)) == 1\n        ):  # images are of varying sizes, so pad them to the maximum size in the batch\n            hs, ws = zip(*img_sizes)\n            pad_fn = A.PadIfNeeded(\n                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n            )\n            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n        imgs = np.stack(imgs, axis=0)\n\n        seq_lengths = [t.shape[0] for t in targets]\n        targets_padded = np.full((len(targets), max(seq_lengths) + 1), pad_val)\n        for i, t in enumerate(targets):\n            targets_padded[i, : seq_lengths[i]] = t\n            targets_padded[i, seq_lengths[i]] = eos_tkn_idx\n\n        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n        if dataset_returns_writer_id:\n            return imgs, targets_padded, torch.tensor(writer_ids)\n        return imgs, targets_padded\n\n    def set_transforms_for_split(self, split: str):\n        _splits = [\"train\", \"val\", \"test\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n        self.transforms = self._get_transforms(split)\n\n    def _get_transforms(self, split: str) -> A.Compose:\n        max_img_w = self.MAX_FORM_WIDTH\n\n        if self.parse_method == \"form\":\n            max_img_h = (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n        else:  # word or line\n            max_img_h = self.MAX_FORM_HEIGHT\n\n        transforms = ImageTransforms(\n            (max_img_h, max_img_w), (IAMDataset.MEAN, IAMDataset.STD)\n        )\n\n        if split == \"train\":\n            return transforms.train_trnsf\n        elif split == \"test\" or split == \"val\":\n            return transforms.test_trnsf\n\n    def statistics(self) -> Dict[str, float]:\n        assert len(self) > 0\n        tmp = self.transforms\n        self.transforms = None\n        mean, std, cnt = 0, 0, 0\n        for img, _ in self:\n            mean += np.mean(img)\n            std += np.var(img)\n            cnt += 1\n        mean /= cnt\n        std = np.sqrt(std / cnt)\n        self.transforms = tmp\n        return {\"mean\": mean, \"std\": std}\n\n    def _get_forms(self) -> pd.DataFrame:\n        \"\"\"Read all form images from the IAM dataset.\n\n        Returns:\n            pd.DataFrame\n                A pandas dataframe containing the image path, image id, target, vertical\n                upper bound, vertical lower bound, and target length.\n        \"\"\"\n        data = {\n            \"img_path\": [],\n            \"img_id\": [],\n            \"target\": [],\n            \"bb_y_start\": [],\n            \"bb_y_end\": [],\n            \"target_len\": [],\n        }\n        for form_dir in [\"formsA-D\", \"formsE-H\", \"formsI-Z\"]:\n            dr = self.root / form_dir\n            for img_path in dr.iterdir():\n                doc_id = img_path.stem\n                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n\n                # Based on some empiricial evaluation, the 'asy' and 'dsy'\n                # attributes of a line xml tag seem to correspond to its upper and\n                # lower bound, respectively. We add padding of 150 pixels.\n                bb_y_start = int(xml_root[1][0].get(\"asy\")) - 150\n                bb_y_end = int(xml_root[1][-1].get(\"dsy\")) + 150\n\n                form_text = []\n                for line in xml_root.iter(\"line\"):\n                    form_text.append(html.unescape(line.get(\"text\", \"\")))\n\n                img_w, img_h = Image.open(str(img_path)).size\n                data[\"img_path\"].append(str(img_path))\n                data[\"img_id\"].append(doc_id)\n                data[\"target\"].append(\"\\n\".join(form_text))\n                data[\"bb_y_start\"].append(bb_y_start)\n                data[\"bb_y_end\"].append(bb_y_end)\n                data[\"target_len\"].append(len(\"\\n\".join(form_text)))\n        return pd.DataFrame(data).sort_values(\n            \"target_len\"\n        )  # by default, sort by target length\n\n    def _get_lines(self, skip_bad_segmentation: bool = False) -> pd.DataFrame:\n        \"\"\"Read all line images from the IAM dataset.\n\n        Args:\n            skip_bad_segmentation (bool): skip lines that have the\n                segmentation='err' xml attribute\n        Returns:\n            List of 2-tuples, where each tuple contains the path to a line image\n            along with its ground truth text.\n        \"\"\"\n        data = {\"img_path\": [], \"img_id\": [], \"target\": []}\n        root = self.root / \"lines\"\n        for d1 in root.iterdir():\n            for d2 in d1.iterdir():\n                doc_id = d2.name\n                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n                for img_path in d2.iterdir():\n                    target = self._find_line(\n                        xml_root, img_path.stem, skip_bad_segmentation\n                    )\n                    if target is not None:\n                        data[\"img_path\"].append(str(img_path.resolve()))\n                        data[\"img_id\"].append(doc_id)\n                        data[\"target\"].append(target)\n        return pd.DataFrame(data)\n\n    def _get_words(self, skip_bad_segmentation: bool = False) -> pd.DataFrame:\n        \"\"\"Read all word images from the IAM dataset.\n\n        Args:\n            skip_bad_segmentation (bool): skip lines that have the\n                segmentation='err' xml attribute\n        Returns:\n            List of 2-tuples, where each tuple contains the path to a word image\n            along with its ground truth text.\n        \"\"\"\n        data = {\"img_path\": [], \"img_id\": [], \"writer_id\": [], \"target\": []}\n        root = self.root / \"words\"\n        parallel_inputs = []\n        for d1 in root.iterdir():\n            if d1.is_file():\n                continue\n            for d2 in d1.iterdir():\n                parallel_inputs.append(d2)\n\n        def process_dir(directory):\n            directory_results = {\"img_path\": [], \"img_id\": [], \"writer_id\": [], \"target\": []}\n            doc_id = directory.name\n            xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n            writer_id = int(xml_root.get(\"writer-id\"))\n            for img_path in directory.iterdir():\n                img = cv.imread(str(img_path.resolve()), cv.IMREAD_GRAYSCALE)\n\n                if isinstance(img, np.ndarray):\n                    target = self._find_word(\n                        xml_root, img_path.stem, skip_bad_segmentation\n                    )\n                    if target is not None:\n                        directory_results[\"img_path\"].append(str(img_path.resolve()))\n                        directory_results[\"img_id\"].append(doc_id)\n                        directory_results[\"writer_id\"].append(writer_id)\n                        directory_results[\"target\"].append(target)\n            return directory_results\n        \n        with ThreadPoolExecutor() as executor:\n            results = list(tqdm(executor.map(process_dir, iter(parallel_inputs)), total=len(parallel_inputs)))\n            for single_results in results:\n                data[\"img_path\"].extend(single_results[\"img_path\"])\n                data[\"img_id\"].extend(single_results[\"img_id\"])\n                data[\"writer_id\"].extend(single_results[\"writer_id\"])\n                data[\"target\"].extend(single_results[\"target\"])\n                \n        return pd.DataFrame(data)\n\n    def _find_line(\n        self,\n        xml_root: ET.Element,\n        line_id: str,\n        skip_bad_segmentation: bool = False,\n    ) -> Union[str, None]:\n        line = find_child_by_tag(xml_root[1].findall(\"line\"), \"id\", line_id)\n        if line is not None and not (\n            skip_bad_segmentation and line.get(\"segmentation\") == \"err\"\n        ):\n            return html.unescape(line.get(\"text\"))\n        return None\n\n    def _find_word(\n        self,\n        xml_root: ET.Element,\n        word_id: str,\n        skip_bad_segmentation: bool = False,\n    ) -> Union[str, None]:\n        line_id = \"-\".join(word_id.split(\"-\")[:-1])\n        line = find_child_by_tag(xml_root[1].findall(\"line\"), \"id\", line_id)\n        if line is not None and not (\n            skip_bad_segmentation and line.get(\"segmentation\") == \"err\"\n        ):\n            word = find_child_by_tag(line.findall(\"word\"), \"id\", word_id)\n            if word is not None:\n                return html.unescape(word.get(\"text\"))\n        return None\n\nclass IAMSyntheticDataGenerator(Dataset):\n    \"\"\"\n    Data generator that creates synthetic line/form images by stitching together word\n    images from the IAM dataset.\n    Calling `__getitem__()` samples a newly generated synthetic image every time\n    it is called.\n    \"\"\"\n\n    PUNCTUATION = [\",\", \".\", \";\", \":\", \"'\", '\"', \"!\", \"?\"]\n\n    def __init__(\n        self,\n        iam_root: Union[str, Path],\n        label_encoder: Optional[LabelParser] = None,\n        transforms: Optional[A.Compose] = None,\n        line_width: Tuple[int, int] = (1500, 2000),\n        lines_per_form: Tuple[int, int] = (1, 11),\n        words_per_line: Tuple[int, int] = (4, 10),\n        words_per_sequence: Tuple[int, int] = (7, 13),\n        px_between_lines: Tuple[int, int] = (25, 50),\n        px_between_words: int = 50,\n        px_around_image: Tuple[int, int] = (100, 200),\n        sample_form: bool = False,\n        only_lowercase: bool = False,\n        rng_seed: int = 0,\n        max_height: Optional[int] = None,\n    ):\n        super().__init__()\n        self.iam_root = iam_root\n        self.label_enc = label_encoder\n        self.transforms = transforms\n        self.line_width = line_width\n        self.lines_per_form = lines_per_form\n        self.words_per_line = words_per_line\n        self.words_per_sequence = words_per_sequence\n        self.px_between_lines = px_between_lines\n        self.px_between_words = px_between_words\n        self.px_around_image = px_around_image\n        self.sample_form = sample_form\n        self.only_lowercase = only_lowercase\n        self.rng_seed = rng_seed\n        self.max_height = max_height\n\n        self.iam_words = IAMDataset(\n            iam_root,\n            \"word\",\n            \"test\",\n            only_lowercase=only_lowercase,\n        )\n        if self.max_height is None:\n            self.max_height = IAMDataset.MAX_FORM_HEIGHT\n        if sample_form and \"\\n\" not in self.label_encoder.classes:\n            # Add the `\\n` token to the label encoder (since forms can contain newlines)\n            self.label_encoder.addClasses([\"\\n\"])\n        self.iam_words.transforms = None\n        self.rng = np.random.default_rng(rng_seed)\n\n    def __len__(self):\n        # This dataset does not have a finite length since it can generate random\n        # images at will, so return 1.\n        return 1\n\n    @property\n    def label_encoder(self):\n        if self.label_enc is not None:\n            return self.label_enc\n        return self.iam_words.label_enc\n\n    def __getitem__(self, *args, **kwargs):\n        \"\"\"By calling this method, a newly generated synthetic image is sampled.\"\"\"\n        if self.sample_form:\n            img, target = self.generate_form()\n        else:\n            img, target = self.generate_line()\n        if self.transforms is not None:\n            img = self.transforms(image=img)[\"image\"]\n        # Encode the target sequence using the label encoder.\n        target_enc = np.array(self.label_encoder.encode_labels([c for c in target]))\n        return img, target_enc\n\n    def generate_line(self) -> Tuple[np.ndarray, str]:\n        words_to_sample = self.rng.integers(*self.words_per_line)\n        line_width = self.rng.integers(*self.line_width)\n        return self.sample_lines(words_to_sample, line_width, sample_one_line=True)\n\n    def generate_form(self) -> Tuple[np.ndarray, str]:\n        # Randomly pick the number of words and inter-line distance in the form.\n        words_to_sample = self.rng.integers(*self.lines_per_form) * 5  # 7 is handpicked\n        px_between_lines = self.rng.integers(*self.px_between_lines)\n\n        # Sample line images.\n        line_width = self.rng.integers(*self.line_width)\n        lines, target = self.sample_lines(words_to_sample, line_width)\n\n        # Concatenate the lines vertically.\n        form_w = max(l.shape[1] for l in lines)\n        form_h = sum(l.shape[0] + px_between_lines for l in lines)\n        if form_h > self.max_height:\n            print(\n                \"Generated form height exceeds maximum height. Generating a new form.\"\n            )\n            return self.generate_form()\n        form = np.ones((form_h, form_w), dtype=lines[0].dtype) * 255\n        curr_h = 0\n        for line_img in lines:\n            h, w = line_img.shape\n            if curr_h + h + px_between_lines > self.max_height:\n                break\n\n            form[curr_h : curr_h + h, :w] = line_img\n            curr_h += h + px_between_lines\n\n        # Add a random amount of padding around the image.\n        pad_px = self.rng.integers(*self.px_around_image)\n        new_h, new_w = form.shape[0] + pad_px * 2, form.shape[1] + pad_px * 2\n        form = A.PadIfNeeded(\n            new_h, new_w, border_mode=cv.BORDER_CONSTANT, value=255, always_apply=True\n        )(image=form)[\"image\"]\n\n        return form, target\n\n    def set_rng(self, seed: int):\n        self.rng = np.random.default_rng(seed)\n\n    def sample_word_image(self) -> Tuple[np.ndarray, str]:\n        idx = random.randint(0, len(self.iam_words) - 1)\n        img, target = self.iam_words[idx]\n        target = \"\".join(self.iam_words.label_enc.decode_labels(target))\n        return img, target\n\n    def sample_word_image_sequence(\n        self, words_to_sample: int\n    ) -> List[Tuple[np.ndarray, str]]:\n        \"\"\"Sample a sequence of contiguous words.\"\"\"\n        assert words_to_sample >= 1\n        start_idx = random.randint(0, len(self.iam_words) - 1)\n\n        img_idxs = [start_idx]\n        img_path = Path(self.iam_words.data.iloc[start_idx][\"img_path\"])\n        _, _, line_id, word_id = img_path.stem.split(\"-\")\n        sampled_words = 1\n        while sampled_words < words_to_sample:\n            word_id = f\"{int(word_id) + 1 :02}\"\n            img_name = (\n                \"-\".join(img_path.stem.split(\"-\")[:-2] + [line_id, word_id]) + \".png\"\n            )\n            if not (img_path.parent / img_name).is_file():\n                # Previous image was the last on its line. Go to the next line.\n                line_id = f\"{int(line_id) + 1 :02}\"\n                word_id = \"00\"\n                img_name = (\n                    \"-\".join(img_path.stem.split(\"-\")[:-2] + [line_id, word_id])\n                    + \".png\"\n                )\n            if not (img_path.parent / img_name).is_file():\n                # End of the document.\n                return self.sample_word_image_sequence(words_to_sample)\n            # Find the dataset index for the sampled word.\n            ix = self.iam_words.id_to_idx.get(Path(img_name).stem)\n            if ix is None:\n                # If the image has segmentation=err attribute, it will\n                # not be in the dataset. In this case try again.\n                return self.sample_word_image_sequence(words_to_sample)\n            img_idxs.append(ix)\n            sampled_words += 1\n\n        imgs, targets = zip(*[self.iam_words[idx] for idx in img_idxs])\n        targets = [\n            \"\".join(self.iam_words.label_enc.decode_labels(t)) for t in targets\n        ]\n        return list(zip(imgs, targets))\n\n    def sample_lines(\n        self, words_to_sample: int, max_line_width: int, sample_one_line: bool = False\n    ) -> Tuple[Union[List[np.ndarray], np.ndarray], str]:\n        \"\"\"\n        Calls `sample_word_image_sequence` several times, using some heuristics\n        to glue the sequences together.\n\n        Returns:\n            - list of line images\n            - transcription for all lines combined\n        \"\"\"\n        curr_pos, sampled_words = 0, 0\n        imgs, targets, lines = [], [], []\n        target_str, last_target = \"\", \"\"\n\n        # Sample images.\n        while sampled_words < words_to_sample:\n            words_per_seq = self.rng.integers(*self.words_per_sequence)\n            # Sample a sequence of contiguous words.\n            img_tgt_seq = self.sample_word_image_sequence(words_per_seq)\n            for i, (img, tgt) in enumerate(img_tgt_seq):\n                # Add the sequence to the sampled words so far.\n                if sampled_words >= words_to_sample:\n                    break\n                h, w = img.shape\n\n                if curr_pos + w > max_line_width:\n                    # Concatenate the sampled images into a line.\n                    line = self.concatenate_line(imgs, targets, max_line_width)\n\n                    if sample_one_line:\n                        return line, target_str\n\n                    lines.append(line)\n                    target_str += \"\\n\"\n                    last_target = \"\\n\"\n                    curr_pos = 0\n                    imgs, targets = [], []\n\n                # Basic heuristics to avoid some strange looking sentences.\n                if i == 0 and (\n                    (last_target in self.PUNCTUATION and tgt in self.PUNCTUATION)\n                    or (tgt in self.PUNCTUATION and sampled_words == 0)\n                ):\n                    continue\n\n                if (\n                    sampled_words == 0\n                    or tgt in [c for c in self.PUNCTUATION if c not in [\"'\", '\"']]\n                    or last_target == \"\\n\"\n                ):\n                    target_str += tgt\n                else:\n                    target_str += \" \" + tgt\n\n                targets.append(tgt)\n                imgs.append(img)\n\n                sampled_words += 1\n                last_target = tgt\n                if tgt in self.PUNCTUATION:\n                    # Reduce horizontal spacing for punctuation tokens.\n                    curr_pos = max(0, curr_pos - self.px_between_words)\n                curr_pos += w + self.px_between_words\n        if imgs and targets:\n            # Concatenate the remaining images into a new line.\n            line = self.concatenate_line(imgs, targets, max_line_width)\n            lines.append(line)\n            if sample_one_line:\n                return line, target_str\n        return lines, target_str\n\n    def concatenate_line(\n        self, imgs: List[np.ndarray], targets: List[str], line_width: int\n    ) -> np.ndarray:\n        \"\"\"\n        Concatenate a series of (img, target) tuples into a line to create a line image.\n        \"\"\"\n        assert len(imgs) == len(targets)\n\n        line_height = max(im.shape[0] for im in imgs)\n        line = np.ones((line_height, line_width), dtype=imgs[0].dtype) * 255\n\n        curr_pos = 0\n        prev_lower_bound = line_height\n        for img, tgt in zip(imgs, targets):\n            h, w = img.shape\n            # Center the image in the middle of the line.\n            start_h = min(max(0, int((line_height - h) / 2)), line_height - h)\n\n            if tgt in [\",\", \".\"]:\n                # If sampled a comma or dot, place them at the bottom of the line.\n                start_h = min(max(0, prev_lower_bound - int(h / 2)), line_height - h)\n            elif tgt in ['\"', \"'\"]:\n                # If sampled a quote, place them at the top of the line.\n                start_h = 0\n            if tgt in self.PUNCTUATION:\n                # Reduce horizontal spacing for punctuation tokens.\n                curr_pos = max(0, curr_pos - self.px_between_words)\n\n            assert curr_pos + w <= line_width, f\"{curr_pos + w} > {line_width}\"\n            assert start_h + h <= line_height, f\"{start_h + h} > {line_height}\"\n\n            # Concatenate the word image to the line.\n            line[start_h : start_h + h, curr_pos : curr_pos + w] = img\n\n            curr_pos += w + self.px_between_words\n            prev_lower_bound = start_h + h\n        return line\n\n    @staticmethod\n    def get_worker_init_fn():\n        def worker_init_fn(worker_id: int):\n            set_seed(worker_id)\n            worker_info = torch.utils.data.get_worker_info()\n            dataset = worker_info.dataset  # the dataset copy in this worker process\n            if hasattr(dataset, \"set_rng\"):\n                dataset.set_rng(worker_id)\n            else:  # dataset is instance of `IAMDatasetSynthetic` class\n                dataset.synth_dataset.set_rng(worker_id)\n\n        return worker_init_fn\n\n\n\nclass IAMDatasetSynthetic(Dataset):\n    \"\"\"\n    A Pytorch dataset combining the IAM dataset with the IAMSyntheticDataGenerator\n    dataset.\n\n    The distribution of real/synthetic images can be controlled by setting the\n    `synth_prob` argument.\n    \"\"\"\n\n    iam_dataset: IAMDataset\n    synth_dataset: IAMSyntheticDataGenerator\n\n    def __init__(self, iam_dataset: IAMDataset, synth_prob: float = 0.3, **kwargs):\n        \"\"\"\n        Args:\n            iam_dataset (Dataset): the IAM dataset to sample from\n            synth_prob (float): the probability of sampling a synthetic image when\n                calling `__getitem__()`.\n        \"\"\"\n        self.iam_dataset = iam_dataset\n        self.synth_prob = synth_prob\n        self.synth_dataset = IAMSyntheticDataGenerator(\n            iam_root=iam_dataset.root,\n            label_encoder=iam_dataset.label_enc,\n            transforms=iam_dataset.transforms,\n            sample_form=(True if iam_dataset.parse_method == \"form\" else False),\n            only_lowercase=iam_dataset.only_lowercase,\n            max_height=(\n                (iam_dataset.data[\"bb_y_end\"] - iam_dataset.data[\"bb_y_start\"]).max()\n                if iam_dataset.parse_method == \"form\"\n                else None\n            ),\n            **kwargs,\n        )\n\n    def __getitem__(self, idx):\n        iam = self.iam_dataset\n        if random.random() > 1 - self.synth_prob:\n            # Sample from the synthetic dataset.\n            img, target = self.synth_dataset[0]\n        else:\n            # Index the IAM dataset.\n            img, target = iam[idx]\n        assert not np.any(np.isnan(img)), img\n        return img, target\n\n    def __len__(self):\n        return len(self.iam_dataset)\n    \n    \nimport time\nclass RIMESDataset(Dataset):\n    MEAN = 0.8275\n    STD = 0.2314\n    \n    root: Path\n    data: pd.DataFrame\n    label_enc: LabelParser\n    transforms: Optional[A.Compose]\n    id_to_idx: Dict[str, int]\n    _split: str\n    _return_writer_id: Optional[bool]\n    \n    _pad_token = \"<PAD>\"\n    _sos_token = \"<SOS>\"\n    _eos_token = \"<EOS>\"\n    \n    max_width: Optional[int]\n    max_height: Optional[int]\n    \n    @staticmethod\n    def process_target(target: str):\n    # Splitting the input string into lines\n        lines = target.split(\"\\\\n\")\n        \n        new_lines = []\n        for line in lines:\n            new_line = line\n            start_index = new_line.find(\"¤{\")\n            \n            while start_index != -1:\n                # Find the corresponding closing bracket\n                end_index = new_line.find(\"¤\", start_index + 1)\n                if end_index == -1:\n                    break  # Safety check\n    \n                seq = new_line[start_index + 2:end_index]\n                choices = seq.split(\"/\")\n                val = choices[randint(0, len(choices) - 1)]\n    \n                new_line = new_line[:start_index] + \" \" + val + \" \" + new_line[end_index + 1:]\n                \n                start_index = new_line.find(\"¤{\", start_index + 1)\n            \n            new_lines.append(new_line)\n        \n        return \"\\n\".join(new_lines)\n    \n    def __init__(\n            self,\n            root: Union[Path, str],\n            split: str, \n            only_lowercase: bool = False,\n            label_enc: Optional[LabelParser] = None,):\n        super().__init__()\n        \n        _splits = [\"train\", \"test\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n        \n        self._split = split\n        self.only_lowercase = only_lowercase\n        self.root = Path(root)\n        self.label_enc = label_enc\n        \n        if not hasattr(self, \"data\"):\n            self.data = self._get_form_data()\n        \n        if self.label_enc is None:\n            vocab = [self._pad_token, self._sos_token, self._eos_token]\n            s = \"\".join(self.data[\"target\"].tolist())\n            if self.only_lowercase:\n                s = s.lower()\n            vocab += sorted(list(set(s)))\n            self.label_enc = LabelParser().fit(vocab)\n            \n        if not \"target_enc\" in self.data.columns:\n            self.data.insert(\n                2,\n                \"target_enc\",\n                self.data[\"target\"].apply(\n                    lambda s: np.array(\n                        self.label_enc.encode_labels(\n                            [c for c in (s.lower() if self.only_lowercase else s)]\n                        )\n                    )\n                )\n            )\n        self.transforms = self._get_transforms(split)\n        self.id_to_idx = {\n            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n        }\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n        \n        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n            img = img[data[\"bb_y_start\"]: data[\"bb_y_end\"], :]\n        assert isinstance(img, np.ndarray), (\n            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n            f\"Is there something wrong with this image?\"\n        )\n        if self.transforms is not None:\n            img = self.transforms(image=img)[\"image\"]\n        \n        if img is None :\n            raise ValueError(\"Image is None in RIMES.\")\n        if data[\"target_enc\"] is None:\n            raise ValueError(\"Image is None in RIMES.\")\n\n        return img, data[\"target_enc\"]\n    \n    def get_max_height(self):\n        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max() + 150\n    \n    def get_max_width(self):\n        return (self.data[\"bb_x_end\"] - self.data[\"bb_x_start\"]).max() + 150\n    \n    @property\n    def vocab(self):\n        return self.label_enc.classes\n        \n    @staticmethod\n    def collate_fn(\n        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n        pad_val: int,\n        eos_tkn_idx: int,\n        dataset_returns_writer_id: bool = False,\n    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n        \n        imgs, targets = zip(*batch)\n\n        img_sizes = [im.shape for im in imgs]\n        if not len(set(img_sizes)) == 1:\n            hs, ws = zip(*img_sizes)\n            pad_fn = A.PadIfNeeded(\n                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n            )\n            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n        imgs = np.stack(imgs, axis=0)\n\n        seq_lengths = [t.shape[0] for t in targets]\n        targets_padded = np.full((len(targets), max(seq_lengths) + 1), pad_val)\n        for i, t in enumerate(targets):\n            targets_padded[i, : seq_lengths[i]] = t\n            targets_padded[i, seq_lengths[i]] = eos_tkn_idx\n\n        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n        \n        \n        return imgs, targets_padded\n    \n    def _get_transforms(self, split: str) -> A.Compose:\n        max_img_w = self.max_width\n    \n        max_img_h = self.max_height\n    \n        transforms = ImageTransforms(\n            (max_img_h, max_img_w), (RIMESDataset.MEAN, RIMESDataset.STD)\n        )\n    \n        if split == \"train\":\n            return transforms.train_trnsf\n        elif split == \"test\" or split == \"val\":\n            return transforms.test_trnsf\n    \n\n    def _get_form_data(self):\n        data = {\n            \"img_path\": [],\n            \"img_id\": [],\n            \"target\": [],\n            \"bb_y_start\": [],\n            \"bb_y_end\": [],\n            \"bb_x_start\": [],\n            \"bb_x_end\": [],\n            \"target_len\": [],\n        }\n        \n        \n        def process_forms(paths: Tuple[str, str, Path]):\n            return_data = {\n                \"img_path\": [],\n                \"img_id\": [],\n                \"target\": [],\n                \"bb_y_start\": [],\n                \"bb_y_end\": [],\n                \"bb_x_start\": [],\n                \"bb_x_end\": [],\n                \"target_len\": []\n            }\n            img_path, xml_path, root = paths\n            img_path = root / img_path\n            xml_path = root / xml_path\n            doc_id = img_path.stem[:-2]\n            xml_root = read_xml(xml_path)\n            \n            bb_y_start, bb_y_end, bb_x_start, bb_x_end = None, None, None, None\n            target = \"\"\n            num_corps = 0\n            for box in xml_root.iter(\"box\"):\n                type_tag =  box.find(\"type\")\n                if type_tag.text == \"Corps de texte\":\n                    target = box.find(\"text\").text\n                    if target is None or target == \"\":\n                        continue\n                    words = target.split(\"\\\\n\")\n                    if len(words) <= 5:\n                        continue\n                    bb_y_start = box.get(\"top_left_y\")\n                    bb_y_end   = box.get(\"bottom_right_y\")\n                    bb_x_start = box.get(\"top_left_x\")\n                    bb_x_end   = box.get(\"bottom_right_x\")\n                    \n                    return_data[\"img_path\"].append(str(img_path.resolve()))\n                    return_data[\"img_id\"].append(doc_id)\n                    return_data[\"target\"].append(self.process_target(target))\n                    return_data[\"bb_y_start\"].append(int(bb_y_start))\n                    return_data[\"bb_y_end\"].append(int(bb_y_end))\n                    return_data[\"bb_x_start\"].append(int(bb_x_start))\n                    return_data[\"bb_x_end\"].append(int(bb_x_end))\n                    return_data[\"target_len\"].append(len(target))\n                    num_corps += 1\n            \n            \n            # print(return_data[\"img_path\"])\n            \n            return return_data\n        \n        image_pairs = []\n        for form_dir in [\"DVD1_TIF\", \"DVD2_TIF\", \"DVD3_TIF\"]:\n            dr = self.root / form_dir\n            for file in dr.iterdir():\n                name = file.stem\n                ext = file.suffix\n                if ext == \".tif\" and name[-1] == \"L\":\n                    image_pairs.append((name + \".tif\", name + \".xml\", dr))\n        \n        with ThreadPoolExecutor(max_workers=10) as executor:\n            results = list(executor.map(process_forms, iter(image_pairs)))\n            \n            for single_results in results:\n                if not single_results[\"img_path\"]:\n                    continue\n                data[\"img_path\"].extend(single_results[\"img_path\"])\n                data[\"img_id\"].extend(single_results[\"img_id\"])\n                data[\"target\"].extend(single_results[\"target\"])\n                data[\"bb_y_start\"].extend(single_results[\"bb_y_start\"])\n                data[\"bb_y_end\"].extend(single_results[\"bb_y_end\"])\n                data[\"bb_x_start\"].extend(single_results[\"bb_x_start\"])\n                data[\"bb_x_end\"].extend(single_results[\"bb_x_end\"])\n                data[\"target_len\"].extend(single_results[\"target_len\"])\n        \n        to_ret = pd.DataFrame(data)\n        self.max_height = (to_ret[\"bb_y_end\"] - to_ret[\"bb_y_start\"]).max() + 150\n        self.max_width = (to_ret[\"bb_x_end\"] - to_ret[\"bb_x_start\"]).max() + 150\n        \n        return to_ret\n    \n    def set_transform_for_split(self, split):\n        _splits = [\"train\", \"val\", \"test\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n        self.transforms = self._get_transforms(split)\n    \n\nclass AggregatedDataset(Dataset):\n    datasets: List[Dataset]\n    def __init__(self, rimes: RIMESDataset,\n                 iam: IAMDataset, \n                 split:str,\n                 only_lowercase: bool = False,\n                 label_enc: Optional[LabelParser] = None,):\n        super().__init__()\n        _splits = [\"train\", \"test\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n        \n        self._split = split\n        self.rimes = rimes\n        self.iam = iam\n        self._only_lowercase = only_lowercase\n        self.label_enc = label_enc\n        \n        if self.label_enc is None:\n            iamLabelEncoder = iam.label_enc\n            rimesLabelEncoder = rimes.label_enc\n            self.label_enc = iamLabelEncoder\n            self.label_enc.addClasses(rimesLabelEncoder.classes)\n    \n    @staticmethod\n    def unified_collate_fn(\n        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n        pad_val: int,\n        eos_tkn_idx: int,) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n        \n        imgs, targets = zip(*batch)\n        img_sizes = [im.shape for im in imgs]\n        if not len(set(img_sizes)) == 1:\n            hs, ws = zip(*img_sizes)\n            pad_fn = A.PadIfNeeded(\n                max(hs), max(ws), border_mode = cv.BORDER_CONSTANT, value=0\n            )\n            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n        imgs = np.stack(imgs, axis=0)\n        \n        seq_lengths = [t.shape[0] for t in targets]\n        targets_padded = np.full((len(targets), max(seq_lengths) + 1), pad_val)\n        for i, t in enumerate(targets):\n            targets_padded[i, : seq_lengths[i]] = t\n            targets_padded[i, seq_lengths[i]] = eos_tkn_idx\n        \n        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n        \n        return imgs, targets_padded\n        \n    \n    def __len__(self):\n        return len(self.rimes) + len(self.iam)\n    \n    def __getitem__(self, idx):\n        iam = self.iam \n        rimes = self.rimes\n        if idx < len(self.rimes):\n            img, target = rimes[idx]\n        if len(rimes) <= idx < len(rimes) + len(iam):\n            img, target = iam[idx - len(self.rimes)]\n        \n        if img is None :\n            raise ValueError(\"Image is None.\")\n        if target is None:\n            raise ValueError(\"Image is None.\")\n        \n        assert not np.any(np.isnan(img)), img\n        return img, target\n    \n    def set_transforms_for_split(self, split):\n        self.iam.set_transforms_for_split(split)\n        self.rimes.set_transform_for_split(split)\n    ","metadata":{"ExecuteTime":{"end_time":"2024-04-17T18:00:09.341978Z","start_time":"2024-04-17T18:00:09.291501Z"},"execution":{"iopub.status.busy":"2024-05-13T20:40:07.959152Z","iopub.execute_input":"2024-05-13T20:40:07.959583Z","iopub.status.idle":"2024-05-13T20:40:08.145836Z","shell.execute_reply.started":"2024-05-13T20:40:07.959549Z","shell.execute_reply":"2024-05-13T20:40:08.144892Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Metrics","metadata":{}},{"cell_type":"code","source":"\nclass CharacterErrorRate(Metric):\n\n    def __init__(self, label_encoder: LabelParser):\n        super().__init__()\n        self.label_encoder = label_encoder\n        \n        self.add_state(\"cer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n        self.add_state(\"nr_samples\",default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        assert preds.ndim == target.ndim\n        eos_tkn_idx, sos_tkn_idx = list(\n            self.label_encoder.encode_labels([\"<EOS>\", \"<SOS>\"])\n        )\n\n        if (preds[:, 0] == sos_tkn_idx).all():  # this should normally be the case\n            preds = preds[:, 1:]\n\n        eos_idxs_prd = (preds == eos_tkn_idx).float().argmax(1).tolist()\n        eos_idxs_tgt = (target == eos_tkn_idx).float().argmax(1).tolist()\n\n        for i, (p, t) in enumerate(zip(preds, target)):\n            eos_idx_p, eos_idx_t = eos_idxs_prd[i], eos_idxs_tgt[i]\n            p = p[:eos_idx_p] if eos_idx_p else p\n            t = t[:eos_idx_t] if eos_idx_t else t\n            p_str, t_str = map(tensor_to_str, (p, t))\n            editd = editdistance.eval(p_str, t_str)\n\n            self.cer_sum += editd/t.numel()\n            self.nr_samples +=1\n\n    def compute(self) -> torch.Tensor:\n        return self.cer_sum / self.nr_samples.float()\n\nclass WordErrorRate(Metric):\n    def __init__(self, label_encoder: LabelParser):\n        super().__init__()\n        self.label_encoder = label_encoder\n        \n        self.add_state(\"wer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n        self.add_state(\"nr_samples\", default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        assert preds.ndim == target.ndim\n\n        eos_tkn_idx, sos_tkn_idx = self.label_encoder.encode_labels([\"<EOS>\", \"<SOS>\"])\n\n        if (preds[:, 0] == sos_tkn_idx).all():\n            preds = preds[:, 1:]\n\n        eos_idxs_prd = (preds == eos_tkn_idx).float().argmax(1).tolist()\n        eos_idxs_tgt = (target == eos_tkn_idx).float().argmax(1).tolist()\n\n        for i, (p, t) in enumerate(zip(preds, target)):\n            eos_idx_p, eos_idx_t = eos_idxs_prd[i], eos_idxs_tgt[i]\n            p = (p[:eos_idx_p] if eos_idx_p else p).flatten().tolist()\n            t = (t[:eos_idx_t] if eos_idx_t else t).flatten().tolist()\n            if not t:\n                continue\n            \n            p_words = \"\".join(self.label_encoder.decode_labels(p)).split()\n            t_words = \"\".join(self.label_encoder.decode_labels(t)).split()\n            editd = editdistance.eval(p_words, t_words)\n            \n            \n            self.wer_sum += editd / len(t_words)\n            self.nr_samples += 1\n            \n    def compute(self) -> torch.Tensor:\n        \"\"\"Compute Word Error Rate.\"\"\"\n        return self.wer_sum / self.nr_samples.float()\n\ndef tensor_to_str(t: torch.Tensor) -> str:\n    return \"\".join(map(str, t.flatten().tolist()))","metadata":{"ExecuteTime":{"end_time":"2024-04-17T18:00:09.353845Z","start_time":"2024-04-17T18:00:09.343701Z"},"execution":{"iopub.status.busy":"2024-05-13T20:40:08.147086Z","iopub.execute_input":"2024-05-13T20:40:08.147482Z","iopub.status.idle":"2024-05-13T20:40:08.168555Z","shell.execute_reply.started":"2024-05-13T20:40:08.147455Z","shell.execute_reply":"2024-05-13T20:40:08.167401Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Model class","metadata":{}},{"cell_type":"code","source":"\nclass PosEmbedding1D(nn.Module):\n    \"\"\"\n    Implements 1D sinusoidal embeddings.\n\n    Adapted from 'The Annotated Transformer', http://nlp.seas.harvard.edu/2018/04/03/attention.html\n    \"\"\"\n\n    def __init__(self, d_model, max_len=1100):\n        super().__init__()\n\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros((max_len, d_model), requires_grad=False)\n        position = torch.arange(0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        \"\"\"\n        Add a 1D positional embedding to an input tensor.\n\n        Args:\n            x (Tensor): tensor of shape (B, T, d_model) to add positional\n                embedding to\n        \"\"\"\n        _, T, _ = x.shape\n        # assert T <= self.pe.size(0) \\\n        assert T <= self.pe.size(1), (\n            f\"Stored 1D positional embedding does not have enough dimensions for the current feature map. \"\n            f\"Currently max_len={self.pe.size(1)}, T={T}. Consider increasing max_len such that max_len >= T.\"\n        )\n        return x + self.pe[:, :T]\n\n\n\nclass PosEmbedding2D(nn.Module):\n    def __init__(self, d_model, max_len=100):\n        super().__init__()\n        self.d_model = d_model\n        pe_x = torch.zeros((max_len, d_model // 2), requires_grad=False)\n        pe_y = torch.zeros((max_len, d_model // 2), requires_grad=False)\n\n        pos = torch.arange(0, max_len).unsqueeze(1)\n\n        div_term = torch.exp(\n            -math.log(10000.0) * torch.arange(0, d_model // 2, 2) / d_model\n        )\n\n        pe_y[:, 0::2] = torch.sin(pos * div_term)\n        pe_y[:, 1::2] = torch.cos(pos * div_term)\n        pe_x[:, 0::2] = torch.sin(pos * div_term)\n        pe_x[:, 1::2] = torch.cos(pos * div_term)\n\n        self.register_buffer(\"pe_x\", pe_x)\n        self.register_buffer(\"pe_y\", pe_y)\n\n    def forward(self, x):\n        _, w, h, _ = x.shape\n\n        pe_x_ = self.pe_x[:w, :].unsqueeze(1).expand(-1, h, -1)\n        pe_y_ = self.pe_y[:h, :].unsqueeze(0).expand(w, -1, -1)\n\n        pe = torch.cat([pe_y_, pe_x_], -1)\n        pe = pe.unsqueeze(0)\n\n        return x + pe\n\n\nclass encoderHTR(nn.Module):\n    def __init__(self, d_model: int, encoder_type: str, dropout=0.1, bias=True):\n        super().__init__()\n        assert encoder_type in [\"resnet18\", \"resnet34\", \"resnet50\"], \"Model not found\"\n\n        self.d_model = d_model\n        self.dropout = nn.Dropout(dropout)\n        self.pos_embd = PosEmbedding2D(d_model)\n\n        resnet = getattr(models, encoder_type)(pretrained=False)\n\n        modules = list(resnet.children())\n        cnv_1 = modules[0]\n        cnv_1 = nn.Conv2d(\n            1,\n            cnv_1.out_channels,\n            cnv_1.kernel_size,\n            cnv_1.stride,\n            cnv_1.padding,\n            bias=cnv_1.bias\n        )\n        self.encoder = nn.Sequential(cnv_1, *modules[1:-2])\n        self.linear = nn.Conv2d(resnet.fc.in_features, d_model, kernel_size=1)\n\n    def forward(self, imgs):\n        x = self.encoder(imgs.unsqueeze(1))\n        x = self.linear(x).transpose(1, 2).transpose(2, 3)\n        x = self.pos_embd(x)\n        x = self.dropout(x)\n        x = x.flatten(1, 2)\n\n        return x\n\n\nclass decoderHTR(nn.Module):\n    def __init__(self,\n                 vocab_length,\n                 max_seq_len,\n                 eos_tkn_idx,\n                 sos_tkn_idx,\n                 pad_tkn_idx,\n                 d_model,\n                 num_layers,\n                 nhead,\n                 dim_ffn,\n                 dropout,\n                 activation=\"relu\"):\n        super().__init__()\n        self.vocab_length = vocab_length\n        self.max_seq_len = max_seq_len\n        self.eos_idx = eos_tkn_idx\n        self.sos_idx = sos_tkn_idx\n        self.pad_idx = pad_tkn_idx\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.nhead = nhead\n        self.dim_ffn = dim_ffn\n        self.pos_emb = PosEmbedding1D(d_model)\n        self.emb = nn.Embedding(vocab_length, d_model)\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model,\n            nhead,\n            dim_ffn,\n            dropout,\n            activation,\n            batch_first=True\n        )\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n        self.clf = nn.Linear(d_model, vocab_length)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, memory: torch.Tensor):\n        B, _, _ = memory.shape\n        all_logits = []\n        sampled_ids = [torch.full([B], self.sos_idx).to(memory.device)]\n        tgt = self.pos_emb(\n            self.emb(sampled_ids[0]).unsqueeze(1) * math.sqrt(self.d_model)\n        )\n        tgt = self.drop(tgt)\n        eos_sampled = torch.zeros(B).bool()\n\n        for t in range(self.max_seq_len):\n            tgt_mask = self.subsequent_mask(len(sampled_ids)).to(memory.device)\n            out = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n            logits = self.clf(out[:, -1, :])\n            _, pred = torch.max(logits, -1)\n            all_logits.append(logits)\n            sampled_ids.append(pred)\n            for i, pr in enumerate(pred):\n                if pr == self.eos_idx:\n                    eos_sampled[i] = True\n            if eos_sampled.all():\n                break\n\n            tgt_ext = self.drop(\n                self.pos_emb.pe[:, len(sampled_ids)]\n                + self.emb(pred) * math.sqrt(self.d_model)\n            ).unsqueeze(1)\n            tgt = torch.cat([tgt, tgt_ext], 1)\n        sampled_ids = torch.stack(sampled_ids, 1)\n        all_logits = torch.stack(all_logits, 1)\n\n        eos_idxs = (sampled_ids == self.eos_idx).float().argmax(1)\n        for i in range(B):\n            if eos_idxs[i] != 0:\n                sampled_ids[i, eos_idxs[i] + 1:] = self.pad_idx\n\n        return all_logits, sampled_ids\n\n    def forward_teacher_forcing(self, memory: torch.Tensor, tgt: torch.Tensor):\n        B, T = tgt.shape\n        tgt = torch.cat(\n            [\n                torch.full([B], self.sos_idx).unsqueeze(1).to(memory.device),\n                tgt[:, :-1]\n            ],\n            1\n        )\n\n        tgt_key_masking = tgt == self.pad_idx\n        tgt_mask = self.subsequent_mask(T).to(tgt.device)\n\n        tgt = self.pos_emb(self.emb(tgt) * math.sqrt(self.d_model))\n        tgt = self.drop(tgt)\n        out = self.decoder(\n            tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_masking\n        )\n        logits = self.clf(out)\n        return logits\n\n    @staticmethod\n    def subsequent_mask(size: int):\n        mask = torch.triu(torch.ones(size, size), diagonal=1)\n        return mask == 1\n\n\nclass FullPageHTR(nn.Module):\n    encoder: encoderHTR\n    decoder: decoderHTR\n    cer_metric: CharacterErrorRate\n    wer_metric: WordErrorRate\n    loss_fn: Callable\n    label_encoder: LabelParser\n\n    def __init__(self, label_encoder: LabelParser,\n                 max_seq_len=500,\n                 d_model=1024,\n                 num_layers=6,\n                 nhead=4,\n                 dim_feedforward=1024,\n                 encoder_name=\"resnet18\",\n                 drop_enc=0.1,\n                 drop_dec=0.1,\n                 activ_dec=\"gelu\",\n                 loss_type=\"cross_entropy\",\n                 label_smoothing=0.0,\n                 vocab_len: Optional[int] = None):\n        super().__init__()\n        self.eos_token_idx, self.sos_token_idx, self.pad_token_idx = label_encoder.encode_labels(\n            [\"<EOS>\", \"<SOS>\", \"<PAD>\"]\n        )\n\n        self.encoder = encoderHTR(d_model, encoder_type=encoder_name, dropout=drop_enc)\n        self.decoder = decoderHTR(vocab_length=(vocab_len or len(label_encoder.classes)),\n                                  max_seq_len=max_seq_len,\n                                  eos_tkn_idx=self.eos_token_idx,\n                                  sos_tkn_idx=self.sos_token_idx,\n                                  pad_tkn_idx=self.pad_token_idx,\n                                  d_model=d_model,\n                                  num_layers=num_layers,\n                                  nhead=nhead,\n                                  dim_ffn=dim_feedforward,\n                                  dropout=drop_dec,\n                                  activation=activ_dec)\n        self.label_encoder = label_encoder\n        self.cer_metric = CharacterErrorRate(label_encoder)\n        self.wer_metric = WordErrorRate(label_encoder)\n        self.log_softmax = nn.LogSoftmax()\n        \n        assert loss_type in [\"cross_entropy\", \"ctc_loss\"]\n        self.loss_type = loss_type\n        if loss_type == \"cross_entropy\":\n            self.loss_fn = nn.CrossEntropyLoss(\n                ignore_index=self.pad_token_idx,\n                label_smoothing=label_smoothing\n            )\n        elif loss_type == \"ctc_loss\":\n            self.loss_fn = nn.CTCLoss(\n                blank=self.pad_token_idx\n            )\n\n    def forward(self, imgs: torch.Tensor, targets: Optional[torch.Tensor] = None):\n        logits, sampled_ids = self.decoder(self.encoder(imgs))\n        loss = None\n        if targets is not None:\n            if self.loss_type == \"cross_entropy\":\n                loss = self.loss_fn(\n                    logits[:, : targets.size(1), :].transpose(1, 2),\n                    targets[:, : logits.size(1)],\n                )\n            elif self.loss_type == \"ctc_loss\":\n                logits = self.log_softmax(logits)\n                _, preds = logits[:,: targets.size(1)].max(-1)\n                lengths = preds == self.eos_token_idx\n                _, target_lengths = (targets == self.eos_token_idx).max(-1)\n\n                # Calculate predicted lengths\n                pred_length = []\n                for batch in lengths:\n                    val, eos_pos = batch.max(-1)\n                    if val == 0:\n                        pred_length.append(batch.size(-1))\n                    else:\n                        pred_length.append(eos_pos.item())\n\n                # Optimize memory usage by avoiding temporary list\n                pred_length_tensor = torch.tensor(pred_length, dtype=torch.long)\n\n                logits = logits[:,: targets.size(1)].permute(1, 0, 2)\n                loss = self.loss_fn(logits, targets, tuple(pred_length), target_lengths.tolist())\n            \n        return logits, sampled_ids, loss\n\n    def forward_teacher_forcing(self, imgs: torch.Tensor, targets: torch.Tensor):\n        memory = self.encoder(imgs)\n        logits = self.decoder.forward_teacher_forcing(memory, targets)\n        if self.loss_type == \"cross_entropy\":\n            loss = self.loss_fn(logits.transpose(1, 2), targets)\n        elif self.loss_type == \"ctc_loss\":\n            logits = self.log_softmax(logits)\n            _, preds = logits.max(-1)\n            lengths = preds == self.eos_token_idx\n            _, target_lengths = (targets == self.eos_token_idx).max(-1)\n\n            # Calculate predicted lengths\n            pred_length = []\n            for batch in lengths:\n                val, eos_pos = batch.max(-1)\n                if val == 0:\n                    pred_length.append(batch.size(-1))\n                else:\n                    pred_length.append(eos_pos.item())\n            \n            logits = logits.permute(1, 0, 2)\n            loss = self.loss_fn(logits, targets, tuple(pred_length), tuple(target_lengths.tolist()) )\n            print(loss.item())\n        return logits, loss\n\n    def calculate_metrics(self, preds: torch.Tensor, targets: torch.Tensor):\n        self.cer_metric.reset()\n        self.wer_metric.reset()\n\n        cer = self.cer_metric(preds, targets)\n        wer = self.wer_metric(preds, targets)\n        return {\"CER\": cer, \"WER\": wer}\n\n    def set_num_output_classes(self, n_classes: int):\n        old_vocab_len = self.decoder.vocab_length\n        self.decoder.vocab_length = n_classes\n        self.decoder.clf = nn.Linear(self.decoder.d_model, n_classes)\n\n        new_embs = nn.Embedding(n_classes, self.decoder.d_model)\n        with torch.no_grad():\n            new_embs.weight[:old_vocab_len] = self.decoder.emb.weight\n            self.decoder.emb = new_embs\n","metadata":{"ExecuteTime":{"end_time":"2024-04-17T18:00:09.379441Z","start_time":"2024-04-17T18:00:09.354931Z"},"execution":{"iopub.status.busy":"2024-05-13T20:43:54.758458Z","iopub.execute_input":"2024-05-13T20:43:54.758869Z","iopub.status.idle":"2024-05-13T20:43:54.830229Z","shell.execute_reply.started":"2024-05-13T20:43:54.758837Z","shell.execute_reply":"2024-05-13T20:43:54.829121Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## Model Trainer Class","metadata":{}},{"cell_type":"code","source":"from copy import copy\nfrom pytorch_lightning import seed_everything\n\nseed_everything(12345)\niam_ds = IAMDataset(root=\"/kaggle/input/iam-rimes/data/raw/IAM\", label_enc=None, parse_method=\"form\" ,split=\"train\")\nrimes_ds = RIMESDataset(root=\"/kaggle/input/iam-rimes/data/raw/RIMES\", label_enc=None, split=\"train\")\nds = AggregatedDataset(rimes_ds, iam_ds, split=\"train\", label_enc=None)\nds_train, ds_val = torch.utils.data.random_split(ds, [math.ceil(0.8 * len(ds)), math.floor(0.2 * len(ds))])\n\nds_val.data = copy(ds)\nds_val.data.set_transforms_for_split(\"val\")\ntrain_len = len(ds_train)\nval_len = len(ds_val)\nprint(train_len, val_len)","metadata":{"ExecuteTime":{"end_time":"2024-04-17T18:00:10.966306Z","start_time":"2024-04-17T18:00:09.397368Z"},"execution":{"iopub.status.busy":"2024-05-13T20:43:54.855621Z","iopub.execute_input":"2024-05-13T20:43:54.856045Z","iopub.status.idle":"2024-05-13T20:44:05.470382Z","shell.execute_reply.started":"2024-05-13T20:43:54.856012Z","shell.execute_reply":"2024-05-13T20:44:05.468944Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"5077 1269\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\nbatch_size = 2\npad_tkn_idx, eos_tkn_idx = ds.label_enc.encode_labels([\"<PAD>\", \"<EOS>\"])\ncollate_fn = partial(\n        AggregatedDataset.unified_collate_fn, pad_val=pad_tkn_idx, eos_tkn_idx=eos_tkn_idx\n)\nnum_workers = 4\ndl_train = DataLoader(\n    ds_train,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=num_workers,\n    pin_memory=True,\n)\ndl_val = DataLoader(\n    ds_val,\n    batch_size=2 * batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=num_workers,\n    pin_memory=True,\n)\ntrain_len //= batch_size\nval_len //= 2 * batch_size","metadata":{"ExecuteTime":{"end_time":"2024-04-17T18:00:10.971166Z","start_time":"2024-04-17T18:00:10.967554Z"},"execution":{"iopub.status.busy":"2024-05-13T19:56:07.314021Z","iopub.execute_input":"2024-05-13T19:56:07.314717Z","iopub.status.idle":"2024-05-13T19:56:07.323070Z","shell.execute_reply.started":"2024-05-13T19:56:07.314684Z","shell.execute_reply":"2024-05-13T19:56:07.321927Z"}}},{"cell_type":"markdown","source":" import gc\n\n try:\n   device = \"cuda\"\n   torch.cuda.empty_cache()\n   gc.collect()\n\n   model = FullPageHTR(ds.label_enc).to(device)\n   optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n   trainer = ModelTrainer(\"Testing_run\", model, ds_name=\"IAM_forms\" , train_data=dl_train, val_data=dl_val, optimizer=optimizer, num_epochs=100, device=device, normalization_steps=56)\n\n   wandb.finish()\n   trainer.train(train_len, val_len, wanda=True)\n except RuntimeError:\n   del model\n   print(\"Error time!!\")","metadata":{"ExecuteTime":{"end_time":"2024-04-17T18:00:35.444932Z","start_time":"2024-04-17T18:00:34.988633Z"},"execution":{"iopub.status.busy":"2024-04-28T11:31:16.198702Z","iopub.status.idle":"2024-04-28T11:31:16.199348Z","shell.execute_reply.started":"2024-04-28T11:31:16.199103Z","shell.execute_reply":"2024-04-28T11:31:16.199124Z"}}},{"cell_type":"code","source":"batch_size = 2\npad_tkn_idx, eos_tkn_idx = ds.label_enc.encode_labels([\"<PAD>\", \"<EOS>\"])\ncollate_fn = partial(\n        AggregatedDataset.unified_collate_fn, pad_val=pad_tkn_idx, eos_tkn_idx=eos_tkn_idx\n)\nnum_workers = 4\ndl_train = DataLoader(\n    ds_train,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=num_workers,\n    pin_memory=True,\n)\ndl_val = DataLoader(\n    ds_val,\n    batch_size=2 * batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=num_workers,\n    pin_memory=True,\n)\ntrain_len //= batch_size\nval_len //= 2 * batch_size","metadata":{"execution":{"iopub.status.busy":"2024-05-13T20:44:05.472214Z","iopub.execute_input":"2024-05-13T20:44:05.472554Z","iopub.status.idle":"2024-05-13T20:44:05.481804Z","shell.execute_reply.started":"2024-05-13T20:44:05.472527Z","shell.execute_reply":"2024-05-13T20:44:05.480720Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"import gc\n\ntry:\n    device = \"cuda\"\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    model = FullPageHTR(ds.label_enc, loss_type=\"cross_entropy\", encoder_name=\"resnet34\").to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n    trainer = ModelTrainer(\"Synth_0.3_ctc_loss\", model, ds_name=\"IAM_forms_synthetic\" , train_data=dl_train_synth, val_data=dl_val_synth, optimizer=optimizer, num_epochs=100, device=device, normalization_steps=56)\n\n    wandb.finish()\n    trainer.train(train_len, val_len, wanda=True)\nfinally:\n    del model\n try:\n  \n except RuntimeError:\n   del model\n   print(\"Error time!!\")","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:35:30.046380Z","iopub.execute_input":"2024-04-28T11:35:30.046811Z","iopub.status.idle":"2024-04-28T11:35:31.975548Z","shell.execute_reply.started":"2024-04-28T11:35:30.046783Z","shell.execute_reply":"2024-04-28T11:35:31.974113Z"}}},{"cell_type":"code","source":"import pytorch_lightning as pl\n\nclass LitFullPageHTREncoderDecoder(pl.LightningModule):\n    model: FullPageHTR\n\n    \"\"\"\n    Pytorch Lightning module that acting as a wrapper around the\n    FullPageHTREncoderDecoder class.\n\n    Using a PL module allows the model to be used in conjunction with a Pytorch\n    Lightning Trainer, and takes care of logging relevant metrics to Tensorboard.\n    \"\"\"\n\n    def __init__(\n        self,\n        label_encoder: LabelParser,\n        learning_rate: float = 0.0002,\n        label_smoothing: float = 0.0,\n        max_seq_len: int = 500,\n        d_model: int = 260,\n        num_layers: int = 6,\n        nhead: int = 4,\n        dim_feedforward: int = 1024,\n        encoder_name: str = \"resnet18\",\n        drop_enc: int = 0.1,\n        drop_dec: int = 0.1,\n        activ_dec: str = \"gelu\",\n        loss_function: str = \"cross_entropy\",\n        vocab_len: Optional[int] = None,  # if not specified len(label_encoder) is used\n        params_to_log: Optional[Dict[str, Union[str, float, int]]] = None,\n    ):\n        super().__init__()\n\n        # Save hyperparameters.\n        self.learning_rate = learning_rate\n        if params_to_log is not None:\n            self.save_hyperparameters(params_to_log)\n        self.save_hyperparameters(\n            \"learning_rate\",\n            \"d_model\",\n            \"num_layers\",\n            \"nhead\",\n            \"dim_feedforward\",\n            \"max_seq_len\",\n            \"encoder_name\",\n            \"drop_enc\",\n            \"drop_dec\",\n            \"activ_dec\",\n        )\n\n        # Initialize the model.\n        self.model = FullPageHTR(\n            label_encoder=label_encoder,\n            max_seq_len=max_seq_len,\n            d_model=d_model,\n            num_layers=num_layers,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            encoder_name=encoder_name,\n            drop_enc=drop_enc,\n            drop_dec=drop_dec,\n            activ_dec=activ_dec,\n            vocab_len=vocab_len,\n            label_smoothing=label_smoothing,\n        )\n\n    @property\n    def encoder(self):\n        return self.model.encoder\n\n    @property\n    def decoder(self):\n        return self.model.decoder\n\n    def forward(self, imgs: Tensor, targets: Optional[Tensor] = None):\n        return self.model(imgs, targets)\n\n    def training_step(self, batch, batch_idx):\n        imgs, targets = batch\n        logits, loss = self.model.forward_teacher_forcing(imgs, targets)\n        self.log(\"train_loss\", loss, sync_dist=True, prog_bar=False)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        return self.val_or_test_step(batch)\n\n    def test_step(self, batch, batch_idx):\n        return self.val_or_test_step(batch)\n\n    def val_or_test_step(self, batch) -> Tensor:\n        imgs, targets = batch\n        logits, _, loss = self(imgs, targets)\n        _, preds = logits.max(-1)\n\n        # Update and log metrics.\n        self.model.cer_metric(preds, targets)\n        self.model.wer_metric(preds, targets)\n        self.log(\"char_error_rate\", self.model.cer_metric, on_step=True ,prog_bar=True)\n        self.log(\"word_error_rate\", self.model.wer_metric, on_step=True ,prog_bar=True)\n        self.log(\"val_loss\", loss, sync_dist=True, prog_bar=True, on_step=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.learning_rate)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T20:44:05.483680Z","iopub.execute_input":"2024-05-13T20:44:05.484063Z","iopub.status.idle":"2024-05-13T20:44:05.510422Z","shell.execute_reply.started":"2024-05-13T20:44:05.484030Z","shell.execute_reply":"2024-05-13T20:44:05.509257Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import Callback, ModelCheckpoint\nimport matplotlib.pyplot as plt\n\nPREDICTIONS_TO_LOG = {\n    \"word\": 10,\n    \"line\": 6,\n    \"form\": 1,\n}\n\n\nclass LogWorstPredictions(Callback):\n    \"\"\"\n    At the end of training, log the worst image prediction, meaning the predictions\n    with the highest character error rates.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_dataloader: Optional[DataLoader] = None,\n        val_dataloader: Optional[DataLoader] = None,\n        test_dataloader: Optional[DataLoader] = None,\n        training_skipped: bool = False,\n        data_format: str = \"word\",\n    ):\n        self.train_dataloader = train_dataloader\n        self.val_dataloader = val_dataloader\n        self.test_dataloader = test_dataloader\n        self.training_skipped = training_skipped\n        self.data_format = data_format\n\n    def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n        if self.training_skipped and self.val_dataloader is not None:\n            self.log_worst_predictions(\n                self.val_dataloader, trainer, pl_module, mode=\"val\"\n            )\n\n    def on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n        if self.test_dataloader is not None:\n            self.log_worst_predictions(\n                self.test_dataloader, trainer, pl_module, mode=\"test\"\n            )\n\n    def on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n        if self.train_dataloader is not None:\n            self.log_worst_predictions(\n                self.train_dataloader, trainer, pl_module, mode=\"train\"\n            )\n        if self.val_dataloader is not None:\n            self.log_worst_predictions(\n                self.val_dataloader, trainer, pl_module, mode=\"val\"\n            )\n\n    def log_worst_predictions(\n        self,\n        dataloader: DataLoader,\n        trainer: \"pl.Trainer\",\n        pl_module: \"pl.LightningModule\",\n        mode: str = \"train\",\n    ):\n        img_cers = []\n        device = \"cuda:0\" if pl_module.on_gpu else \"cpu\"\n        if not self.training_skipped:\n            self._load_best_model(trainer, pl_module)\n            pl_module = trainer.model\n\n        print(f\"Running {mode} inference on best model...\")\n\n        # Run inference on the validation set.\n        pl_module.eval()\n        for img, target in dataloader:\n            assert target.ndim == 2, target.ndim\n            cer_metric = pl_module.model.cer_metric\n            with torch.inference_mode():\n                logits, preds, _ = pl_module(img.to(device), target.to(device))\n                for prd, tgt, im in zip(preds, target, img):\n                    cer_metric.reset()\n                    cer = cer_metric(prd.unsqueeze(0), tgt.unsqueeze(0)).item()\n                    img_cers.append((im, cer, prd, tgt))\n\n        # Log the worst k predictions.\n        to_log = PREDICTIONS_TO_LOG[self.data_format] * 2\n        img_cers.sort(key=lambda x: x[1], reverse=True)  # sort by CER\n        img_cers = img_cers[:to_log]\n        fig = plt.figure(figsize=(24, 16))\n        for i, (im, cer, prd, tgt) in enumerate(img_cers):\n            pred_str, target_str = decode_prediction_and_target(\n                prd, tgt, pl_module.model.label_encoder, pl_module.decoder.eos_tkn_idx\n            )\n\n            # Create plot.\n            ncols = 4 if self.data_format == \"word\" else 2\n            nrows = math.ceil(to_log / ncols)\n            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n            matplotlib_imshow(im, IAMDataset.MEAN, IAMDataset.STD)\n            ax.set_title(f\"Pred: {pred_str} (CER: {cer:.2f})\\nTarget: {target_str}\")\n\n        # Log the results to Tensorboard.\n        tensorboard = trainer.logger.experiment\n        tensorboard.add_figure(f\"{mode}: worst predictions\", fig, trainer.global_step)\n        plt.close(fig)\n\n        print(\"Done.\")\n\n    @staticmethod\n    def _load_best_model(trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n        ckpt_callback = None\n        for cb in trainer.callbacks:\n            if isinstance(cb, ModelCheckpoint):\n                ckpt_callback = cb\n                break\n        assert ckpt_callback is not None, \"ModelCheckpoint not found in callbacks.\"\n        best_model_path = ckpt_callback.best_model_path\n\n        print(f\"Loading best model at {best_model_path}\")\n        label_encoder = pl_module.model.label_encoder\n        model = LitFullPageHTREncoderDecoder.load_from_checkpoint(\n            best_model_path,\n            label_encoder=label_encoder,\n        )\n        trainer.model.load_state_dict(model.state_dict())\n\n\nclass LogModelPredictions(Callback):\n    \"\"\"\n    Use a fixed test batch to monitor model predictions at the end of every epoch.\n\n    Specifically: it generates matplotlib Figure using a trained network, along with images\n    and labels from a batch, that shows the network's prediction alongside the actual target.\n    \"\"\"\n\n    def __init__(\n        self,\n        label_encoder: LabelParser,\n        val_batch: Tuple[torch.Tensor, torch.Tensor],\n        use_gpu: bool = True,\n        data_format: str = \"word\",\n        train_batch: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n    ):\n        self.label_encoder = label_encoder\n        self.val_batch = val_batch\n        self.use_gpu = use_gpu\n        self.data_format = data_format\n        self.train_batch = train_batch\n\n    def on_validation_epoch_end(\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n    ):\n        self._predict_intermediate(trainer, pl_module, split=\"val\")\n\n    def on_train_epoch_end(\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n    ):\n        if self.train_batch is not None:\n            self._predict_intermediate(trainer, pl_module, split=\"train\")\n\n    def _predict_intermediate(\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", split=\"val\"\n    ):\n        \"\"\"Make predictions on a fixed batch of data and log the results to Tensorboard.\"\"\"\n\n        # Make predictions.\n        if split == \"train\":\n            imgs, targets = self.train_batch\n        else:  # split == \"val\"\n            imgs, targets = self.val_batch\n        with torch.inference_mode():\n            pl_module.eval()\n            _, preds, _ = pl_module(imgs.cuda() if self.use_gpu else imgs)\n\n        # Decode predictions and generate a plot.\n        fig = plt.figure(figsize=(12, 16))\n        for i, (p, t) in enumerate(zip(preds, targets)):\n            pred_str, target_str = decode_prediction_and_target(\n                p, t, self.label_encoder, pl_module.decoder.eos_idx\n            )\n\n            # Create plot.\n            ncols = 2 if self.data_format == \"word\" else 1\n            nrows = math.ceil(preds.size(0) / ncols)\n            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n            matplotlib_imshow(imgs[i], IAMDataset.MEAN, IAMDataset.STD)\n            ax.set_title(f\"Pred: {pred_str}\\nTarget: {target_str}\")\n\n        # Log the results to Tensorboard.\n#         tensorboard = trainer.logger.experiment\n#         tensorboard.add_figure(\n#             f\"{split}: predictions vs targets\", fig, trainer.global_step\n#         )\n        trainer.logger.experiment.log({f\"{split}: predictions vs targets\": wandb.Image(fig)})\n\n        plt.close(fig)","metadata":{"execution":{"iopub.status.busy":"2024-05-13T20:44:05.512525Z","iopub.execute_input":"2024-05-13T20:44:05.512850Z","iopub.status.idle":"2024-05-13T20:44:05.553623Z","shell.execute_reply.started":"2024-05-13T20:44:05.512823Z","shell.execute_reply":"2024-05-13T20:44:05.552568Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ModelSummary\nfrom torch.utils.data import DataLoader, Subset\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb\n\ntorch.cuda.ipc_collect()  # collect garbage\ntorch.cuda.empty_cache()  # empty cache\nwandb.login(key=\"0350b0cc5bd9521bb37a798168d31b6b65e9caca\")\nwandb_logger = WandbLogger(project=\"bach_thesis\", log_model=\"all\")\ncallbacks = [\n        ModelCheckpoint(\n            save_top_k=(3),\n            mode=\"min\",\n            monitor=\"word_error_rate\",\n            filename=\"{epoch}-{char_error_rate:.4f}-{word_error_rate:.4f}\",\n        ),\n        ModelSummary(max_depth=2),\n        LitProgressBar(),\n        LogWorstPredictions(\n            ds_train,\n            ds_val,\n            training_skipped=False,\n            data_format=\"form\",\n        ),\n        LogModelPredictions(\n            ds.label_enc,\n            val_batch=next(\n                iter(\n                    DataLoader(\n                        Subset(\n                            ds_val,\n                            random.sample(\n                                range(len(ds_val)), 1\n                            ),\n                        ),\n                        batch_size=1,\n                        shuffle=False,\n                        collate_fn=collate_fn,\n                        num_workers=4,\n                        pin_memory=True,\n                    )\n                )\n            ),\n            train_batch=next(\n                iter(\n                    DataLoader(\n                        Subset(\n                            ds_train,\n                            random.sample(\n                                range(len(ds_train)),\n                                1,\n                            ),\n                        ),\n                        batch_size=1,\n                        shuffle=False,\n                        collate_fn=collate_fn,\n                        num_workers=4,\n                        pin_memory=True,\n                    )\n                )\n            ),\n            data_format=\"form\",\n            use_gpu=True,\n        ),\n        EarlyStopping(\n                    monitor=\"word_error_rate\",\n                    patience=50,\n                    verbose=True,\n                    mode=\"min\",\n                    check_on_train_epoch_end=False,\n                )\n    ]\n\ntrainer = Trainer(\n    max_epochs=3000,\n    accelerator=\"gpu\", \n    devices=1,\n    callbacks = callbacks,\n    logger=wandb_logger,\n    fast_dev_run=False)\nmodel = LitFullPageHTREncoderDecoder(ds.label_enc,\n        learning_rate = 0.00002,\n        label_smoothing = 0.0,\n        max_seq_len = 500,\n        d_model = 260,\n        num_layers = 6,\n        nhead = 4,\n        dim_feedforward = 1024,\n        encoder_name = \"resnet34\",\n        drop_enc = 0.1,\n        drop_dec = 0.1,\n        activ_dec = \"gelu\",\n        loss_function = \"cross_entropy\",\n        vocab_len = None, \n    )\ntrainer.fit(model, dl_train, dl_val)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T20:48:01.646094Z","iopub.execute_input":"2024-05-13T20:48:01.647081Z","iopub.status.idle":"2024-05-13T20:48:05.673942Z","shell.execute_reply.started":"2024-05-13T20:48:01.647047Z","shell.execute_reply":"2024-05-13T20:48:05.671933Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd396bb00e4a4536879230742a5f384e"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 97\u001b[0m\n\u001b[1;32m     82\u001b[0m     model \u001b[38;5;241m=\u001b[39m LitFullPageHTREncoderDecoder(ds\u001b[38;5;241m.\u001b[39mlabel_enc,\n\u001b[1;32m     83\u001b[0m             learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00002\u001b[39m,\n\u001b[1;32m     84\u001b[0m             label_smoothing \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m             vocab_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     96\u001b[0m         )\n\u001b[0;32m---> 97\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:987\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1031\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1031\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1060\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1060\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:370\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    369\u001b[0m     batch \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39m_on_before_batch_transfer(batch, dataloader_idx\u001b[38;5;241m=\u001b[39mdataloader_idx)\n\u001b[0;32m--> 370\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_to_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# the `_step` methods don't take a batch_idx when `dataloader_iter` is used, but all other hooks still do,\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# so we need different kwargs\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:278\u001b[0m, in \u001b[0;36mStrategy.batch_to_device\u001b[0;34m(self, batch, device, dataloader_idx)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_batch_transfer_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m move_data_to_device(batch, device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py:347\u001b[0m, in \u001b[0;36mLightningModule._apply_batch_transfer_handler\u001b[0;34m(self, batch, device, dataloader_idx)\u001b[0m\n\u001b[1;32m    346\u001b[0m device \u001b[38;5;241m=\u001b[39m device \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 347\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransfer_batch_to_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_after_batch_transfer\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch, dataloader_idx)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/module.py:336\u001b[0m, in \u001b[0;36mLightningModule._call_batch_hook\u001b[0;34m(self, hook_name, *args)\u001b[0m\n\u001b[1;32m    334\u001b[0m         trainer_method \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_lightning_datamodule_hook\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, hook_name)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/hooks.py:613\u001b[0m, in \u001b[0;36mDataHooks.transfer_batch_to_device\u001b[0;34m(self, batch, device, dataloader_idx)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Override this hook if your :class:`~torch.utils.data.DataLoader` returns tensors wrapped in a custom data\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;124;03mstructure.\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    611\u001b[0m \n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmove_data_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning_fabric/utilities/apply_func.py:103\u001b[0m, in \u001b[0;36mmove_data_to_device\u001b[0;34m(batch, device)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_TransferableDataType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_to\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py:66\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [function(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous tuple\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py:66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous tuple\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightning_fabric/utilities/apply_func.py:97\u001b[0m, in \u001b[0;36mmove_data_to_device.<locals>.batch_to\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     96\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon_blocking\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m data_output \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 116.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 130.12 MiB is free. Process 5178 has 15.76 GiB memory in use. Of the allocated memory 14.97 GiB is allocated by PyTorch, and 492.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 98\u001b[0m\n\u001b[1;32m     82\u001b[0m     model \u001b[38;5;241m=\u001b[39m LitFullPageHTREncoderDecoder(ds\u001b[38;5;241m.\u001b[39mlabel_enc,\n\u001b[1;32m     83\u001b[0m             learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.00002\u001b[39m,\n\u001b[1;32m     84\u001b[0m             label_smoothing \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m             vocab_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model, dl_train, dl_val)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43me\u001b[49m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"],"ename":"NameError","evalue":"name 'e' is not defined","output_type":"error"}]}]}
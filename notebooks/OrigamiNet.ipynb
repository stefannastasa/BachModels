{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.695236Z",
     "start_time": "2024-05-16T11:42:16.322123Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from random import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import cv2 as cv\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from random import randint\n",
    "import html\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, Dict, Sequence, Optional, List, Any, Callable, Optional\n",
    "import pandas as pd\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchmetrics import Metric\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset\n",
    "import editdistance\n",
    "import wandb\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LabelParser:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.vocab_size = None\n",
    "        self.class_to_idx = None\n",
    "        self.idx_to_class = None\n",
    "        self.ctc_classes = None\n",
    "        self.ctc_idx_to_class = None\n",
    "        self.ctc_class_to_idx = None\n",
    "        \n",
    "    def fit(self, classes: Sequence[str]):\n",
    "        self.classes = list(classes)\n",
    "        self.vocab_size = len(classes)\n",
    "        self.idx_to_class = dict(enumerate(classes))\n",
    "        self.class_to_idx = {cls: i for i, cls in self.idx_to_class.items()}\n",
    "        \n",
    "        self.ctc_classes = [\"<blank>\"] + self.classes\n",
    "        self.ctc_idx_to_class = dict(enumerate(self.ctc_classes))\n",
    "        self.ctc_class_to_idx = {cls: i for i, cls in self.ctc_idx_to_class.items()}\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def addClasses(self, classes: List[str]):\n",
    "        all_classes = sorted(set(self.classes + classes))\n",
    "\n",
    "        self.fit(all_classes)\n",
    "\n",
    "    def encode_labels(self, sequence: Sequence[str]):\n",
    "        self._check_fitted()\n",
    "        return [self.class_to_idx[c] for c in sequence]\n",
    "\n",
    "    def decode_labels(self, sequence: Sequence[int]):\n",
    "        self._check_fitted()\n",
    "        return [self.idx_to_class[c] for c in sequence]\n",
    "    \n",
    "    def ctc_encode_labels(self, sequence: Sequence[str]):\n",
    "        self._check_fitted()\n",
    "        return [self.ctc_class_to_idx[c] for c in sequence]\n",
    "    \n",
    "    def ctc_decode_labels(self, sequence: Sequence[int]):\n",
    "        self._check_fitted()\n",
    "        return [self.ctc_idx_to_class[c] for c in sequence]\n",
    "    \n",
    "    def _check_fitted(self):\n",
    "        if self.classes is None:\n",
    "            raise ValueError(\"LabelParser class was not fitted yet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.700937Z",
     "start_time": "2024-05-16T11:42:20.696431Z"
    }
   },
   "id": "c3cf44dc45a20bb0",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pickle_load(file) -> Any:\n",
    "    with open(file, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def pickle_save(obj, file):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def read_xml(file: Union[Path, str]) -> ET.Element:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    return root\n",
    "\n",
    "def find_child_by_tag(element: ET.Element, tag: str, value: str) -> Union[ET.Element, None]:\n",
    "    for child in element:\n",
    "        if child.get(tag) == value:\n",
    "            return child\n",
    "    return None\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def dpi_adjusting(img: np.ndarray, scale: float, **kwargs) -> np.ndarray:\n",
    "    height, width = img.shape[:2]\n",
    "    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n",
    "    return cv.resize(img, (new_width, new_height))\n",
    "\n",
    "class LitProgressBar(TQDMProgressBar):\n",
    "    def get_metrics(self, trainer, model):\n",
    "        # don't show the version number\n",
    "        items = super().get_metrics(trainer, model)\n",
    "        for k in list(items.keys()):\n",
    "            if k.startswith(\"grad\"):\n",
    "                items.pop(k, None)\n",
    "        items.pop(\"v_num\", None)\n",
    "        return items\n",
    "    \n",
    "def decode_prediction_and_target(\n",
    "    pred: Tensor, target: Tensor, label_encoder: LabelParser\n",
    ") -> Tuple[str, str]:\n",
    "\n",
    "    # Decode prediction and target.\n",
    "    p, t = pred.tolist(), target.tolist()\n",
    "    pred_str = \"\".join(label_encoder.ctc_decode_labels(p))\n",
    "    target_str = \"\".join(label_encoder.ctc_decode_labels(t))\n",
    "    return pred_str, target_str\n",
    "\n",
    "def matplotlib_imshow(\n",
    "    img: torch.Tensor, mean: float = 0.5, std: float = 0.5, one_channel=True\n",
    "):\n",
    "    assert img.device.type == \"cpu\"\n",
    "    if one_channel and img.ndim == 3:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img * std + mean  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.708328Z",
     "start_time": "2024-05-16T11:42:20.701808Z"
    }
   },
   "id": "e530c433919920ec",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SafeRandomScale(A.RandomScale):\n",
    "    def apply(self, img, scale=0, interpolation=cv.INTER_LINEAR, **params):\n",
    "        height, width = img.shape[:2]\n",
    "        new_height, new_width = int(height * scale), int(width * scale)\n",
    "        if new_height <= 0 or new_width <= 0:\n",
    "            return img\n",
    "        return super().apply(img, scale, interpolation, **params)\n",
    "\n",
    "def adjust_dpi(img: np.ndarray, scale: float, **kwargs):\n",
    "    height, width = img.shape\n",
    "    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n",
    "    return cv.resize(img, (new_width, new_height))\n",
    "\n",
    "def randomly_displace_and_pad(\n",
    "    img: np.ndarray,\n",
    "    padded_size: Tuple[int, int],\n",
    "    crop_if_necessary: bool = False,\n",
    "    **kwargs,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly displace an image within a frame, and pad zeros around the image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): image to process\n",
    "        padded_size (Tuple[int, int]): (height, width) tuple indicating the size of the frame\n",
    "        crop_if_necessary (bool): whether to crop the image if its size exceeds that\n",
    "            of the frame\n",
    "    \"\"\"\n",
    "    frame_h, frame_w = padded_size\n",
    "    img_h, img_w = img.shape\n",
    "    if frame_h < img_h or frame_w < img_w:\n",
    "        if crop_if_necessary:\n",
    "            print(\n",
    "                \"WARNING (`randomly_displace_and_pad`): cropping input image before \"\n",
    "                \"padding because it exceeds the size of the frame.\"\n",
    "            )\n",
    "            img_h, img_w = min(img_h, frame_h), min(img_w, frame_w)\n",
    "            img = img[:img_h, :img_w]\n",
    "        else:\n",
    "            raise AssertionError(\n",
    "                f\"Frame is smaller than the image: ({frame_h}, {frame_w}) vs. ({img_h},\"\n",
    "                f\" {img_w})\"\n",
    "            )\n",
    "\n",
    "    res = np.zeros((frame_h, frame_w), dtype=img.dtype)\n",
    "\n",
    "    pad_top =  randint(0, frame_h - img_h)\n",
    "    pad_bottom = pad_top + img_h\n",
    "    pad_left = randint(0, frame_w - img_w)\n",
    "    pad_right = pad_left + img_w\n",
    "\n",
    "    res[pad_top:pad_bottom, pad_left:pad_right] = img\n",
    "    return res\n",
    "\n",
    "@dataclass\n",
    "class ImageTransforms:\n",
    "    max_img_size: Tuple[int, int]  # (h, w)\n",
    "    normalize_params: Tuple[float, float]  # (mean, std)\n",
    "    scale: float = (\n",
    "        0.75\n",
    "    )\n",
    "    random_scale_limit: float = 0.1\n",
    "    random_rotate_limit: int = 1\n",
    "\n",
    "    train_trnsf: A.Compose = field(init=False)\n",
    "    test_trnsf: A.Compose = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        scale, random_scale_limit, random_rotate_limit, normalize_params =(\n",
    "            self.scale,\n",
    "            self.random_scale_limit,\n",
    "            self.random_rotate_limit,\n",
    "            self.normalize_params\n",
    "        )\n",
    "\n",
    "        max_img_h, max_img_w = self.max_img_size\n",
    "        max_scale = scale + scale * random_scale_limit\n",
    "        padded_h, padded_w = math.ceil(max_scale * max_img_h), math.ceil(max_scale * max_img_w)\n",
    "\n",
    "        self.train_trnsf = A.Compose([\n",
    "            A.Lambda(partial(adjust_dpi, scale=scale)),\n",
    "            SafeRandomScale(scale_limit=random_scale_limit, p=0.5),\n",
    "            # A.SafeRotate(\n",
    "            #     limit = random_rotate_limit,\n",
    "            #     border_mode = cv.BORDER_CONSTANT,\n",
    "            #     value = 0\n",
    "            # ),\n",
    "            A.RandomBrightnessContrast(),\n",
    "            # A.Perspective(scale=(0.01, 0.05)),\n",
    "            A.GaussNoise(),\n",
    "            A.Normalize(*normalize_params),\n",
    "            # A.Lambda(\n",
    "            #     image=partial(\n",
    "            #         randomly_displace_and_pad,\n",
    "            #         padded_size=(padded_h, padded_w),\n",
    "            #         crop_if_necessary=False,\n",
    "            #     )\n",
    "            # )\n",
    "        ])\n",
    "\n",
    "        self.test_trnsf = A.Compose([\n",
    "            A.Lambda(partial(adjust_dpi, scale=scale)),\n",
    "            A.Normalize(*normalize_params),\n",
    "            # A.PadIfNeeded(\n",
    "            #     max_img_h, max_img_w, border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            # )\n",
    "        ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.716503Z",
     "start_time": "2024-05-16T11:42:20.708975Z"
    }
   },
   "id": "eed805e60815386c",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CharacterErrorRate(Metric):\n",
    "\n",
    "    def __init__(self, label_encoder: LabelParser):\n",
    "        super().__init__()\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        self.add_state(\"cer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"nr_samples\",default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.ndim == target.ndim\n",
    "        eos_tkn_idx, sos_tkn_idx = list(\n",
    "            self.label_encoder.encode_labels([\"<EOS>\", \"<SOS>\"])\n",
    "        )\n",
    "\n",
    "        if (preds[:, 0] == sos_tkn_idx).all():  # this should normally be the case\n",
    "            preds = preds[:, 1:]\n",
    "\n",
    "        eos_idxs_prd = (preds == eos_tkn_idx).float().argmax(1).tolist()\n",
    "        eos_idxs_tgt = (target == eos_tkn_idx).float().argmax(1).tolist()\n",
    "\n",
    "        for i, (p, t) in enumerate(zip(preds, target)):\n",
    "            eos_idx_p, eos_idx_t = eos_idxs_prd[i], eos_idxs_tgt[i]\n",
    "            p = p[:eos_idx_p] if eos_idx_p else p\n",
    "            t = t[:eos_idx_t] if eos_idx_t else t\n",
    "            p_str, t_str = map(tensor_to_str, (p, t))\n",
    "            editd = editdistance.eval(p_str, t_str)\n",
    "\n",
    "            self.cer_sum += editd/t.numel()\n",
    "            self.nr_samples +=1\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.cer_sum / self.nr_samples.float()\n",
    "\n",
    "class WordErrorRate(Metric):\n",
    "    def __init__(self, label_encoder: LabelParser):\n",
    "        super().__init__()\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        self.add_state(\"wer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"nr_samples\", default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n",
    "\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.ndim == target.ndim\n",
    "\n",
    "        eos_tkn_idx, sos_tkn_idx = self.label_encoder.encode_labels([\"<EOS>\", \"<SOS>\"])\n",
    "\n",
    "        if (preds[:, 0] == sos_tkn_idx).all():\n",
    "            preds = preds[:, 1:]\n",
    "\n",
    "        eos_idxs_prd = (preds == eos_tkn_idx).float().argmax(1).tolist()\n",
    "        eos_idxs_tgt = (target == eos_tkn_idx).float().argmax(1).tolist()\n",
    "\n",
    "        for i, (p, t) in enumerate(zip(preds, target)):\n",
    "            eos_idx_p, eos_idx_t = eos_idxs_prd[i], eos_idxs_tgt[i]\n",
    "            p = (p[:eos_idx_p] if eos_idx_p else p).flatten().tolist()\n",
    "            t = (t[:eos_idx_t] if eos_idx_t else t).flatten().tolist()\n",
    "            if not t:\n",
    "                continue\n",
    "            \n",
    "            p_words = \"\".join(self.label_encoder.decode_labels(p)).split()\n",
    "            t_words = \"\".join(self.label_encoder.decode_labels(t)).split()\n",
    "            editd = editdistance.eval(p_words, t_words)\n",
    "            \n",
    "            \n",
    "            self.wer_sum += editd / len(t_words)\n",
    "            self.nr_samples += 1\n",
    "            \n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Compute Word Error Rate.\"\"\"\n",
    "        return self.wer_sum / self.nr_samples.float()\n",
    "\n",
    "def tensor_to_str(t: torch.Tensor) -> str:\n",
    "    return \"\".join(map(str, t.flatten().tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:44:10.989326Z",
     "start_time": "2024-05-16T11:44:10.961990Z"
    }
   },
   "id": "9f2d889f309c8525",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class IAMDataset(Dataset):\n",
    "    MEAN = 0.8275\n",
    "    STD = 0.2314\n",
    "    MAX_FORM_HEIGHT = 3542\n",
    "    MAX_FORM_WIDTH = 2479\n",
    "\n",
    "    MAX_SEQ_LENS = {\n",
    "        \"word\": 55,\n",
    "        \"line\": 90,\n",
    "        \"form\": 700,\n",
    "    }  # based on the maximum seq lengths found in the dataset\n",
    "\n",
    "    _pad_token = \"<PAD>\"\n",
    "    _sos_token = \"<SOS>\"\n",
    "    _eos_token = \"<EOS>\"\n",
    "\n",
    "    root: Path\n",
    "    data: pd.DataFrame\n",
    "    label_enc: LabelParser\n",
    "    parse_method: str\n",
    "    only_lowercase: bool\n",
    "    transforms: Optional[A.Compose]\n",
    "    id_to_idx: Dict[str, int]\n",
    "    _split: str\n",
    "    _return_writer_id: Optional[bool]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[Path, str],\n",
    "        parse_method: str,\n",
    "        split: str,\n",
    "        return_writer_id: bool = False,\n",
    "        only_lowercase: bool = False,\n",
    "        label_enc: Optional[LabelParser] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _parse_methods = [\"form\", \"line\", \"word\"]\n",
    "        err_message = (\n",
    "            f\"{parse_method} is not a possible parsing method: {_parse_methods}\"\n",
    "        )\n",
    "        assert parse_method in _parse_methods, err_message\n",
    "\n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "\n",
    "        self._split = split\n",
    "        self._return_writer_id = return_writer_id\n",
    "        self.only_lowercase = only_lowercase\n",
    "        self.root = Path(root)\n",
    "        self.label_enc = label_enc\n",
    "        self.parse_method = parse_method\n",
    "\n",
    "        # Process the data.\n",
    "        if not hasattr(self, \"data\"):\n",
    "            self.data = self._get_forms()\n",
    "\n",
    "        # Create the label encoder.\n",
    "        if self.label_enc is None:\n",
    "            vocab = []\n",
    "            s = \"\".join(self.data[\"target\"].tolist())\n",
    "            if self.only_lowercase:\n",
    "                s = s.lower()\n",
    "            vocab += sorted(list(set(s)))\n",
    "            self.label_enc = LabelParser().fit(vocab)\n",
    "        if not \"target_enc\" in self.data.columns:\n",
    "            self.data.insert(\n",
    "                2,\n",
    "                \"target_enc\",\n",
    "                self.data[\"target\"].apply(\n",
    "                    lambda s: np.array(\n",
    "                        self.label_enc.encode_labels(\n",
    "                            [c for c in (s.lower() if self.only_lowercase else s)]\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.transforms = self._get_transforms(split)\n",
    "        self.id_to_idx = {\n",
    "            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n",
    "        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n",
    "            # Crop the image vertically.\n",
    "            img = img[data[\"bb_y_start\"] : data[\"bb_y_end\"], :]\n",
    "        assert isinstance(img, np.ndarray), (\n",
    "            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n",
    "            f\"Is there something wrong with this image?\"\n",
    "        )\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        if self._return_writer_id:\n",
    "            return img, data[\"writer_id\"], data[\"target_enc\"]\n",
    "        return img, data[\"target_enc\"]\n",
    "\n",
    "    def get_max_height(self):\n",
    "        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.label_enc.classes\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(\n",
    "        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n",
    "        dataset_returns_writer_id: bool = False,\n",
    "    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n",
    "        if dataset_returns_writer_id:\n",
    "            imgs, writer_ids, targets = zip(*batch)\n",
    "        else:\n",
    "            imgs, targets = zip(*batch)\n",
    "\n",
    "        img_sizes = [im.shape for im in imgs]\n",
    "        if (\n",
    "            not len(set(img_sizes)) == 1\n",
    "        ):  # images are of varying sizes, so pad them to the maximum size in the batch\n",
    "            hs, ws = zip(*img_sizes)\n",
    "            pad_fn = A.PadIfNeeded(\n",
    "                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "\n",
    "        seq_lengths = [t.shape[0] for t in targets]\n",
    "        targets_padded = np.full((len(targets), max(seq_lengths) + 1))\n",
    "\n",
    "        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n",
    "        \n",
    "        return imgs, targets_padded\n",
    "\n",
    "    def set_transforms_for_split(self, split: str):\n",
    "        _splits = [\"train\", \"val\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        self.transforms = self._get_transforms(split)\n",
    "\n",
    "    def _get_transforms(self, split: str) -> A.Compose:\n",
    "        max_img_w = self.MAX_FORM_WIDTH\n",
    "\n",
    "        if self.parse_method == \"form\":\n",
    "            max_img_h = (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n",
    "        else:  # word or line\n",
    "            max_img_h = self.MAX_FORM_HEIGHT\n",
    "\n",
    "        transforms = ImageTransforms(\n",
    "            (max_img_h, max_img_w), (IAMDataset.MEAN, IAMDataset.STD)\n",
    "        )\n",
    "\n",
    "        if split == \"train\":\n",
    "            return transforms.train_trnsf\n",
    "        elif split == \"test\" or split == \"val\":\n",
    "            return transforms.test_trnsf\n",
    "\n",
    "    def statistics(self) -> Dict[str, float]:\n",
    "        assert len(self) > 0\n",
    "        tmp = self.transforms\n",
    "        self.transforms = None\n",
    "        mean, std, cnt = 0, 0, 0\n",
    "        for img, _ in self:\n",
    "            mean += np.mean(img)\n",
    "            std += np.var(img)\n",
    "            cnt += 1\n",
    "        mean /= cnt\n",
    "        std = np.sqrt(std / cnt)\n",
    "        self.transforms = tmp\n",
    "        return {\"mean\": mean, \"std\": std}\n",
    "    \n",
    "    def get_max_target_len(self):\n",
    "        return (self.data[\"target_len\"]).max()\n",
    "    \n",
    "    def _get_forms(self) -> pd.DataFrame:\n",
    "        \n",
    "        data = {\n",
    "            \"img_path\": [],\n",
    "            \"img_id\": [],\n",
    "            \"target\": [],\n",
    "            \"bb_y_start\": [],\n",
    "            \"bb_y_end\": [],\n",
    "            \"target_len\": [],\n",
    "        }\n",
    "        for form_dir in [\"formsA-D\", \"formsE-H\", \"formsI-Z\"]:\n",
    "            dr = self.root / form_dir\n",
    "            for img_path in dr.iterdir():\n",
    "                doc_id = img_path.stem\n",
    "                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n",
    "\n",
    "                bb_y_start = int(xml_root[1][0].get(\"asy\")) - 150\n",
    "                bb_y_end = int(xml_root[1][-1].get(\"dsy\")) + 150\n",
    "\n",
    "                form_text = []\n",
    "                for line in xml_root.iter(\"line\"):\n",
    "                    form_text.append(html.unescape(line.get(\"text\", \"\")))\n",
    "                \n",
    "                img_w, img_h = Image.open(str(img_path)).size\n",
    "                target = \" \".join(form_text)\n",
    "                data[\"img_path\"].append(str(img_path))\n",
    "                data[\"img_id\"].append(doc_id)\n",
    "                data[\"target\"].append(target)\n",
    "                data[\"bb_y_start\"].append(bb_y_start)\n",
    "                data[\"bb_y_end\"].append(bb_y_end)\n",
    "                data[\"target_len\"].append(len(target))\n",
    "        return pd.DataFrame(data).sort_values(\n",
    "            \"target_len\"\n",
    "        )  # by default, sort by target length\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.732251Z",
     "start_time": "2024-05-16T11:42:20.717929Z"
    }
   },
   "id": "1ff1ca365ba0bf41",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RIMESDataset(Dataset):\n",
    "    MEAN = 0.8275\n",
    "    STD = 0.2314\n",
    "    \n",
    "    root: Path\n",
    "    data: pd.DataFrame\n",
    "    label_enc: LabelParser\n",
    "    transforms: Optional[A.Compose]\n",
    "    id_to_idx: Dict[str, int]\n",
    "    _split: str\n",
    "    _return_writer_id: Optional[bool]\n",
    "    \n",
    "    _pad_token = \"<PAD>\"\n",
    "    _sos_token = \"<SOS>\"\n",
    "    _eos_token = \"<EOS>\"\n",
    "    \n",
    "    max_width: Optional[int]\n",
    "    max_height: Optional[int]\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_target(target: str):\n",
    "    # Splitting the input string into lines\n",
    "        lines = target.split(\"\\\\n\")\n",
    "        \n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            new_line = line\n",
    "            start_index = new_line.find(\"¤{\")\n",
    "            \n",
    "            while start_index != -1:\n",
    "                # Find the corresponding closing bracket\n",
    "                end_index = new_line.find(\"¤\", start_index + 1)\n",
    "                if end_index == -1:\n",
    "                    break  # Safety check\n",
    "    \n",
    "                seq = new_line[start_index + 2:end_index]\n",
    "                choices = seq.split(\"/\")\n",
    "                val = choices[randint(0, len(choices) - 1)]\n",
    "    \n",
    "                new_line = new_line[:start_index] + \" \" + val + \" \" + new_line[end_index + 1:]\n",
    "                \n",
    "                start_index = new_line.find(\"¤{\", start_index + 1)\n",
    "            \n",
    "            new_lines.append(new_line)\n",
    "        \n",
    "        return \" \".join(new_lines)\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            root: Union[Path, str],\n",
    "            split: str, \n",
    "            only_lowercase: bool = False,\n",
    "            label_enc: Optional[LabelParser] = None,):\n",
    "        super().__init__()\n",
    "        \n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        \n",
    "        self._split = split\n",
    "        self.only_lowercase = only_lowercase\n",
    "        self.root = Path(root)\n",
    "        self.label_enc = label_enc\n",
    "        \n",
    "        if not hasattr(self, \"data\"):\n",
    "            self.data = self._get_form_data()\n",
    "        \n",
    "        if self.label_enc is None:\n",
    "            vocab = []\n",
    "            s = \"\".join(self.data[\"target\"].tolist())\n",
    "            if self.only_lowercase:\n",
    "                s = s.lower()\n",
    "            vocab += sorted(list(set(s)))\n",
    "            self.label_enc = LabelParser().fit(vocab)\n",
    "            \n",
    "        if not \"target_enc\" in self.data.columns:\n",
    "            self.data.insert(\n",
    "                2,\n",
    "                \"target_enc\",\n",
    "                self.data[\"target\"].apply(\n",
    "                    lambda s: np.array(\n",
    "                        self.label_enc.ctc_encode_labels(\n",
    "                            [c for c in (s.lower() if self.only_lowercase else s)]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.transforms = self._get_transforms(split)\n",
    "        self.id_to_idx = {\n",
    "            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n",
    "            img = img[data[\"bb_y_start\"]: data[\"bb_y_end\"], :]\n",
    "        assert isinstance(img, np.ndarray), (\n",
    "            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n",
    "            f\"Is there something wrong with this image?\"\n",
    "        )\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        \n",
    "        return img, data[\"target_enc\"]\n",
    "    \n",
    "    def get_max_height(self):\n",
    "        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max() + 150\n",
    "    \n",
    "    def get_max_width(self):\n",
    "        return (self.data[\"bb_x_end\"] - self.data[\"bb_x_start\"]).max() + 150\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.label_enc.classes\n",
    "        \n",
    "    @staticmethod\n",
    "    def collate_fn(\n",
    "        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n",
    "        pad_val: int,\n",
    "        eos_tkn_idx: int,\n",
    "        dataset_returns_writer_id: bool = False,\n",
    "    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n",
    "        \n",
    "        imgs, targets = zip(*batch)\n",
    "\n",
    "        img_sizes = [im.shape for im in imgs]\n",
    "        if not len(set(img_sizes)) == 1:\n",
    "            hs, ws = zip(*img_sizes)\n",
    "            pad_fn = A.PadIfNeeded(\n",
    "                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "\n",
    "        seq_lengths = [t.shape[0] for t in targets]\n",
    "        targets_padded = np.full((len(targets), max(seq_lengths) + 1), pad_val)\n",
    "        for i, t in enumerate(targets):\n",
    "            targets_padded[i, : seq_lengths[i]] = t\n",
    "            targets_padded[i, seq_lengths[i]] = eos_tkn_idx\n",
    "\n",
    "        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n",
    "        \n",
    "        \n",
    "        return imgs, targets_padded\n",
    "    \n",
    "    def _get_transforms(self, split: str) -> A.Compose:\n",
    "        max_img_w = self.max_width\n",
    "    \n",
    "        max_img_h = self.max_height\n",
    "    \n",
    "        transforms = ImageTransforms(\n",
    "            (max_img_h, max_img_w), (RIMESDataset.MEAN, RIMESDataset.STD)\n",
    "        )\n",
    "    \n",
    "        if split == \"train\":\n",
    "            return transforms.train_trnsf\n",
    "        elif split == \"test\" or split == \"val\":\n",
    "            return transforms.test_trnsf\n",
    "    \n",
    "    def get_max_target_len(self):\n",
    "        return (self.data[\"target_len\"]).max()\n",
    "\n",
    "    def _get_form_data(self):\n",
    "        data = {\n",
    "            \"img_path\": [],\n",
    "            \"img_id\": [],\n",
    "            \"target\": [],\n",
    "            \"bb_y_start\": [],\n",
    "            \"bb_y_end\": [],\n",
    "            \"bb_x_start\": [],\n",
    "            \"bb_x_end\": [],\n",
    "            \"target_len\": [],\n",
    "        }\n",
    "        \n",
    "        \n",
    "        def process_forms(paths: Tuple[str, str, Path]):\n",
    "            return_data = {\n",
    "                \"img_path\": [],\n",
    "                \"img_id\": [],\n",
    "                \"target\": [],\n",
    "                \"bb_y_start\": [],\n",
    "                \"bb_y_end\": [],\n",
    "                \"bb_x_start\": [],\n",
    "                \"bb_x_end\": [],\n",
    "                \"target_len\": []\n",
    "            }\n",
    "            img_path, xml_path, root = paths\n",
    "            img_path = root / img_path\n",
    "            xml_path = root / xml_path\n",
    "            doc_id = img_path.stem[:-2]\n",
    "            xml_root = read_xml(xml_path)\n",
    "            \n",
    "            target = \"\"\n",
    "            num_corps = 0\n",
    "            for box in xml_root.iter(\"box\"):\n",
    "                type_tag =  box.find(\"type\")\n",
    "                if type_tag.text == \"Corps de texte\":\n",
    "                    target = box.find(\"text\").text\n",
    "                    if target is None or target == \"\":\n",
    "                        continue\n",
    "                    words = target.split(\"\\\\n\")\n",
    "                    if len(words) <= 5:\n",
    "                        continue\n",
    "                    \n",
    "                    return_data[\"img_path\"].append(str(img_path.resolve()))\n",
    "                    return_data[\"img_id\"].append(doc_id)\n",
    "                    return_data[\"target\"].append(self.process_target(target))\n",
    "                    return_data[\"bb_y_start\"].append(int(box.get(\"top_left_y\")))\n",
    "                    return_data[\"bb_y_end\"].append(int(box.get(\"bottom_right_y\")))\n",
    "                    return_data[\"bb_x_start\"].append(int(box.get(\"top_left_x\")))\n",
    "                    return_data[\"bb_x_end\"].append(int(box.get(\"bottom_right_x\")))\n",
    "                    return_data[\"target_len\"].append(len(target))\n",
    "                    num_corps += 1\n",
    "            \n",
    "            return return_data\n",
    "        \n",
    "        image_pairs = []\n",
    "        for form_dir in [\"DVD1_TIF\", \"DVD2_TIF\", \"DVD3_TIF\"]:\n",
    "            dr = self.root / form_dir\n",
    "            for file in dr.iterdir():\n",
    "                name = file.stem\n",
    "                ext = file.suffix\n",
    "                if ext == \".tif\" and name[-1] == \"L\":\n",
    "                    image_pairs.append((name + \".tif\", name + \".xml\", dr))\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(process_forms, iter(image_pairs)))\n",
    "            \n",
    "            for single_results in results:\n",
    "                if single_results[\"img_path\"] == \"\":\n",
    "                    continue\n",
    "                data[\"img_path\"].extend(single_results[\"img_path\"])\n",
    "                data[\"img_id\"].extend(single_results[\"img_id\"])\n",
    "                data[\"target\"].extend(single_results[\"target\"])\n",
    "                data[\"bb_y_start\"].extend(single_results[\"bb_y_start\"])\n",
    "                data[\"bb_y_end\"].extend(single_results[\"bb_y_end\"])\n",
    "                data[\"bb_x_start\"].extend(single_results[\"bb_x_start\"])\n",
    "                data[\"bb_x_end\"].extend(single_results[\"bb_x_end\"])\n",
    "                data[\"target_len\"].extend(single_results[\"target_len\"])\n",
    "        \n",
    "        to_ret = pd.DataFrame(data)\n",
    "        self.max_height = (to_ret[\"bb_y_end\"] - to_ret[\"bb_y_start\"]).max() + 150\n",
    "        self.max_width = (to_ret[\"bb_x_end\"] - to_ret[\"bb_x_start\"]).max() + 150\n",
    "        \n",
    "        return to_ret"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.749373Z",
     "start_time": "2024-05-16T11:42:20.733039Z"
    }
   },
   "id": "792a552ca46529fd",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class AggregatedDataset(Dataset):\n",
    "    datasets: List[Dataset]\n",
    "    def __init__(self, rimes: RIMESDataset,\n",
    "                 iam: IAMDataset, \n",
    "                 split:str,\n",
    "                 only_lowercase: bool = False,\n",
    "                 label_enc: Optional[LabelParser] = None,):\n",
    "        super().__init__()\n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        \n",
    "        self._split = split\n",
    "        self.rimes = rimes\n",
    "        self.iam = iam\n",
    "        self._only_lowercase = only_lowercase\n",
    "        self.label_enc = label_enc\n",
    "        \n",
    "        if self.label_enc is None:\n",
    "            iamLabelEncoder = iam.label_enc\n",
    "            rimesLabelEncoder = rimes.label_enc\n",
    "            self.label_enc = LabelParser()\n",
    "            self.label_enc.addClasses(iamLabelEncoder.classes)\n",
    "            self.label_enc.addClasses(rimesLabelEncoder.classes)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.rimes) + len(self.iam)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.rimes):\n",
    "            img, target = self.rimes[idx]\n",
    "        if len(self.rimes) <= idx < len(self.iam):\n",
    "            img, target = self.iam[idx - len(self.rimes)]\n",
    "        \n",
    "        return img, target"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.754035Z",
     "start_time": "2024-05-16T11:42:20.750092Z"
    }
   },
   "id": "8040b6395574cb93",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return nn.functional.layer_norm(x, x.size()[1:], weight=None, bias=None, eps=1e-05)\n",
    "\n",
    "def pCnv(inp,out,groups=1):\n",
    "  return nn.Sequential(\n",
    "      nn.Conv2d(inp,out,1,bias=False,groups=groups),\n",
    "      nn.InstanceNorm2d(out,affine=True)\n",
    "  )\n",
    "\n",
    "def dsCnv(inp,k):\n",
    "  return nn.Sequential(\n",
    "      nn.Conv2d(inp,inp,k,groups=inp,bias=False,padding=(k - 1) // 2),\n",
    "      nn.InstanceNorm2d(inp,affine=True)\n",
    "  )\n",
    "\n",
    "class InitBlock(nn.Module):\n",
    "    def __init__(self, fup, num_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n1 = LayerNorm()\n",
    "        self.InitSeq = nn.Sequential(\n",
    "            pCnv(num_channels, fup),\n",
    "            nn.Softmax(dim=1),\n",
    "            dsCnv(fup, 11),\n",
    "            LayerNorm()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x  = self.n1(x)\n",
    "        xt = x\n",
    "        x = self.InitSeq(x)\n",
    "        x = torch.cat([x, xt], dim=1)\n",
    "        return x\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, ifsz):\n",
    "        super().__init__()\n",
    "        self.ln = LayerNorm()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t0, t1 = torch.chunk(x, 2, dim=1)\n",
    "        t0 = torch.tanh(t0)\n",
    "        t1.sub(2)\n",
    "        t1 = torch.sigmoid(t1)\n",
    "        \n",
    "        return t1 * t0\n",
    "\n",
    "class GateBlock(nn.Module):\n",
    "    def __init__(self, ifsz, ofsz, gt = True, ksz = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        cfsz = int(math.floor(ifsz / 2))\n",
    "        ifsz2 = ifsz + ifsz%2\n",
    "        \n",
    "        self.sq = nn.Sequential(\n",
    "            pCnv(ifsz, cfsz),\n",
    "            dsCnv(cfsz, ksz),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            pCnv(cfsz, cfsz * 2),\n",
    "            dsCnv(cfsz * 2, ksz),\n",
    "            Gate(cfsz),\n",
    "            \n",
    "            pCnv(cfsz, ifsz),\n",
    "            dsCnv(ifsz, ksz),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        self.gt = gt\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.sq(x)\n",
    "        \n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "class OrigamiNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_channels: int, \n",
    "                 label_enc: LabelParser, \n",
    "                 mul_rate, \n",
    "                 layer_resizes, \n",
    "                 layer_sizes, \n",
    "                 num_layers, \n",
    "                 fup, \n",
    "                 reduceAxis=3 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_resizes = layer_resizes\n",
    "        self.Init_sequence = InitBlock(fup, 1)\n",
    "        self.label_enc = label_enc\n",
    "        \n",
    "        self.cer_metric = CharacterErrorRate(label_enc)\n",
    "        self.wer_metric = WordErrorRate(label_enc)\n",
    "        \n",
    "        layers = []\n",
    "        input_size = fup + n_channels\n",
    "        output_size = input_size\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            output_size = int(math.floor(layer_sizes[i] * mul_rate) ) if i in layer_sizes else input_size\n",
    "            layers.append(GateBlock(input_size, output_size, True, 3))\n",
    "            \n",
    "            if input_size != output_size:\n",
    "                layers.append(pCnv(input_size, output_size))\n",
    "                layers.append(nn.ELU())\n",
    "            input_size = output_size\n",
    "            \n",
    "            if i in layer_resizes:\n",
    "                layers.append(layer_resizes[i])\n",
    "        \n",
    "        layers.append(LayerNorm())\n",
    "        self.Gatesq = nn.Sequential(*layers)\n",
    "        self.Finsq = nn.Sequential(\n",
    "            pCnv(output_size, self.label_enc.vocab_size),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.n1 = LayerNorm()\n",
    "        self.it = 0\n",
    "        self.reduceAxis = reduceAxis\n",
    "        self.loss_fn = nn.CTCLoss(reduction=\"none\", zero_infinity=True)\n",
    "        \n",
    "    def forward(self, image, targets: Optional[torch.Tensor]):\n",
    "        x = self.Init_sequence(image)\n",
    "        x = self.Gatesq(x)\n",
    "        x = self.Finsq(x)\n",
    "        \n",
    "        x = torch.mean(x, self.reduceAxis, keepdim=False)\n",
    "        x = self.n1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        if targets is not None:\n",
    "            logits = x\n",
    "            logits = logits.permute(1, 0, 2).log_softmax(2)\n",
    "            logits_size = torch.IntTensor([logits.size(1)] * targets.size(0))\n",
    "            print(logits_size)\n",
    "            targets_size = torch.IntTensor([targets.size(1)] * targets.size(0))\n",
    "            print(targets_size)\n",
    "            loss = self.loss_fn(logits, targets, logits_size, targets_size)\n",
    "            return x, loss\n",
    "        return x \n",
    "    \n",
    "    def calculate_metrics(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        self.cer_metric.reset()\n",
    "        self.wer_metric.reset()\n",
    "        \n",
    "        cer = self.cer_metric(preds, targets)\n",
    "        wer = self.wer_metric(preds, targets)\n",
    "        \n",
    "        return {\"CER\": cer, \"WER\":wer}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.764031Z",
     "start_time": "2024-05-16T11:42:20.754686Z"
    }
   },
   "id": "207e080dc93b2156",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitOrigamiNet(pl.LightningModule):\n",
    "    model: OrigamiNet\n",
    "\n",
    "    \"\"\"\n",
    "    Pytorch Lightning module that acting as a wrapper around the\n",
    "    FullPageHTREncoderDecoder class.\n",
    "\n",
    "    Using a PL module allows the model to be used in conjunction with a Pytorch\n",
    "    Lightning Trainer, and takes care of logging relevant metrics to Tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels: int, \n",
    "        label_encoder: LabelParser,\n",
    "        mul_rate: int, \n",
    "        layer_resizes: dict,\n",
    "        layer_sizes: dict,\n",
    "        num_layers: int,\n",
    "        fup: int,\n",
    "        reduce_axis:int = 3,\n",
    "        learning_rate: float = 0.0002,\n",
    "        params_to_log: Optional[Dict[str, Union[str, float, int]]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save hyperparameters.\n",
    "        self.learning_rate = learning_rate\n",
    "        if params_to_log is not None:\n",
    "            self.save_hyperparameters(params_to_log)\n",
    "        self.save_hyperparameters(\n",
    "            \"learning_rate\",\n",
    "            \"n_channels\",\n",
    "            \"num_layers\",\n",
    "        )\n",
    "\n",
    "        # Initialize the model.\n",
    "        self.model = OrigamiNet(\n",
    "            n_channels,\n",
    "            label_encoder,\n",
    "            mul_rate,\n",
    "            layer_resizes,\n",
    "            layer_sizes,\n",
    "            fup,\n",
    "            reduce_axis\n",
    "        )\n",
    "\n",
    "    def forward(self, imgs: Tensor, targets: Optional[Tensor] = None):\n",
    "        return self.model(imgs, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, targets = batch\n",
    "        logits, loss = self.model(imgs, targets)\n",
    "        self.log(\"train_loss\", loss, sync_dist=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.val_or_test_step(batch)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.val_or_test_step(batch)\n",
    "\n",
    "    def val_or_test_step(self, batch) -> Tensor:\n",
    "        imgs, targets = batch\n",
    "        logits, loss = self(imgs, targets)\n",
    "        _, preds = logits.max(-1)\n",
    "\n",
    "        # Update and log metrics.\n",
    "        self.model.cer_metric(preds, targets)\n",
    "        self.model.wer_metric(preds, targets)\n",
    "        self.log(\"char_error_rate\", self.model.cer_metric, on_step=True ,prog_bar=True)\n",
    "        self.log(\"word_error_rate\", self.model.wer_metric, on_step=True ,prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, sync_dist=True, prog_bar=True, on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d456354883c8032b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PREDICTIONS_TO_LOG = {\n",
    "    \"word\": 10,\n",
    "    \"line\": 6,\n",
    "    \"form\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "class LogWorstPredictions(Callback):\n",
    "    \"\"\"\n",
    "    At the end of training, log the worst image prediction, meaning the predictions\n",
    "    with the highest character error rates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dataloader: Optional[DataLoader] = None,\n",
    "        val_dataloader: Optional[DataLoader] = None,\n",
    "        test_dataloader: Optional[DataLoader] = None,\n",
    "        training_skipped: bool = False,\n",
    "        data_format: str = \"word\",\n",
    "    ):\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.training_skipped = training_skipped\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if self.training_skipped and self.val_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.val_dataloader, trainer, pl_module, mode=\"val\"\n",
    "            )\n",
    "\n",
    "    def on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if self.test_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.test_dataloader, trainer, pl_module, mode=\"test\"\n",
    "            )\n",
    "\n",
    "    def on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if self.train_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.train_dataloader, trainer, pl_module, mode=\"train\"\n",
    "            )\n",
    "        if self.val_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.val_dataloader, trainer, pl_module, mode=\"val\"\n",
    "            )\n",
    "\n",
    "    def log_worst_predictions(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        trainer: \"pl.Trainer\",\n",
    "        pl_module: \"pl.LightningModule\",\n",
    "        mode: str = \"train\",\n",
    "    ):\n",
    "        img_cers = []\n",
    "        device = \"cuda:0\" if pl_module.on_gpu else \"cpu\"\n",
    "        if not self.training_skipped:\n",
    "            self._load_best_model(trainer, pl_module)\n",
    "            pl_module = trainer.model\n",
    "\n",
    "        print(f\"Running {mode} inference on best model...\")\n",
    "\n",
    "        # Run inference on the validation set.\n",
    "        pl_module.eval()\n",
    "        for img, target in dataloader:\n",
    "            assert target.ndim == 2, target.ndim\n",
    "            cer_metric = pl_module.model.cer_metric\n",
    "            with torch.inference_mode():\n",
    "                preds, _ = pl_module(img.to(device), target.to(device))\n",
    "                for prd, tgt, im in zip(preds, target, img):\n",
    "                    cer_metric.reset()\n",
    "                    cer = cer_metric(prd.unsqueeze(0), tgt.unsqueeze(0)).item()\n",
    "                    img_cers.append((im, cer, prd, tgt))\n",
    "\n",
    "        # Log the worst k predictions.\n",
    "        to_log = PREDICTIONS_TO_LOG[self.data_format] * 2\n",
    "        img_cers.sort(key=lambda x: x[1], reverse=True)  # sort by CER\n",
    "        img_cers = img_cers[:to_log]\n",
    "        fig = plt.figure(figsize=(100, 100))\n",
    "        for i, (im, cer, prd, tgt) in enumerate(img_cers):\n",
    "            pred_str, target_str = decode_prediction_and_target(\n",
    "                prd, tgt, pl_module.model.label_encoder\n",
    "            )\n",
    "\n",
    "            # Create plot.\n",
    "            ncols = 4 if self.data_format == \"word\" else 2\n",
    "            nrows = math.ceil(to_log / ncols)\n",
    "            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n",
    "            matplotlib_imshow(im, IAMDataset.MEAN, IAMDataset.STD)\n",
    "            ax.set_title(f\"Pred: {pred_str} (CER: {cer:.2f})\\nTarget: {target_str}\")\n",
    "\n",
    "        # # Log the results to Tensorboard.\n",
    "        # tensorboard = trainer.logger.experiment\n",
    "        # tensorboard.add_figure(f\"{mode}: worst predictions\", fig, trainer.global_step)\n",
    "        trainer.logger.experiment.log({f\"{mode}: predictions vs targets\": wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(\"Done.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_best_model(trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        ckpt_callback = None\n",
    "        for cb in trainer.callbacks:\n",
    "            if isinstance(cb, ModelCheckpoint):\n",
    "                ckpt_callback = cb\n",
    "                break\n",
    "        assert ckpt_callback is not None, \"ModelCheckpoint not found in callbacks.\"\n",
    "        best_model_path = ckpt_callback.best_model_path\n",
    "\n",
    "        print(f\"Loading best model at {best_model_path}\")\n",
    "        label_encoder = pl_module.model.label_encoder\n",
    "        model = LitOrigamiNet.load_from_checkpoint(\n",
    "            best_model_path,\n",
    "            label_encoder=label_encoder,\n",
    "        )\n",
    "        trainer.model.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class LogModelPredictions(Callback):\n",
    "    \"\"\"\n",
    "    Use a fixed test batch to monitor model predictions at the end of every epoch.\n",
    "\n",
    "    Specifically: it generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's prediction alongside the actual target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_encoder: LabelParser,\n",
    "        val_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "        use_gpu: bool = True,\n",
    "        data_format: str = \"word\",\n",
    "        train_batch: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    ):\n",
    "        self.label_encoder = label_encoder\n",
    "        self.val_batch = val_batch\n",
    "        self.use_gpu = use_gpu\n",
    "        self.data_format = data_format\n",
    "        self.train_batch = train_batch\n",
    "\n",
    "    def on_validation_epoch_end(\n",
    "        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n",
    "    ):\n",
    "        self._predict_intermediate(trainer, pl_module, split=\"val\")\n",
    "\n",
    "    def on_train_epoch_end(\n",
    "        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n",
    "    ):\n",
    "        if self.train_batch is not None:\n",
    "            self._predict_intermediate(trainer, pl_module, split=\"train\")\n",
    "\n",
    "    def _predict_intermediate(\n",
    "        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", split=\"val\"\n",
    "    ):\n",
    "        \"\"\"Make predictions on a fixed batch of data and log the results to Tensorboard.\"\"\"\n",
    "\n",
    "        # Make predictions.\n",
    "        if split == \"train\":\n",
    "            imgs, targets = self.train_batch\n",
    "        else:  # split == \"val\"\n",
    "            imgs, targets = self.val_batch\n",
    "        with torch.inference_mode():\n",
    "            pl_module.eval()\n",
    "            _, preds, _ = pl_module(imgs.cuda() if self.use_gpu else imgs)\n",
    "\n",
    "        # Decode predictions and generate a plot.\n",
    "        fig = plt.figure(figsize=(100, 100))\n",
    "        for i, (p, t) in enumerate(zip(preds, targets)):\n",
    "            pred_str, target_str = decode_prediction_and_target(\n",
    "                p, t, self.label_encoder\n",
    "            )\n",
    "\n",
    "            # Create plot.\n",
    "            ncols = 2 if self.data_format == \"word\" else 1\n",
    "            nrows = math.ceil(preds.size(0) / ncols)\n",
    "            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n",
    "            matplotlib_imshow(imgs[i], IAMDataset.MEAN, IAMDataset.STD)\n",
    "            ax.set_title(f\"Pred: {pred_str}\\nTarget: {target_str}\")\n",
    "\n",
    "\n",
    "        trainer.logger.experiment.log({f\"{split}: predictions vs targets\": wandb.Image(fig)})\n",
    "        plt.close(fig)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88db9abcad325ee7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 12345\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "from copy import copy\n",
    "\n",
    "seed_everything(12345)\n",
    "ds = IAMDataset(root=\"/Users/tefannastasa/BachelorsWorkspace/BachModels/BachModels/data/raw/IAM\", label_enc=None, parse_method=\"form\" ,split=\"train\")\n",
    "ds_train, ds_val = torch.utils.data.random_split(ds, [math.ceil(0.8 * len(ds)), math.floor(0.2 * len(ds))])\n",
    "\n",
    "ds_val.data = copy(ds)\n",
    "ds_val.data.set_transforms_for_split(\"val\")\n",
    "train_len = len(ds_train)\n",
    "val_len = len(ds_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:22.343984Z",
     "start_time": "2024-05-16T11:42:20.764696Z"
    }
   },
   "id": "97b1d47a0f8a25fb",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 2\n",
    "\n",
    "collate_fn = partial(\n",
    "        IAMDataset.collate_fn\n",
    ")\n",
    "num_workers = 4\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=2 * batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_len //= batch_size\n",
    "val_len //= 2 * batch_size"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8818cb03730ac33c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ModelSummary\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=\"0350b0cc5bd9521bb37a798168d31b6b65e9caca\")\n",
    "wandb_logger = WandbLogger(project=\"bach_thesis\", log_model=\"all\")\n",
    "callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_top_k=(3),\n",
    "            mode=\"min\",\n",
    "            monitor=\"word_error_rate\",\n",
    "            filename=\"{epoch}-{char_error_rate:.4f}-{word_error_rate:.4f}\",\n",
    "        ),\n",
    "        ModelSummary(max_depth=2),\n",
    "        LitProgressBar(),\n",
    "        LogWorstPredictions(\n",
    "            dl_train,\n",
    "            dl_val,\n",
    "            training_skipped=False,\n",
    "            data_format=\"form\",\n",
    "        ),\n",
    "        LogModelPredictions(\n",
    "            ds.label_enc,\n",
    "            val_batch=next(\n",
    "                iter(\n",
    "                    DataLoader(\n",
    "                        Subset(\n",
    "                            ds_val,\n",
    "                            random.sample(\n",
    "                                range(len(ds_val)), 1\n",
    "                            ),\n",
    "                        ),\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_fn,\n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=True,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            train_batch=next(\n",
    "                iter(\n",
    "                    DataLoader(\n",
    "                        Subset(\n",
    "                            ds_train,\n",
    "                            random.sample(\n",
    "                                range(len(ds_train)),\n",
    "                                1,\n",
    "                            ),\n",
    "                        ),\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_fn,\n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=True,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            data_format=\"form\",\n",
    "            use_gpu=True,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "                    monitor=\"word_error_rate\",\n",
    "                    patience=50,\n",
    "                    verbose=True,\n",
    "                    mode=\"min\",\n",
    "                    check_on_train_epoch_end=False,\n",
    "                )\n",
    "    ]\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=3000,\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1,\n",
    "    callbacks = callbacks,\n",
    "    logger=wandb_logger,\n",
    "    fast_dev_run=False)\n",
    "model = LitOrigamiNet(\n",
    "    n_channels = 1, \n",
    "    label_encoder=ds.label_enc,\n",
    "    mul_rate= 1.0, \n",
    "    layer_resizes= {\n",
    "            0: nn.MaxPool2d(2, 2),\n",
    "            2: nn.MaxPool2d(2, 2),\n",
    "            4: nn.MaxPool2d(2,2),\n",
    "            6: nn.ZeroPad2d(1),\n",
    "            8: nn.ZeroPad2d(1),\n",
    "            10: nn.Upsample((450, 15), align_corners=True, mode=\"bilinear\"),\n",
    "            11: nn.Upsample((1100, 8), align_corners=True, mode=\"bilinear\")\n",
    "        },\n",
    "    layer_sizes= {\n",
    "            0:  128,\n",
    "            2:  256,\n",
    "            4:  512,\n",
    "            11: 256\n",
    "        }, \n",
    "    num_layers=12, \n",
    "    fup=33,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "try:\n",
    "    # Start training\n",
    "    trainer.fit(model, dl_train, dl_val)\n",
    "except Exception as e:\n",
    "    # Catch and print any exceptions during training\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    # Ensure wandb is properly closed\n",
    "    wandb.finish()\n",
    "\n",
    "    # Cleanup to free GPU memory, if the objects are not needed anymore\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training finished, resources cleared.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c266cdde6870a0ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

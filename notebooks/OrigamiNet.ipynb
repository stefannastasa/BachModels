{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "875cda80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:07.233412Z",
     "iopub.status.busy": "2024-05-18T12:35:07.232459Z",
     "iopub.status.idle": "2024-05-18T12:35:21.024183Z",
     "shell.execute_reply": "2024-05-18T12:35:21.023005Z"
    },
    "papermill": {
     "duration": 13.803018,
     "end_time": "2024-05-18T12:35:21.026731",
     "exception": false,
     "start_time": "2024-05-18T12:35:07.223713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting editdistance\r\n",
      "  Downloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\r\n",
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\r\n",
      "Requirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\r\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.66.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (6.0.1)\r\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.9.0)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\r\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\r\n",
      "Downloading editdistance-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: editdistance\r\n",
      "Successfully installed editdistance-0.8.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install editdistance torchmetrics pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accdfe52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.695236Z",
     "start_time": "2024-05-16T11:42:16.322123Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:21.045480Z",
     "iopub.status.busy": "2024-05-18T12:35:21.045151Z",
     "iopub.status.idle": "2024-05-18T12:35:32.883742Z",
     "shell.execute_reply": "2024-05-18T12:35:32.882752Z"
    },
    "papermill": {
     "duration": 11.850914,
     "end_time": "2024-05-18T12:35:32.886440",
     "exception": false,
     "start_time": "2024-05-18T12:35:21.035526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from random import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import cv2 as cv\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from random import randint\n",
    "import html\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, Dict, Sequence, Optional, List, Any, Callable, Optional\n",
    "import pandas as pd\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchmetrics import Metric\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset\n",
    "import editdistance\n",
    "import wandb\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc88cf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.700937Z",
     "start_time": "2024-05-16T11:42:20.696431Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:32.905097Z",
     "iopub.status.busy": "2024-05-18T12:35:32.904785Z",
     "iopub.status.idle": "2024-05-18T12:35:32.916124Z",
     "shell.execute_reply": "2024-05-18T12:35:32.915334Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.022137,
     "end_time": "2024-05-18T12:35:32.917984",
     "exception": false,
     "start_time": "2024-05-18T12:35:32.895847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelParser:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.vocab_size = None\n",
    "        self.class_to_idx = None\n",
    "        self.idx_to_class = None\n",
    "        self.ctc_classes = None\n",
    "        self.ctc_idx_to_class = None\n",
    "        self.ctc_class_to_idx = None\n",
    "        \n",
    "    def fit(self, classes: Sequence[str]):\n",
    "        self.classes = list(classes)\n",
    "        self.vocab_size = len(classes)\n",
    "        self.idx_to_class = dict(enumerate(classes))\n",
    "        self.class_to_idx = {cls: i for i, cls in self.idx_to_class.items()}\n",
    "        \n",
    "        self.ctc_classes = [\"<blank>\"] + self.classes\n",
    "        self.ctc_idx_to_class = dict(enumerate(self.ctc_classes))\n",
    "        self.ctc_class_to_idx = {cls: i for i, cls in self.ctc_idx_to_class.items()}\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def addClasses(self, classes: List[str]):\n",
    "        all_classes = sorted(set(self.classes + classes))\n",
    "\n",
    "        self.fit(all_classes)\n",
    "\n",
    "    def encode_labels(self, sequence: Sequence[str]):\n",
    "        self._check_fitted()\n",
    "        return [self.class_to_idx[c] for c in sequence]\n",
    "\n",
    "    def decode_labels(self, sequence: Sequence[int]):\n",
    "        self._check_fitted()\n",
    "        return [self.idx_to_class[c] for c in sequence]\n",
    "    \n",
    "    def ctc_encode_labels(self, sequence: Sequence[str]):\n",
    "        self._check_fitted()\n",
    "        return [self.ctc_class_to_idx[c] for c in sequence]\n",
    "    \n",
    "    def ctc_decode_labels(self, sequence: Sequence[int]):\n",
    "        self._check_fitted()\n",
    "        return [self.ctc_idx_to_class[c] for c in sequence]\n",
    "    \n",
    "    def _check_fitted(self):\n",
    "        if self.classes is None:\n",
    "            raise ValueError(\"LabelParser class was not fitted yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c399910",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.708328Z",
     "start_time": "2024-05-16T11:42:20.701808Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:32.934992Z",
     "iopub.status.busy": "2024-05-18T12:35:32.934717Z",
     "iopub.status.idle": "2024-05-18T12:35:32.949396Z",
     "shell.execute_reply": "2024-05-18T12:35:32.948526Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.025452,
     "end_time": "2024-05-18T12:35:32.951326",
     "exception": false,
     "start_time": "2024-05-18T12:35:32.925874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pickle_load(file) -> Any:\n",
    "    with open(file, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def pickle_save(obj, file):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def read_xml(file: Union[Path, str]) -> ET.Element:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    return root\n",
    "\n",
    "def find_child_by_tag(element: ET.Element, tag: str, value: str) -> Union[ET.Element, None]:\n",
    "    for child in element:\n",
    "        if child.get(tag) == value:\n",
    "            return child\n",
    "    return None\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def dpi_adjusting(img: np.ndarray, scale: float, **kwargs) -> np.ndarray:\n",
    "    height, width = img.shape[:2]\n",
    "    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n",
    "    return cv.resize(img, (new_width, new_height))\n",
    "\n",
    "class LitProgressBar(TQDMProgressBar):\n",
    "    def get_metrics(self, trainer, model):\n",
    "        # don't show the version number\n",
    "        items = super().get_metrics(trainer, model)\n",
    "        for k in list(items.keys()):\n",
    "            if k.startswith(\"grad\"):\n",
    "                items.pop(k, None)\n",
    "        items.pop(\"v_num\", None)\n",
    "        return items\n",
    "    \n",
    "def decode_prediction_and_target(\n",
    "    pred: Tensor, target: Tensor, label_encoder: LabelParser\n",
    ") -> Tuple[str, str]:\n",
    "\n",
    "    # Decode prediction and target.\n",
    "    p, t = pred.tolist(), target.tolist()\n",
    "    pred_str = \"\".join(label_encoder.ctc_decode_labels(p))\n",
    "    target_str = \"\".join(label_encoder.ctc_decode_labels(t))\n",
    "    return pred_str, target_str\n",
    "\n",
    "def matplotlib_imshow(\n",
    "    img: torch.Tensor, mean: float = 0.5, std: float = 0.5, one_channel=True\n",
    "):\n",
    "    assert img.device.type == \"cpu\"\n",
    "    if one_channel and img.ndim == 3:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img * std + mean  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9299d032",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.716503Z",
     "start_time": "2024-05-16T11:42:20.708975Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:32.968926Z",
     "iopub.status.busy": "2024-05-18T12:35:32.968660Z",
     "iopub.status.idle": "2024-05-18T12:35:32.985945Z",
     "shell.execute_reply": "2024-05-18T12:35:32.985168Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.028413,
     "end_time": "2024-05-18T12:35:32.987757",
     "exception": false,
     "start_time": "2024-05-18T12:35:32.959344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SafeRandomScale(A.RandomScale):\n",
    "    def apply(self, img, scale=0, interpolation=cv.INTER_LINEAR, **params):\n",
    "        height, width = img.shape[:2]\n",
    "        new_height, new_width = int(height * scale), int(width * scale)\n",
    "        if new_height <= 0 or new_width <= 0:\n",
    "            return img\n",
    "        return super().apply(img, scale, interpolation, **params)\n",
    "\n",
    "def adjust_dpi(img: np.ndarray, scale: float, **kwargs):\n",
    "    height, width = img.shape\n",
    "    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n",
    "    return cv.resize(img, (new_width, new_height))\n",
    "\n",
    "def randomly_displace_and_pad(\n",
    "    img: np.ndarray,\n",
    "    padded_size: Tuple[int, int],\n",
    "    crop_if_necessary: bool = False,\n",
    "    **kwargs,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly displace an image within a frame, and pad zeros around the image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): image to process\n",
    "        padded_size (Tuple[int, int]): (height, width) tuple indicating the size of the frame\n",
    "        crop_if_necessary (bool): whether to crop the image if its size exceeds that\n",
    "            of the frame\n",
    "    \"\"\"\n",
    "    frame_h, frame_w = padded_size\n",
    "    img_h, img_w = img.shape\n",
    "    if frame_h < img_h or frame_w < img_w:\n",
    "        if crop_if_necessary:\n",
    "            print(\n",
    "                \"WARNING (`randomly_displace_and_pad`): cropping input image before \"\n",
    "                \"padding because it exceeds the size of the frame.\"\n",
    "            )\n",
    "            img_h, img_w = min(img_h, frame_h), min(img_w, frame_w)\n",
    "            img = img[:img_h, :img_w]\n",
    "        else:\n",
    "            raise AssertionError(\n",
    "                f\"Frame is smaller than the image: ({frame_h}, {frame_w}) vs. ({img_h},\"\n",
    "                f\" {img_w})\"\n",
    "            )\n",
    "\n",
    "    res = np.zeros((frame_h, frame_w), dtype=img.dtype)\n",
    "\n",
    "    pad_top =  randint(0, frame_h - img_h)\n",
    "    pad_bottom = pad_top + img_h\n",
    "    pad_left = randint(0, frame_w - img_w)\n",
    "    pad_right = pad_left + img_w\n",
    "\n",
    "    res[pad_top:pad_bottom, pad_left:pad_right] = img\n",
    "    return res\n",
    "\n",
    "@dataclass\n",
    "class ImageTransforms:\n",
    "    max_img_size: Tuple[int, int]  # (h, w)\n",
    "    normalize_params: Tuple[float, float]  # (mean, std)\n",
    "    scale: float = (\n",
    "        0.25\n",
    "    )\n",
    "    random_scale_limit: float = 0.1\n",
    "    random_rotate_limit: int = 1\n",
    "\n",
    "    train_trnsf: A.Compose = field(init=False)\n",
    "    test_trnsf: A.Compose = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        scale, random_scale_limit, random_rotate_limit, normalize_params =(\n",
    "            self.scale,\n",
    "            self.random_scale_limit,\n",
    "            self.random_rotate_limit,\n",
    "            self.normalize_params\n",
    "        )\n",
    "\n",
    "        max_img_h, max_img_w = self.max_img_size\n",
    "        max_scale = scale + scale * random_scale_limit\n",
    "        padded_h, padded_w = math.ceil(max_scale * max_img_h), math.ceil(max_scale * max_img_w)\n",
    "\n",
    "        self.train_trnsf = A.Compose([\n",
    "            A.Lambda(partial(adjust_dpi, scale=scale)),\n",
    "            SafeRandomScale(scale_limit=random_scale_limit, p=0.5),\n",
    "#             A.SafeRotate(\n",
    "#                 limit = random_rotate_limit,\n",
    "#                 border_mode = cv.BORDER_CONSTANT,\n",
    "#                 value = 0\n",
    "#             ),\n",
    "            A.RandomBrightnessContrast(),\n",
    "#             A.Perspective(scale=(0.01, 0.05)),\n",
    "            A.GaussNoise(),\n",
    "            A.Normalize(*normalize_params),\n",
    "#             A.Lambda(\n",
    "#                 image=partial(\n",
    "#                     randomly_displace_and_pad,\n",
    "#                     padded_size=(padded_h, padded_w),\n",
    "#                     crop_if_necessary=False,\n",
    "#                 )\n",
    "#             )\n",
    "        ])\n",
    "\n",
    "        self.test_trnsf = A.Compose([\n",
    "            A.Lambda(partial(adjust_dpi, scale=scale)),\n",
    "            A.Normalize(*normalize_params),\n",
    "            A.PadIfNeeded(\n",
    "                max_img_h, max_img_w, border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f40bbbd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:44:10.989326Z",
     "start_time": "2024-05-16T11:44:10.961990Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:33.004754Z",
     "iopub.status.busy": "2024-05-18T12:35:33.004476Z",
     "iopub.status.idle": "2024-05-18T12:35:33.018005Z",
     "shell.execute_reply": "2024-05-18T12:35:33.017155Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.024301,
     "end_time": "2024-05-18T12:35:33.019963",
     "exception": false,
     "start_time": "2024-05-18T12:35:32.995662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CharacterErrorRate(Metric):\n",
    "\n",
    "    def __init__(self, label_encoder: LabelParser):\n",
    "        super().__init__()\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        self.add_state(\"cer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"nr_samples\",default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.ndim == target.ndim\n",
    "\n",
    "        for i, (p, t) in enumerate(zip(preds, target)):\n",
    "            p_str, t_str = map(tensor_to_str, (p, t))\n",
    "            editd = editdistance.eval(p_str, t_str)\n",
    "\n",
    "            self.cer_sum += editd/t.numel()\n",
    "            self.nr_samples +=1\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.cer_sum / self.nr_samples.float()\n",
    "\n",
    "class WordErrorRate(Metric):\n",
    "    def __init__(self, label_encoder: LabelParser):\n",
    "        super().__init__()\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        self.add_state(\"wer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"nr_samples\", default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n",
    "\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.ndim == target.ndim\n",
    "\n",
    "        for i, (p, t) in enumerate(zip(preds, target)):\n",
    "            p = p.flatten().tolist()\n",
    "            t = t.flatten().tolist()\n",
    "            p_words = \"\".join(self.label_encoder.ctc_decode_labels(p)).split()\n",
    "            t_words = \"\".join(self.label_encoder.ctc_decode_labels(t)).split()\n",
    "            editd = editdistance.eval(p_words, t_words)\n",
    "            \n",
    "            self.wer_sum += editd / len(t_words)\n",
    "            self.nr_samples += 1\n",
    "            \n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Compute Word Error Rate.\"\"\"\n",
    "        return self.wer_sum / self.nr_samples.float()\n",
    "\n",
    "def tensor_to_str(t: torch.Tensor) -> str:\n",
    "    return \"\".join(map(str, t.flatten().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a10425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.732251Z",
     "start_time": "2024-05-16T11:42:20.717929Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:33.037285Z",
     "iopub.status.busy": "2024-05-18T12:35:33.037034Z",
     "iopub.status.idle": "2024-05-18T12:35:33.069898Z",
     "shell.execute_reply": "2024-05-18T12:35:33.069167Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.043841,
     "end_time": "2024-05-18T12:35:33.071824",
     "exception": false,
     "start_time": "2024-05-18T12:35:33.027983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IAMDataset(Dataset):\n",
    "    MEAN = 0.8275\n",
    "    STD = 0.2314\n",
    "    MAX_FORM_HEIGHT = 3542\n",
    "    MAX_FORM_WIDTH = 2479\n",
    "\n",
    "    MAX_SEQ_LENS = {\n",
    "        \"word\": 55,\n",
    "        \"line\": 90,\n",
    "        \"form\": 700,\n",
    "    }  # based on the maximum seq lengths found in the dataset\n",
    "\n",
    "    root: Path\n",
    "    data: pd.DataFrame\n",
    "    label_enc: LabelParser\n",
    "    parse_method: str\n",
    "    only_lowercase: bool\n",
    "    transforms: Optional[A.Compose]\n",
    "    id_to_idx: Dict[str, int]\n",
    "    _split: str\n",
    "    _return_writer_id: Optional[bool]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[Path, str],\n",
    "        parse_method: str,\n",
    "        split: str,\n",
    "        return_writer_id: bool = False,\n",
    "        only_lowercase: bool = False,\n",
    "        label_enc: Optional[LabelParser] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _parse_methods = [\"form\", \"line\", \"word\"]\n",
    "        err_message = (\n",
    "            f\"{parse_method} is not a possible parsing method: {_parse_methods}\"\n",
    "        )\n",
    "        assert parse_method in _parse_methods, err_message\n",
    "\n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "\n",
    "        self._split = split\n",
    "        self._return_writer_id = return_writer_id\n",
    "        self.only_lowercase = only_lowercase\n",
    "        self.root = Path(root)\n",
    "        self.label_enc = label_enc\n",
    "        self.parse_method = parse_method\n",
    "\n",
    "        # Process the data.\n",
    "        if not hasattr(self, \"data\"):\n",
    "            self.data = self._get_forms()\n",
    "\n",
    "        # Create the label encoder.\n",
    "        if self.label_enc is None:\n",
    "            vocab = []\n",
    "            s = \"\".join(self.data[\"target\"].tolist())\n",
    "\n",
    "            if self.only_lowercase:\n",
    "                s = s.lower()\n",
    "            vocab += sorted(list(set(s)))\n",
    "            self.label_enc = LabelParser().fit(vocab)\n",
    "        if not \"target_enc\" in self.data.columns:\n",
    "            self.data.insert(\n",
    "                2,\n",
    "                \"target_enc\",\n",
    "                self.data[\"target\"].apply(\n",
    "                    lambda s: np.array(\n",
    "                        self.label_enc.encode_labels(\n",
    "                            [c for c in (s.lower() if self.only_lowercase else s)]\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.transforms = self._get_transforms(split)\n",
    "        self.id_to_idx = {\n",
    "            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n",
    "        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n",
    "            # Crop the image vertically.\n",
    "            img = img[data[\"bb_y_start\"] : data[\"bb_y_end\"], :]\n",
    "        assert isinstance(img, np.ndarray), (\n",
    "            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n",
    "            f\"Is there something wrong with this image?\"\n",
    "        )\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        if self._return_writer_id:\n",
    "            return img, data[\"writer_id\"], data[\"target_enc\"]\n",
    "        return img, data[\"target_enc\"]\n",
    "\n",
    "    def get_max_height(self):\n",
    "        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.label_enc.classes\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(\n",
    "        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n",
    "        dataset_returns_writer_id: bool = False,\n",
    "    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n",
    "        if dataset_returns_writer_id:\n",
    "            imgs, writer_ids, targets = zip(*batch)\n",
    "        else:\n",
    "            imgs, targets = zip(*batch)\n",
    "\n",
    "        img_sizes = [im.shape for im in imgs]\n",
    "        if (\n",
    "            not len(set(img_sizes)) == 1\n",
    "        ):  # images are of varying sizes, so pad them to the maximum size in the batch\n",
    "            hs, ws = zip(*img_sizes)\n",
    "            pad_fn = A.PadIfNeeded(\n",
    "                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "        imgs, targets = torch.Tensor(imgs), torch.Tensor(targets)\n",
    "        return imgs, targets\n",
    "\n",
    "    def set_transforms_for_split(self, split: str):\n",
    "        _splits = [\"train\", \"val\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        self.transforms = self._get_transforms(split)\n",
    "\n",
    "    def _get_transforms(self, split: str) -> A.Compose:\n",
    "        max_img_w = self.MAX_FORM_WIDTH\n",
    "\n",
    "        if self.parse_method == \"form\":\n",
    "            max_img_h = (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n",
    "        else:  # word or line\n",
    "            max_img_h = self.MAX_FORM_HEIGHT\n",
    "\n",
    "        transforms = ImageTransforms(\n",
    "            (max_img_h, max_img_w), (IAMDataset.MEAN, IAMDataset.STD)\n",
    "        )\n",
    "\n",
    "        if split == \"train\":\n",
    "            return transforms.train_trnsf\n",
    "        elif split == \"test\" or split == \"val\":\n",
    "            return transforms.test_trnsf\n",
    "\n",
    "    def statistics(self) -> Dict[str, float]:\n",
    "        assert len(self) > 0\n",
    "        tmp = self.transforms\n",
    "        self.transforms = None\n",
    "        mean, std, cnt = 0, 0, 0\n",
    "        for img, _ in self:\n",
    "            mean += np.mean(img)\n",
    "            std += np.var(img)\n",
    "            cnt += 1\n",
    "        mean /= cnt\n",
    "        std = np.sqrt(std / cnt)\n",
    "        self.transforms = tmp\n",
    "        return {\"mean\": mean, \"std\": std}\n",
    "    \n",
    "    def get_max_target_len(self):\n",
    "        return (self.data[\"target_len\"]).max()\n",
    "    \n",
    "    def _get_forms(self) -> pd.DataFrame:\n",
    "        \n",
    "        data = {\n",
    "            \"img_path\": [],\n",
    "            \"img_id\": [],\n",
    "            \"target\": [],\n",
    "            \"bb_y_start\": [],\n",
    "            \"bb_y_end\": [],\n",
    "            \"target_len\": [],\n",
    "        }\n",
    "        for form_dir in [\"formsA-D\", \"formsE-H\", \"formsI-Z\"]:\n",
    "            dr = self.root / form_dir\n",
    "            for img_path in dr.iterdir():\n",
    "                doc_id = img_path.stem\n",
    "                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n",
    "\n",
    "                bb_y_start = int(xml_root[1][0].get(\"asy\")) - 150\n",
    "                bb_y_end = int(xml_root[1][-1].get(\"dsy\")) + 150\n",
    "\n",
    "                form_text = []\n",
    "                for line in xml_root.iter(\"line\"):\n",
    "                    form_text.append(html.unescape(line.get(\"text\", \"\")))\n",
    "                \n",
    "                img_w, img_h = Image.open(str(img_path)).size\n",
    "                target = \" \".join(form_text)\n",
    "                target = target.replace(\"\\n\", \" \")\n",
    "                data[\"img_path\"].append(str(img_path))\n",
    "                data[\"img_id\"].append(doc_id)\n",
    "                data[\"target\"].append(target)\n",
    "                data[\"bb_y_start\"].append(bb_y_start)\n",
    "                data[\"bb_y_end\"].append(bb_y_end)\n",
    "                data[\"target_len\"].append(len(target))\n",
    "        return pd.DataFrame(data).sort_values(\n",
    "            \"target_len\"\n",
    "        )  # by default, sort by target length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54aa872b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.749373Z",
     "start_time": "2024-05-16T11:42:20.733039Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:33.089898Z",
     "iopub.status.busy": "2024-05-18T12:35:33.089618Z",
     "iopub.status.idle": "2024-05-18T12:35:33.131093Z",
     "shell.execute_reply": "2024-05-18T12:35:33.130382Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.05284,
     "end_time": "2024-05-18T12:35:33.132894",
     "exception": false,
     "start_time": "2024-05-18T12:35:33.080054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "class RIMESDataset(Dataset):\n",
    "    MEAN = 0.8275\n",
    "    STD = 0.2314\n",
    "    \n",
    "    root: Path\n",
    "    data: pd.DataFrame\n",
    "    label_enc: LabelParser\n",
    "    transforms: Optional[A.Compose]\n",
    "    id_to_idx: Dict[str, int]\n",
    "    _split: str\n",
    "    _return_writer_id: Optional[bool]\n",
    "    \n",
    "    max_width: Optional[int]\n",
    "    max_height: Optional[int]\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_target(target: str):\n",
    "        # Splitting the input string into lines\n",
    "        target = target.replace(\"\\\\n\", \"\\n\")\n",
    "        target = target.replace(\"\\n\", \" \")\n",
    "        \n",
    "        pattern = re.compile(r'¤?\\{?([^ ¤{}0-9]*)\\/([^ ¤{}0-9]*)\\}?¤?')\n",
    "        matches = pattern.findall(target)\n",
    "        match_pos = [a for a in pattern.finditer(target)]\n",
    "        new_target = \"\"\n",
    "        last_ind = 0\n",
    "\n",
    "        for i, (choices, position) in enumerate(zip(matches, match_pos)):\n",
    "            if position.start() > 0:\n",
    "                new_target += target[last_ind:position.start()-1]\n",
    "            if len(choices) != 0:\n",
    "                new_target += \" \" + choices[random.randint(0, len(choices) - 1)] + \" \"\n",
    "            last_ind = position.end() + 1\n",
    "        \n",
    "        new_target += target[last_ind:]\n",
    "        \n",
    "        return new_target\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            root: Union[Path, str],\n",
    "            split: str, \n",
    "            only_lowercase: bool = False,\n",
    "            label_enc: Optional[LabelParser] = None,):\n",
    "        super().__init__()\n",
    "        \n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        \n",
    "        self._split = split\n",
    "        self.only_lowercase = only_lowercase\n",
    "        self.root = Path(root)\n",
    "        self.label_enc = label_enc\n",
    "        \n",
    "        if not hasattr(self, \"data\"):\n",
    "            self.data = self._get_form_data()\n",
    "        \n",
    "        if self.label_enc is None:\n",
    "            vocab = []\n",
    "            s = \"\".join(self.data[\"target\"].tolist())\n",
    "            if self.only_lowercase:\n",
    "                s = s.lower()\n",
    "            vocab += sorted(list(set(s)))\n",
    "            self.label_enc = LabelParser().fit(vocab)\n",
    "        else:\n",
    "            vocab = []\n",
    "            s = \"\".join(self.data[\"target\"].tolist())\n",
    "            if self.only_lowercase:\n",
    "                s = s.lower()\n",
    "            vocab += sorted(list(set(s)))\n",
    "            self.label_enc.addClasses(vocab)\n",
    "        \n",
    "        if not \"target_enc\" in self.data.columns:\n",
    "            self.data.insert(\n",
    "                2,\n",
    "                \"target_enc\",\n",
    "                self.data[\"target\"].apply(\n",
    "                    lambda s: np.array(\n",
    "                        self.label_enc.ctc_encode_labels(\n",
    "                            [c for c in (s.lower() if self.only_lowercase else s)]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.transforms = self._get_transforms(split)\n",
    "        self.id_to_idx = {\n",
    "            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def set_transform_for_split(self, split):\n",
    "        _splits = [\"train\", \"val\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        self.transforms = self._get_transforms(split)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n",
    "            img = img[data[\"bb_y_start\"]: data[\"bb_y_end\"], :]\n",
    "        assert isinstance(img, np.ndarray), (\n",
    "            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n",
    "            f\"Is there something wrong with this image?\"\n",
    "        )\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        \n",
    "        return img, data[\"target_enc\"]\n",
    "    \n",
    "    def get_max_height(self):\n",
    "        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max() + 150\n",
    "    \n",
    "    def get_max_width(self):\n",
    "        return (self.data[\"bb_x_end\"] - self.data[\"bb_x_start\"]).max() + 150\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.label_enc.classes\n",
    "        \n",
    "    @staticmethod\n",
    "    def collate_fn(\n",
    "        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n",
    "    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n",
    "        \n",
    "        imgs, targets = zip(*batch)\n",
    "\n",
    "        img_sizes = [im.shape for im in imgs]\n",
    "        if not len(set(img_sizes)) == 1:\n",
    "            hs, ws = zip(*img_sizes)\n",
    "            pad_fn = A.PadIfNeeded(\n",
    "                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "        imgs, targets = torch.Tensor(imgs), torch.Tensor(targets)\n",
    "        return imgs, targets\n",
    "    \n",
    "    def _get_transforms(self, split: str) -> A.Compose:\n",
    "        max_img_w = self.max_width\n",
    "    \n",
    "        max_img_h = self.max_height\n",
    "    \n",
    "        transforms = ImageTransforms(\n",
    "            (max_img_h, max_img_w), (RIMESDataset.MEAN, RIMESDataset.STD)\n",
    "        )\n",
    "    \n",
    "        if split == \"train\":\n",
    "            return transforms.train_trnsf\n",
    "        elif split == \"test\" or split == \"val\":\n",
    "            return transforms.test_trnsf\n",
    "    \n",
    "    def get_max_target_len(self):\n",
    "        return (self.data[\"target_len\"]).max()\n",
    "\n",
    "    def _get_form_data(self):\n",
    "        data = {\n",
    "            \"img_path\": [],\n",
    "            \"img_id\": [],\n",
    "            \"target\": [],\n",
    "            \"bb_y_start\": [],\n",
    "            \"bb_y_end\": [],\n",
    "            \"bb_x_start\": [],\n",
    "            \"bb_x_end\": [],\n",
    "            \"target_len\": [],\n",
    "        }\n",
    "        \n",
    "        \n",
    "        def process_forms(paths: Tuple[str, str, Path]):\n",
    "            return_data = {\n",
    "                \"img_path\": [],\n",
    "                \"img_id\": [],\n",
    "                \"target\": [],\n",
    "                \"bb_y_start\": [],\n",
    "                \"bb_y_end\": [],\n",
    "                \"bb_x_start\": [],\n",
    "                \"bb_x_end\": [],\n",
    "                \"target_len\": []\n",
    "            }\n",
    "            img_path, xml_path, root = paths\n",
    "            img_path = root / img_path\n",
    "            xml_path = root / xml_path\n",
    "            doc_id = img_path.stem[:-2]\n",
    "            xml_root = read_xml(xml_path)\n",
    "            \n",
    "            target = \"\"\n",
    "            num_corps = 0\n",
    "            for box in xml_root.iter(\"box\"):\n",
    "                type_tag =  box.find(\"type\")\n",
    "                if type_tag.text == \"Corps de texte\":\n",
    "                    target = box.find(\"text\").text\n",
    "                    \n",
    "                    \n",
    "                    if target is None or target == \"\":\n",
    "                        continue\n",
    "                    words = target.split(\"\\\\n\")\n",
    "                    if len(words) <= 5:\n",
    "                        continue\n",
    "\n",
    "                    target = self.process_target(target)\n",
    "                    if target.find(\"¤\") != -1 or target.find(\"{\") != -1 or target.find(\"}\") != -1:\n",
    "                        continue #skip if the target sequence is not standard\n",
    "                        \n",
    "                    return_data[\"img_path\"].append(str(img_path.resolve()))\n",
    "                    return_data[\"img_id\"].append(doc_id)\n",
    "                    return_data[\"target\"].append(target)\n",
    "                    return_data[\"bb_y_start\"].append(int(box.get(\"top_left_y\")))\n",
    "                    return_data[\"bb_y_end\"].append(int(box.get(\"bottom_right_y\")))\n",
    "                    return_data[\"bb_x_start\"].append(int(box.get(\"top_left_x\")))\n",
    "                    return_data[\"bb_x_end\"].append(int(box.get(\"bottom_right_x\")))\n",
    "                    return_data[\"target_len\"].append(len(target))\n",
    "                    num_corps += 1\n",
    "            \n",
    "            return return_data\n",
    "        \n",
    "        image_pairs = []\n",
    "        for form_dir in [\"DVD1_TIF\", \"DVD2_TIF\", \"DVD3_TIF\"]:\n",
    "            dr = self.root / form_dir\n",
    "            for file in dr.iterdir():\n",
    "                name = file.stem\n",
    "                ext = file.suffix\n",
    "                if ext == \".tif\" and name[-1] == \"L\":\n",
    "                    image_pairs.append((name + \".tif\", name + \".xml\", dr))\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(process_forms, iter(image_pairs)))\n",
    "            \n",
    "            for single_results in results:\n",
    "                if single_results[\"img_path\"] == \"\":\n",
    "                    continue\n",
    "                data[\"img_path\"].extend(single_results[\"img_path\"])\n",
    "                data[\"img_id\"].extend(single_results[\"img_id\"])\n",
    "                data[\"target\"].extend(single_results[\"target\"])\n",
    "                data[\"bb_y_start\"].extend(single_results[\"bb_y_start\"])\n",
    "                data[\"bb_y_end\"].extend(single_results[\"bb_y_end\"])\n",
    "                data[\"bb_x_start\"].extend(single_results[\"bb_x_start\"])\n",
    "                data[\"bb_x_end\"].extend(single_results[\"bb_x_end\"])\n",
    "                data[\"target_len\"].extend(single_results[\"target_len\"])\n",
    "        \n",
    "        to_ret = pd.DataFrame(data)\n",
    "        self.max_height = (to_ret[\"bb_y_end\"] - to_ret[\"bb_y_start\"]).max() + 150\n",
    "        self.max_width = (to_ret[\"bb_x_end\"] - to_ret[\"bb_x_start\"]).max() + 150\n",
    "        \n",
    "        return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ff17473",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.754035Z",
     "start_time": "2024-05-16T11:42:20.750092Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:33.149998Z",
     "iopub.status.busy": "2024-05-18T12:35:33.149709Z",
     "iopub.status.idle": "2024-05-18T12:35:33.165808Z",
     "shell.execute_reply": "2024-05-18T12:35:33.164994Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.026733,
     "end_time": "2024-05-18T12:35:33.167622",
     "exception": false,
     "start_time": "2024-05-18T12:35:33.140889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AggregatedDataset(Dataset):\n",
    "    datasets: List[Dataset]\n",
    "    def __init__(self, rimes: RIMESDataset,\n",
    "                 iam: IAMDataset, \n",
    "                 split:str,\n",
    "                 only_lowercase: bool = False,\n",
    "                 label_enc: Optional[LabelParser] = None,):\n",
    "        super().__init__()\n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        \n",
    "        self._split = split\n",
    "        self.rimes = rimes\n",
    "        self.iam = iam\n",
    "        self._only_lowercase = only_lowercase\n",
    "        self.label_enc = label_enc\n",
    "        \n",
    "        if self.label_enc is None:\n",
    "            iamLabelEncoder = iam.label_enc\n",
    "            rimesLabelEncoder = rimes.label_enc\n",
    "            self.label_enc = LabelParser()\n",
    "            self.label_enc.addClasses(iamLabelEncoder.classes)\n",
    "            self.label_enc.addClasses(rimesLabelEncoder.classes)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.rimes) + len(self.iam)\n",
    "    \n",
    "    def get_max_target_len(self):\n",
    "        return max(self.iam.get_max_target_len(), self.rimes.get_max_target_len())\n",
    "    \n",
    "    @staticmethod\n",
    "    def unified_collate_fn(\n",
    "        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n",
    "        ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n",
    "        \n",
    "        imgs, targets = zip(*batch)\n",
    "        img_sizes = [im.shape for im in imgs]\n",
    "        if not len(set(img_sizes)) == 1:\n",
    "            hs, ws = zip(*img_sizes)\n",
    "            pad_fn = A.PadIfNeeded(\n",
    "                max(hs), max(ws), border_mode = cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n",
    "            \n",
    "        imgs = [np.expand_dims(im, axis=0) for im in imgs]\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "        \n",
    "        seq_lengths = [t.shape[0] for t in targets]\n",
    "        targets_padded = np.full((len(targets), max(seq_lengths) + 1), 0)\n",
    "        for i, t in enumerate(targets):\n",
    "            targets_padded[i, : seq_lengths[i]] = t\n",
    "        \n",
    "        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n",
    "        \n",
    "        return imgs, targets_padded\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        iam = self.iam \n",
    "        rimes = self.rimes\n",
    "        if idx < len(self.rimes):\n",
    "            img, target = rimes[idx]\n",
    "        if len(rimes) <= idx < len(rimes) + len(iam):\n",
    "            img, target = iam[idx - len(self.rimes)]\n",
    "        \n",
    "        if img is None :\n",
    "            raise ValueError(\"Image is None.\")\n",
    "        if target is None:\n",
    "            raise ValueError(\"Image is None.\")\n",
    "        \n",
    "        assert not np.any(np.isnan(img)), img\n",
    "        return img, target\n",
    "        \n",
    "        return img, target\n",
    "    def set_transforms_for_split(self, split):\n",
    "        self.iam.set_transforms_for_split(split)\n",
    "        self.rimes.set_transform_for_split(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f700fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:20.764031Z",
     "start_time": "2024-05-16T11:42:20.754686Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:33.185015Z",
     "iopub.status.busy": "2024-05-18T12:35:33.184739Z",
     "iopub.status.idle": "2024-05-18T12:35:33.209928Z",
     "shell.execute_reply": "2024-05-18T12:35:33.209084Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.036293,
     "end_time": "2024-05-18T12:35:33.211844",
     "exception": false,
     "start_time": "2024-05-18T12:35:33.175551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gpu_memory_map():\n",
    "    result = os.popen('nvidia-smi --query-gpu=memory.used --format=csv,nounits,noheader').read()\n",
    "    return int(result.strip())\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return nn.functional.layer_norm(x, x.size()[1:], weight=None, bias=None, eps=1e-05)\n",
    "\n",
    "def pCnv(inp,out,groups=1):\n",
    "  return nn.Sequential(\n",
    "      nn.Conv2d(inp,out,1,bias=False,groups=groups),\n",
    "      nn.InstanceNorm2d(out,affine=True)\n",
    "  )\n",
    "\n",
    "def dsCnv(inp,k):\n",
    "  return nn.Sequential(\n",
    "      nn.Conv2d(inp,inp,k,groups=inp,bias=False,padding=(k - 1) // 2),\n",
    "      nn.InstanceNorm2d(inp,affine=True)\n",
    "  )\n",
    "\n",
    "class InitBlock(nn.Module):\n",
    "    def __init__(self, fup, num_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n1 = LayerNorm()\n",
    "        self.InitSeq = nn.Sequential(\n",
    "            pCnv(num_channels, fup),\n",
    "            nn.Softmax(dim=1),\n",
    "            dsCnv(fup, 11),\n",
    "            LayerNorm()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x  = self.n1(x)\n",
    "        xt = x\n",
    "        x = self.InitSeq(x)\n",
    "        x = torch.cat([x, xt], dim=1)\n",
    "        return x\n",
    "\n",
    "class Gate(nn.Module):\n",
    "    def __init__(self, ifsz):\n",
    "        super().__init__()\n",
    "        self.ln = LayerNorm()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        t0, t1 = torch.chunk(x, 2, dim=1)\n",
    "        t0 = torch.tanh(t0)\n",
    "        t1.sub(2)\n",
    "        t1 = torch.sigmoid(t1)\n",
    "        \n",
    "        return t1 * t0\n",
    "\n",
    "class GateBlock(nn.Module):\n",
    "    def __init__(self, ifsz, ofsz, gt = True, ksz = 3):\n",
    "        super().__init__()\n",
    "        \n",
    "        cfsz = int(math.floor(ifsz / 2))\n",
    "        ifsz2 = ifsz + ifsz%2\n",
    "        \n",
    "        self.sq = nn.Sequential(\n",
    "            pCnv(ifsz, cfsz),\n",
    "            dsCnv(cfsz, ksz),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            pCnv(cfsz, cfsz * 2),\n",
    "            dsCnv(cfsz * 2, ksz),\n",
    "            Gate(cfsz),\n",
    "            \n",
    "            pCnv(cfsz, ifsz),\n",
    "            dsCnv(ifsz, ksz),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        self.gt = gt\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.sq(x)\n",
    "        \n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "class OrigamiNet(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_channels: int, \n",
    "                 label_enc: LabelParser, \n",
    "                 mul_rate, \n",
    "                 layer_resizes, \n",
    "                 layer_sizes, \n",
    "                 num_layers, \n",
    "                 fup, \n",
    "                 reduceAxis=3 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_resizes = layer_resizes\n",
    "        self.Init_sequence = InitBlock(fup, 1)\n",
    "        self.label_enc = label_enc\n",
    "        \n",
    "        self.cer_metric = CharacterErrorRate(label_enc)\n",
    "        self.wer_metric = WordErrorRate(label_enc)\n",
    "        \n",
    "        layers = []\n",
    "        input_size = fup + n_channels\n",
    "        output_size = input_size\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            output_size = int(math.floor(layer_sizes[i] * mul_rate) ) if i in layer_sizes else input_size\n",
    "            layers.append(GateBlock(input_size, output_size, True, 3))\n",
    "            \n",
    "            if input_size != output_size:\n",
    "                layers.append(pCnv(input_size, output_size))\n",
    "                layers.append(nn.ELU())\n",
    "            input_size = output_size\n",
    "            \n",
    "            if i in layer_resizes:\n",
    "                layers.append(layer_resizes[i])\n",
    "        \n",
    "        layers.append(LayerNorm())\n",
    "        self.Gatesq = nn.Sequential(*layers)\n",
    "        self.Finsq = nn.Sequential(\n",
    "            pCnv(output_size, self.label_enc.vocab_size),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.n1 = LayerNorm()\n",
    "        self.it = 0\n",
    "        self.reduceAxis = reduceAxis\n",
    "        self.loss_fn = nn.CTCLoss(reduction=\"none\", zero_infinity=True)\n",
    "        \n",
    "    def forward(self, image, targets: Optional[torch.Tensor]):\n",
    "        x = self.Init_sequence(image)\n",
    "        x = self.Gatesq(x)\n",
    "        x = self.Finsq(x)\n",
    "        x = torch.mean(x, self.reduceAxis, keepdim=False)\n",
    "        x = self.n1(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        if targets is not None:\n",
    "            logits = x\n",
    "            logits = logits.permute(1, 0, 2).log_softmax(2)\n",
    "            logits_size = torch.IntTensor([logits.size(0)] * targets.size(0))\n",
    "            targets_size = torch.IntTensor([targets.size(1)] * targets.size(0))\n",
    "            targets = targets.cpu()\n",
    "            loss = self.loss_fn(logits, targets, logits_size, targets_size).mean() \n",
    "            return x, loss\n",
    "        return x\n",
    "    \n",
    "    def calculate_metrics(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        self.cer_metric.reset()\n",
    "        self.wer_metric.reset()\n",
    "        \n",
    "        cer = self.cer_metric(preds, targets)\n",
    "        wer = self.wer_metric(preds, targets)\n",
    "        \n",
    "        return {\"CER\": cer, \"WER\":wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae365d38",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:33.228866Z",
     "iopub.status.busy": "2024-05-18T12:35:33.228582Z",
     "iopub.status.idle": "2024-05-18T12:35:33.241455Z",
     "shell.execute_reply": "2024-05-18T12:35:33.240686Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.023482,
     "end_time": "2024-05-18T12:35:33.243262",
     "exception": false,
     "start_time": "2024-05-18T12:35:33.219780",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitOrigamiNet(pl.LightningModule):\n",
    "    model: OrigamiNet\n",
    "\n",
    "    \"\"\"\n",
    "    Pytorch Lightning module that acting as a wrapper around the\n",
    "    FullPageHTREncoderDecoder class.\n",
    "\n",
    "    Using a PL module allows the model to be used in conjunction with a Pytorch\n",
    "    Lightning Trainer, and takes care of logging relevant metrics to Tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels: int, \n",
    "        label_encoder: LabelParser,\n",
    "        mul_rate: int, \n",
    "        layer_resizes: dict,\n",
    "        layer_sizes: dict,\n",
    "        num_layers: int,\n",
    "        fup: int,\n",
    "        reduce_axis:int = 3,\n",
    "        learning_rate: float = 0.0002,\n",
    "        params_to_log: Optional[Dict[str, Union[str, float, int]]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save hyperparameters.\n",
    "        self.learning_rate = learning_rate\n",
    "        if params_to_log is not None:\n",
    "            self.save_hyperparameters(params_to_log)\n",
    "        self.save_hyperparameters(\n",
    "            \"learning_rate\",\n",
    "            \"n_channels\",\n",
    "            \"num_layers\",\n",
    "        )\n",
    "\n",
    "        # Initialize the model.\n",
    "        self.model = OrigamiNet(\n",
    "            n_channels,\n",
    "            label_encoder,\n",
    "            mul_rate,\n",
    "            layer_resizes,\n",
    "            layer_sizes,\n",
    "            fup,\n",
    "            reduce_axis\n",
    "        )\n",
    "\n",
    "    def forward(self, imgs: Tensor, targets: Optional[Tensor] = None):\n",
    "        return self.model(imgs, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, targets = batch\n",
    "        logits, loss = self.model(imgs, targets)\n",
    "        self.log(\"train_loss\", loss, sync_dist=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.val_or_test_step(batch)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.val_or_test_step(batch)\n",
    "\n",
    "    def val_or_test_step(self, batch) -> Tensor:\n",
    "        imgs, targets = batch\n",
    "        logits, loss = self(imgs, targets)\n",
    "        _, preds = logits.max(-1)\n",
    "\n",
    "        # Update and log metrics.\n",
    "        self.model.cer_metric(preds, targets)\n",
    "        self.model.wer_metric(preds, targets)\n",
    "        self.log(\"char_error_rate\", self.model.cer_metric, on_step=True ,prog_bar=True)\n",
    "        self.log(\"word_error_rate\", self.model.wer_metric, on_step=True ,prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, sync_dist=True, prog_bar=True, on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe9e03fa",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:33.261486Z",
     "iopub.status.busy": "2024-05-18T12:35:33.261237Z",
     "iopub.status.idle": "2024-05-18T12:35:33.290336Z",
     "shell.execute_reply": "2024-05-18T12:35:33.289455Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.041046,
     "end_time": "2024-05-18T12:35:33.292202",
     "exception": false,
     "start_time": "2024-05-18T12:35:33.251156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PREDICTIONS_TO_LOG = {\n",
    "    \"word\": 10,\n",
    "    \"line\": 6,\n",
    "    \"form\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "class LogWorstPredictions(Callback):\n",
    "    \"\"\"\n",
    "    At the end of training, log the worst image prediction, meaning the predictions\n",
    "    with the highest character error rates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dataloader: Optional[DataLoader] = None,\n",
    "        val_dataloader: Optional[DataLoader] = None,\n",
    "        test_dataloader: Optional[DataLoader] = None,\n",
    "        training_skipped: bool = False,\n",
    "        data_format: str = \"word\",\n",
    "    ):\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.training_skipped = training_skipped\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if self.training_skipped and self.val_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.val_dataloader, trainer, pl_module, mode=\"val\"\n",
    "            )\n",
    "\n",
    "    def on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if self.test_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.test_dataloader, trainer, pl_module, mode=\"test\"\n",
    "            )\n",
    "\n",
    "    def on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if self.train_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.train_dataloader, trainer, pl_module, mode=\"train\"\n",
    "            )\n",
    "        if self.val_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.val_dataloader, trainer, pl_module, mode=\"val\"\n",
    "            )\n",
    "\n",
    "    def log_worst_predictions(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        trainer: \"pl.Trainer\",\n",
    "        pl_module: \"pl.LightningModule\",\n",
    "        mode: str = \"train\",\n",
    "    ):\n",
    "        img_cers = []\n",
    "        device = \"cuda:0\" if pl_module.on_gpu else \"cpu\"\n",
    "        if not self.training_skipped:\n",
    "            self._load_best_model(trainer, pl_module)\n",
    "            pl_module = trainer.model\n",
    "\n",
    "        print(f\"Running {mode} inference on best model...\")\n",
    "        # Run inference on the validation set.\n",
    "        pl_module.eval()\n",
    "        for img, target in dataloader:\n",
    "            assert target.ndim == 2, target.ndim\n",
    "            cer_metric = pl_module.model.cer_metric\n",
    "            with torch.inference_mode():\n",
    "                preds, _ = pl_module(img.to(device), target.to(device))\n",
    "#                 preds = preds.log_softmax(2)\n",
    "                preds = torch.max(preds, dim=-1)[1] # extract the predicted characters\n",
    "                for prd, tgt, im in zip(preds, target, img):\n",
    "                    cer_metric.reset()\n",
    "                    cer = cer_metric(prd.unsqueeze(0), tgt.unsqueeze(0)).item()\n",
    "                    img_cers.append((im, cer, prd, tgt))\n",
    "        \n",
    "        # Log the worst k predictions.\n",
    "        to_log = PREDICTIONS_TO_LOG[self.data_format] * 2\n",
    "        img_cers.sort(key=lambda x: x[1], reverse=True)  # sort by CER\n",
    "        img_cers = img_cers[:to_log]\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        for i, (im, cer, prd, tgt) in enumerate(img_cers):\n",
    "            pred_str, target_str = decode_prediction_and_target(\n",
    "                prd, tgt, pl_module.model.label_encoder\n",
    "            )\n",
    "\n",
    "            # Create plot.\n",
    "            ncols = 4 if self.data_format == \"word\" else 2\n",
    "            nrows = math.ceil(to_log / ncols)\n",
    "            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n",
    "            matplotlib_imshow(im, IAMDataset.MEAN, IAMDataset.STD)\n",
    "            ax.set_title(f\"Pred: {pred_str} (CER: {cer:.2f})\\nTarget: {target_str}\")\n",
    "\n",
    "        # # Log the results to Tensorboard.\n",
    "        # tensorboard = trainer.logger.experiment\n",
    "        # tensorboard.add_figure(f\"{mode}: worst predictions\", fig, trainer.global_step)\n",
    "        trainer.logger.experiment.log({f\"{mode}: predictions vs targets\": wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(\"Done.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_best_model(trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        ckpt_callback = None\n",
    "        for cb in trainer.callbacks:\n",
    "            if isinstance(cb, ModelCheckpoint):\n",
    "                ckpt_callback = cb\n",
    "                break\n",
    "        assert ckpt_callback is not None, \"ModelCheckpoint not found in callbacks.\"\n",
    "        best_model_path = ckpt_callback.best_model_path\n",
    "\n",
    "        print(f\"Loading best model at {best_model_path}\")\n",
    "        label_encoder = pl_module.model.label_encoder\n",
    "        model = LitOrigamiNet.load_from_checkpoint(\n",
    "            best_model_path,\n",
    "            label_encoder=label_encoder,\n",
    "        )\n",
    "        trainer.model.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class LogModelPredictions(Callback):\n",
    "    \"\"\"\n",
    "    Use a fixed test batch to monitor model predictions at the end of every epoch.\n",
    "\n",
    "    Specifically: it generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's prediction alongside the actual target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_encoder: LabelParser,\n",
    "        val_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "        use_gpu: bool = True,\n",
    "        data_format: str = \"word\",\n",
    "        train_batch: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    ):\n",
    "        self.label_encoder = label_encoder\n",
    "        self.val_batch = val_batch\n",
    "        self.use_gpu = use_gpu\n",
    "        self.data_format = data_format\n",
    "        self.train_batch = train_batch\n",
    "\n",
    "    def on_validation_epoch_end(\n",
    "        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n",
    "    ):\n",
    "        self._predict_intermediate(trainer, pl_module, split=\"val\")\n",
    "\n",
    "    def on_train_epoch_end(\n",
    "        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n",
    "    ):\n",
    "        if self.train_batch is not None:\n",
    "            self._predict_intermediate(trainer, pl_module, split=\"train\")\n",
    "\n",
    "    def _predict_intermediate(\n",
    "        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", split=\"val\"\n",
    "    ):\n",
    "        \"\"\"Make predictions on a fixed batch of data and log the results to Tensorboard.\"\"\"\n",
    "\n",
    "        # Make predictions.\n",
    "        if split == \"train\":\n",
    "            imgs, targets = self.train_batch\n",
    "        else:  # split == \"val\"\n",
    "            imgs, targets = self.val_batch\n",
    "        with torch.inference_mode():\n",
    "            pl_module.eval()\n",
    "            preds = pl_module(imgs.cuda() if self.use_gpu else imgs)\n",
    "#             preds = preds.log_softmax(2)\n",
    "            preds = torch.max(preds, dim=-1)[1] # get the predicted outputs\n",
    "\n",
    "        # Decode predictions and generate a plot.\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        for i, (p, t) in enumerate(zip(preds, targets)):\n",
    "            pred_str, target_str = decode_prediction_and_target(\n",
    "                p, t, self.label_encoder\n",
    "            )\n",
    "\n",
    "            # Create plot.\n",
    "            ncols = 2 if self.data_format == \"word\" else 1\n",
    "            nrows = math.ceil(preds.size(0) / ncols)\n",
    "            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n",
    "            matplotlib_imshow(imgs[i], IAMDataset.MEAN, IAMDataset.STD)\n",
    "            ax.set_title(f\"Pred: {pred_str}\\nTarget: {target_str}\")\n",
    "\n",
    "\n",
    "        trainer.logger.experiment.log({f\"{split}: predictions vs targets\": wandb.Image(fig)})\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5761a76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T11:42:22.343984Z",
     "start_time": "2024-05-16T11:42:20.764696Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:33.309379Z",
     "iopub.status.busy": "2024-05-18T12:35:33.309102Z",
     "iopub.status.idle": "2024-05-18T12:35:55.479924Z",
     "shell.execute_reply": "2024-05-18T12:35:55.478970Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 22.181893,
     "end_time": "2024-05-18T12:35:55.482217",
     "exception": false,
     "start_time": "2024-05-18T12:35:33.300324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232 307\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(12345)\n",
    "ds = IAMDataset(root=\"/kaggle/input/iam-rimes/data/raw/IAM\", label_enc=None, parse_method=\"form\" ,split=\"train\")\n",
    "# rimes_ds = RIMESDataset(root=\"/kaggle/input/iam-rimes/data/raw/RIMES\", label_enc=iam_ds.label_enc, split=\"train\")\n",
    "# ds = AggregatedDataset(rimes_ds, iam_ds, split=\"train\", label_enc=rimes_ds.label_enc)\n",
    "# print(len(ds.label_enc.ctc_classes))\n",
    "# print(ds.label_enc.ctc_decode_labels([7]))\n",
    "# print(f\"Maximum target length for Aggregated dataset: {ds.get_max_target_len()}\")\n",
    "# ds_orig, ds_compl = torch.utils.data.random_split(ds, [math.ceil(0.5 * len(ds)), math.floor(0.5 * len(ds))])\n",
    "\n",
    "ds_train, ds_val = torch.utils.data.random_split(ds, [math.ceil(0.8 * len(ds)), math.floor(0.2 * len(ds))])\n",
    "\n",
    "ds_val.data = copy(ds)\n",
    "ds_val.data.set_transforms_for_split(\"val\")\n",
    "train_len = len(ds_train)\n",
    "val_len = len(ds_val)\n",
    "print(train_len, val_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a35b5240",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:55.500050Z",
     "iopub.status.busy": "2024-05-18T12:35:55.499725Z",
     "iopub.status.idle": "2024-05-18T12:35:55.505625Z",
     "shell.execute_reply": "2024-05-18T12:35:55.504811Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.01684,
     "end_time": "2024-05-18T12:35:55.507470",
     "exception": false,
     "start_time": "2024-05-18T12:35:55.490630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 2\n",
    "\n",
    "collate_fn = partial(\n",
    "        AggregatedDataset.unified_collate_fn\n",
    ")\n",
    "num_workers = 2\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size= batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_len //= batch_size\n",
    "val_len //= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b936ccc",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-05-18T12:35:55.524784Z",
     "iopub.status.busy": "2024-05-18T12:35:55.524529Z",
     "iopub.status.idle": "2024-05-18T12:36:32.703462Z",
     "shell.execute_reply": "2024-05-18T12:36:32.702373Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 37.190729,
     "end_time": "2024-05-18T12:36:32.706115",
     "exception": false,
     "start_time": "2024-05-18T12:35:55.515386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstefannastasa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Used: 256 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./wandb/run-20240518_123558-egfv9bm3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mhopeful-smoke-7\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/stefannastasa/origami_net_bach\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/stefannastasa/origami_net_bach/runs/egfv9bm3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c30d5ca02564b279f718743e476897a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35382c96dd754aa481509b43b219c935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 52.12 MiB is free. Process 3239 has 15.84 GiB memory in use. Of the allocated memory 15.24 GiB is allocated by PyTorch, and 312.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mhopeful-smoke-7\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/stefannastasa/origami_net_bach/runs/egfv9bm3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/stefannastasa/origami_net_bach\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20240518_123558-egfv9bm3/logs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished, resources cleared.\n",
      "Memory Used: 330 MB\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ModelSummary\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.login(key=\"0350b0cc5bd9521bb37a798168d31b6b65e9caca\")\n",
    "wandb_logger = WandbLogger(project=\"origami_net_bach\", log_model=\"all\")\n",
    "callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_top_k=(3),\n",
    "            mode=\"min\",\n",
    "            monitor=\"word_error_rate\",\n",
    "            filename=\"{epoch}-{char_error_rate:.4f}-{word_error_rate:.4f}\",\n",
    "        ),\n",
    "        ModelSummary(max_depth=2),\n",
    "        LitProgressBar(),\n",
    "        LogWorstPredictions(\n",
    "            dl_train,\n",
    "            dl_val,\n",
    "            training_skipped=False,\n",
    "            data_format=\"form\",\n",
    "        ),\n",
    "        LogModelPredictions(\n",
    "            ds.label_enc,\n",
    "            val_batch=next(\n",
    "                iter(\n",
    "                    DataLoader(\n",
    "                        Subset(\n",
    "                            ds_val,\n",
    "                            random.sample(\n",
    "                                range(len(ds_val)), 1\n",
    "                            ),\n",
    "                        ),\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_fn,\n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=True,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            train_batch=next(\n",
    "                iter(\n",
    "                    DataLoader(\n",
    "                        Subset(\n",
    "                            ds_train,\n",
    "                            random.sample(\n",
    "                                range(len(ds_train)),\n",
    "                                1,\n",
    "                            ),\n",
    "                        ),\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_fn,\n",
    "                        num_workers=num_workers,\n",
    "                        pin_memory=True,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            data_format=\"form\",\n",
    "            use_gpu=True,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "                    monitor=\"word_error_rate\",\n",
    "                    patience=50,\n",
    "                    verbose=True,\n",
    "                    mode=\"min\",\n",
    "                    check_on_train_epoch_end=False,\n",
    "                )\n",
    "    ]\n",
    "\n",
    "def get_gpu_memory_map():\n",
    "    result = os.popen('nvidia-smi --query-gpu=memory.used --format=csv,nounits,noheader').read()\n",
    "    return int(result.strip())\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=3000,\n",
    "    accelerator=\"gpu\", \n",
    "    devices=1,\n",
    "    callbacks = callbacks,\n",
    "#     accumulate_grad_batches=54,\n",
    "    logger=wandb_logger,\n",
    "    fast_dev_run=False,\n",
    "    precision=\"16-mixed\")\n",
    "model = LitOrigamiNet(\n",
    "    n_channels = 1, \n",
    "    label_encoder=ds.label_enc,\n",
    "    mul_rate= 1.0, \n",
    "    layer_resizes= {\n",
    "            0: nn.MaxPool2d(2, 2),\n",
    "            2: nn.MaxPool2d(2, 2),\n",
    "            4: nn.MaxPool2d(2,2),\n",
    "            6: nn.ZeroPad2d(1),\n",
    "            8: nn.ZeroPad2d(1),\n",
    "            10: nn.Upsample((450, 15), align_corners=True, mode=\"bilinear\"),\n",
    "            11: nn.Upsample((1100, 8), align_corners=True, mode=\"bilinear\")\n",
    "        },\n",
    "    layer_sizes= {\n",
    "            0:  512,\n",
    "            4:  1024,\n",
    "            11: 512\n",
    "        }, \n",
    "    num_layers=12, \n",
    "    fup=33,\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "# def profile_memory(model, dataloader):\n",
    "#     model.cuda()\n",
    "#     model.train()\n",
    "#     for batch in dataloader:\n",
    "#         inputs, _ = batch\n",
    "#         inputs = inputs.cuda()\n",
    "#         outputs = model(inputs)\n",
    "#         print(f\"Memory Usage: {torch.cuda.memory_allocated()} bytes\")\n",
    "        \n",
    "            \n",
    "# profile_memory(model, dl_train)\n",
    "\n",
    "# trainer.fit(model, dl_train, dl_val)\n",
    "print(f\"Memory Used: {get_gpu_memory_map()} MB\")\n",
    "import traceback\n",
    "try:\n",
    "    # Start training\n",
    "    trainer.fit(model, dl_train, dl_val)\n",
    "except Exception as e:\n",
    "    # Catch and print any exceptions during training\n",
    "    print(f\"An error occurred: {e}\")\n",
    "#     traceback.print_exc()\n",
    "finally:\n",
    "    # Ensure wandb is properly closed\n",
    "    wandb.finish()\n",
    "\n",
    "    # Cleanup to free GPU memory, if the objects are not needed anymore\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training finished, resources cleared.\")\n",
    "    print(f\"Memory Used: {get_gpu_memory_map()} MB\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5000806,
     "sourceId": 8404124,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 90.628311,
   "end_time": "2024-05-18T12:36:35.438304",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-18T12:35:04.809993",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "047e5398d5c742dca1ce03090b8ab4eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "11e8adee8d304e8ab6e55f80b4cc678f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_92defd5b5d634407b49ee4a13cd95b45",
       "placeholder": "​",
       "style": "IPY_MODEL_b6976f8830db4115b47bfe6a5770374d",
       "value": "Sanity Checking DataLoader 0: 100%"
      }
     },
     "1e4aac8382414456bea99292d0d5b455": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "35382c96dd754aa481509b43b219c935": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cc263fd3acff4d2082180d563cbcc26b",
        "IPY_MODEL_b853bd37fa054f82af589b27381a5fb5",
        "IPY_MODEL_4102cca6af7b4ac28bd96a5f17365eb2"
       ],
       "layout": "IPY_MODEL_da943326dc144d9ca51b0459a9980708"
      }
     },
     "3f3a1abb46704d88885a9fb21c2574ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4102cca6af7b4ac28bd96a5f17365eb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_48d5a5695f9c438a96dba3fb8178bba2",
       "placeholder": "​",
       "style": "IPY_MODEL_047e5398d5c742dca1ce03090b8ab4eb",
       "value": " 4/616 [00:07&lt;18:00,  0.57it/s]"
      }
     },
     "48d5a5695f9c438a96dba3fb8178bba2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4dc9783860f94dbeac83fedc6059f668": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5c30d5ca02564b279f718743e476897a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_11e8adee8d304e8ab6e55f80b4cc678f",
        "IPY_MODEL_71a9120e75494856b11322cd479fae9d",
        "IPY_MODEL_737f640efd6b4dbabe00c3ed648af981"
       ],
       "layout": "IPY_MODEL_8d68c6c643214c6483df6cf952ca3426"
      }
     },
     "71a9120e75494856b11322cd479fae9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3f3a1abb46704d88885a9fb21c2574ff",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1e4aac8382414456bea99292d0d5b455",
       "value": 2.0
      }
     },
     "737f640efd6b4dbabe00c3ed648af981": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c18c6fea8b744aa4930d1c9b36365514",
       "placeholder": "​",
       "style": "IPY_MODEL_4dc9783860f94dbeac83fedc6059f668",
       "value": " 2/2 [00:01&lt;00:00,  1.22it/s]"
      }
     },
     "75212343eeca421d9fac5c14be6a9e21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8d68c6c643214c6483df6cf952ca3426": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": "100%"
      }
     },
     "92defd5b5d634407b49ee4a13cd95b45": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a8a7de40a91c4fa99089296afaf7773a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": "2",
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b6976f8830db4115b47bfe6a5770374d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b853bd37fa054f82af589b27381a5fb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a8a7de40a91c4fa99089296afaf7773a",
       "max": 616.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_75212343eeca421d9fac5c14be6a9e21",
       "value": 4.0
      }
     },
     "c18c6fea8b744aa4930d1c9b36365514": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cc263fd3acff4d2082180d563cbcc26b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_da55c0c3808b47b186654af8d0dcf1f2",
       "placeholder": "​",
       "style": "IPY_MODEL_f70cbfaef3394fc7aa8747fa03c07923",
       "value": "Epoch 0:   1%"
      }
     },
     "da55c0c3808b47b186654af8d0dcf1f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da943326dc144d9ca51b0459a9980708": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": "inline-flex",
       "flex": null,
       "flex_flow": "row wrap",
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "100%"
      }
     },
     "f70cbfaef3394fc7aa8747fa03c07923": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8404124,"sourceType":"datasetVersion","datasetId":5000806}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install editdistance torchmetrics pytorch_lightning","metadata":{"execution":{"iopub.status.busy":"2024-05-18T10:19:52.308637Z","iopub.execute_input":"2024-05-18T10:19:52.308980Z","iopub.status.idle":"2024-05-18T10:20:04.375538Z","shell.execute_reply.started":"2024-05-18T10:19:52.308953Z","shell.execute_reply":"2024-05-18T10:20:04.374570Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: editdistance in /opt/conda/lib/python3.10/site-packages (0.8.1)\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.3.2)\nRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.2.2)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.1.2)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.2)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.66.1)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (6.0.1)\nRequirement already satisfied: fsspec>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.2.0)\nRequirement already satisfied: typing-extensions>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.9.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nimport math\nfrom random import random\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport albumentations as A\nimport cv2 as cv\nfrom dataclasses import dataclass, field\nfrom functools import partial\nfrom random import randint\nimport html\nimport random\nfrom pathlib import Path\nfrom typing import Union, Tuple, Dict, Sequence, Optional, List, Any, Callable, Optional\nimport pandas as pd\nfrom torch import Tensor, nn\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom torchmetrics import Metric\nimport torch\nfrom torchvision import models\nfrom torch.utils.data import Dataset\nimport editdistance\nimport wandb\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.optim import Optimizer\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nimport concurrent\nfrom pytorch_lightning.callbacks import TQDMProgressBar","metadata":{"ExecuteTime":{"end_time":"2024-05-16T11:42:20.695236Z","start_time":"2024-05-16T11:42:16.322123Z"},"execution":{"iopub.status.busy":"2024-05-18T10:20:04.377896Z","iopub.execute_input":"2024-05-18T10:20:04.378620Z","iopub.status.idle":"2024-05-18T10:20:10.980855Z","shell.execute_reply.started":"2024-05-18T10:20:04.378580Z","shell.execute_reply":"2024-05-18T10:20:10.979996Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class LabelParser:\n    def __init__(self):\n        self.classes = None\n        self.vocab_size = None\n        self.class_to_idx = None\n        self.idx_to_class = None\n        self.ctc_classes = None\n        self.ctc_idx_to_class = None\n        self.ctc_class_to_idx = None\n        \n    def fit(self, classes: Sequence[str]):\n        self.classes = list(classes)\n        self.vocab_size = len(classes)\n        self.idx_to_class = dict(enumerate(classes))\n        self.class_to_idx = {cls: i for i, cls in self.idx_to_class.items()}\n        \n        self.ctc_classes = [\"<blank>\"] + self.classes\n        self.ctc_idx_to_class = dict(enumerate(self.ctc_classes))\n        self.ctc_class_to_idx = {cls: i for i, cls in self.ctc_idx_to_class.items()}\n        \n        return self\n\n    def addClasses(self, classes: List[str]):\n        all_classes = sorted(set(self.classes + classes))\n\n        self.fit(all_classes)\n\n    def encode_labels(self, sequence: Sequence[str]):\n        self._check_fitted()\n        return [self.class_to_idx[c] for c in sequence]\n\n    def decode_labels(self, sequence: Sequence[int]):\n        self._check_fitted()\n        return [self.idx_to_class[c] for c in sequence]\n    \n    def ctc_encode_labels(self, sequence: Sequence[str]):\n        self._check_fitted()\n        return [self.ctc_class_to_idx[c] for c in sequence]\n    \n    def ctc_decode_labels(self, sequence: Sequence[int]):\n        self._check_fitted()\n        return [self.ctc_idx_to_class[c] for c in sequence]\n    \n    def _check_fitted(self):\n        if self.classes is None:\n            raise ValueError(\"LabelParser class was not fitted yet\")","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-05-16T11:42:20.700937Z","start_time":"2024-05-16T11:42:20.696431Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:20:10.981958Z","iopub.execute_input":"2024-05-18T10:20:10.982252Z","iopub.status.idle":"2024-05-18T10:20:10.994187Z","shell.execute_reply.started":"2024-05-18T10:20:10.982226Z","shell.execute_reply":"2024-05-18T10:20:10.993104Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def pickle_load(file) -> Any:\n    with open(file, \"rb\") as f:\n        return pickle.load(f)\n\ndef pickle_save(obj, file):\n    with open(file, \"wb\") as f:\n        pickle.dump(obj, f)\n\ndef read_xml(file: Union[Path, str]) -> ET.Element:\n    tree = ET.parse(file)\n    root = tree.getroot()\n\n    return root\n\ndef find_child_by_tag(element: ET.Element, tag: str, value: str) -> Union[ET.Element, None]:\n    for child in element:\n        if child.get(tag) == value:\n            return child\n    return None\n\ndef set_seed(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef dpi_adjusting(img: np.ndarray, scale: float, **kwargs) -> np.ndarray:\n    height, width = img.shape[:2]\n    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n    return cv.resize(img, (new_width, new_height))\n\nclass LitProgressBar(TQDMProgressBar):\n    def get_metrics(self, trainer, model):\n        # don't show the version number\n        items = super().get_metrics(trainer, model)\n        for k in list(items.keys()):\n            if k.startswith(\"grad\"):\n                items.pop(k, None)\n        items.pop(\"v_num\", None)\n        return items\n    \ndef decode_prediction_and_target(\n    pred: Tensor, target: Tensor, label_encoder: LabelParser\n) -> Tuple[str, str]:\n\n    # Decode prediction and target.\n    p, t = pred.tolist(), target.tolist()\n    pred_str = \"\".join(label_encoder.ctc_decode_labels(p))\n    target_str = \"\".join(label_encoder.ctc_decode_labels(t))\n    return pred_str, target_str\n\ndef matplotlib_imshow(\n    img: torch.Tensor, mean: float = 0.5, std: float = 0.5, one_channel=True\n):\n    assert img.device.type == \"cpu\"\n    if one_channel and img.ndim == 3:\n        img = img.mean(dim=0)\n    img = img * std + mean  # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-05-16T11:42:20.708328Z","start_time":"2024-05-16T11:42:20.701808Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:20:10.996897Z","iopub.execute_input":"2024-05-18T10:20:10.997466Z","iopub.status.idle":"2024-05-18T10:20:11.013550Z","shell.execute_reply.started":"2024-05-18T10:20:10.997432Z","shell.execute_reply":"2024-05-18T10:20:11.012772Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class SafeRandomScale(A.RandomScale):\n    def apply(self, img, scale=0, interpolation=cv.INTER_LINEAR, **params):\n        height, width = img.shape[:2]\n        new_height, new_width = int(height * scale), int(width * scale)\n        if new_height <= 0 or new_width <= 0:\n            return img\n        return super().apply(img, scale, interpolation, **params)\n\ndef adjust_dpi(img: np.ndarray, scale: float, **kwargs):\n    height, width = img.shape\n    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n    return cv.resize(img, (new_width, new_height))\n\ndef randomly_displace_and_pad(\n    img: np.ndarray,\n    padded_size: Tuple[int, int],\n    crop_if_necessary: bool = False,\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"\n    Randomly displace an image within a frame, and pad zeros around the image.\n\n    Args:\n        img (np.ndarray): image to process\n        padded_size (Tuple[int, int]): (height, width) tuple indicating the size of the frame\n        crop_if_necessary (bool): whether to crop the image if its size exceeds that\n            of the frame\n    \"\"\"\n    frame_h, frame_w = padded_size\n    img_h, img_w = img.shape\n    if frame_h < img_h or frame_w < img_w:\n        if crop_if_necessary:\n            print(\n                \"WARNING (`randomly_displace_and_pad`): cropping input image before \"\n                \"padding because it exceeds the size of the frame.\"\n            )\n            img_h, img_w = min(img_h, frame_h), min(img_w, frame_w)\n            img = img[:img_h, :img_w]\n        else:\n            raise AssertionError(\n                f\"Frame is smaller than the image: ({frame_h}, {frame_w}) vs. ({img_h},\"\n                f\" {img_w})\"\n            )\n\n    res = np.zeros((frame_h, frame_w), dtype=img.dtype)\n\n    pad_top =  randint(0, frame_h - img_h)\n    pad_bottom = pad_top + img_h\n    pad_left = randint(0, frame_w - img_w)\n    pad_right = pad_left + img_w\n\n    res[pad_top:pad_bottom, pad_left:pad_right] = img\n    return res\n\n@dataclass\nclass ImageTransforms:\n    max_img_size: Tuple[int, int]  # (h, w)\n    normalize_params: Tuple[float, float]  # (mean, std)\n    scale: float = (\n        0.75\n    )\n    random_scale_limit: float = 0.1\n    random_rotate_limit: int = 1\n\n    train_trnsf: A.Compose = field(init=False)\n    test_trnsf: A.Compose = field(init=False)\n\n    def __post_init__(self):\n        scale, random_scale_limit, random_rotate_limit, normalize_params =(\n            self.scale,\n            self.random_scale_limit,\n            self.random_rotate_limit,\n            self.normalize_params\n        )\n\n        max_img_h, max_img_w = self.max_img_size\n        max_scale = scale + scale * random_scale_limit\n        padded_h, padded_w = math.ceil(max_scale * max_img_h), math.ceil(max_scale * max_img_w)\n\n        self.train_trnsf = A.Compose([\n            A.Lambda(partial(adjust_dpi, scale=scale)),\n            SafeRandomScale(scale_limit=random_scale_limit, p=0.5),\n#             A.SafeRotate(\n#                 limit = random_rotate_limit,\n#                 border_mode = cv.BORDER_CONSTANT,\n#                 value = 0\n#             ),\n            A.RandomBrightnessContrast(),\n#             A.Perspective(scale=(0.01, 0.05)),\n            A.GaussNoise(),\n            A.Normalize(*normalize_params),\n#             A.Lambda(\n#                 image=partial(\n#                     randomly_displace_and_pad,\n#                     padded_size=(padded_h, padded_w),\n#                     crop_if_necessary=False,\n#                 )\n#             )\n        ])\n\n        self.test_trnsf = A.Compose([\n            A.Lambda(partial(adjust_dpi, scale=scale)),\n            A.Normalize(*normalize_params),\n            A.PadIfNeeded(\n                max_img_h, max_img_w, border_mode=cv.BORDER_CONSTANT, value=0\n            )\n        ])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-05-16T11:42:20.716503Z","start_time":"2024-05-16T11:42:20.708975Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:30:23.645597Z","iopub.execute_input":"2024-05-18T10:30:23.646099Z","iopub.status.idle":"2024-05-18T10:30:23.672254Z","shell.execute_reply.started":"2024-05-18T10:30:23.646056Z","shell.execute_reply":"2024-05-18T10:30:23.671346Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class CharacterErrorRate(Metric):\n\n    def __init__(self, label_encoder: LabelParser):\n        super().__init__()\n        self.label_encoder = label_encoder\n        \n        self.add_state(\"cer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n        self.add_state(\"nr_samples\",default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        assert preds.ndim == target.ndim\n\n        for i, (p, t) in enumerate(zip(preds, target)):\n            p_str, t_str = map(tensor_to_str, (p, t))\n            editd = editdistance.eval(p_str, t_str)\n\n            self.cer_sum += editd/t.numel()\n            self.nr_samples +=1\n\n    def compute(self) -> torch.Tensor:\n        return self.cer_sum / self.nr_samples.float()\n\nclass WordErrorRate(Metric):\n    def __init__(self, label_encoder: LabelParser):\n        super().__init__()\n        self.label_encoder = label_encoder\n        \n        self.add_state(\"wer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n        self.add_state(\"nr_samples\", default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        assert preds.ndim == target.ndim\n\n        for i, (p, t) in enumerate(zip(preds, target)):\n            p = p.flatten().tolist()\n            t = t.flatten().tolist()\n            p_words = \"\".join(self.label_encoder.ctc_decode_labels(p)).split()\n            t_words = \"\".join(self.label_encoder.ctc_decode_labels(t)).split()\n            editd = editdistance.eval(p_words, t_words)\n            \n            self.wer_sum += editd / len(t_words)\n            self.nr_samples += 1\n            \n    def compute(self) -> torch.Tensor:\n        \"\"\"Compute Word Error Rate.\"\"\"\n        return self.wer_sum / self.nr_samples.float()\n\ndef tensor_to_str(t: torch.Tensor) -> str:\n    return \"\".join(map(str, t.flatten().tolist()))","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-05-16T11:44:10.989326Z","start_time":"2024-05-16T11:44:10.961990Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:20:11.036390Z","iopub.execute_input":"2024-05-18T10:20:11.036676Z","iopub.status.idle":"2024-05-18T10:20:11.050492Z","shell.execute_reply.started":"2024-05-18T10:20:11.036653Z","shell.execute_reply":"2024-05-18T10:20:11.049656Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class IAMDataset(Dataset):\n    MEAN = 0.8275\n    STD = 0.2314\n    MAX_FORM_HEIGHT = 3542\n    MAX_FORM_WIDTH = 2479\n\n    MAX_SEQ_LENS = {\n        \"word\": 55,\n        \"line\": 90,\n        \"form\": 700,\n    }  # based on the maximum seq lengths found in the dataset\n\n    root: Path\n    data: pd.DataFrame\n    label_enc: LabelParser\n    parse_method: str\n    only_lowercase: bool\n    transforms: Optional[A.Compose]\n    id_to_idx: Dict[str, int]\n    _split: str\n    _return_writer_id: Optional[bool]\n\n    def __init__(\n        self,\n        root: Union[Path, str],\n        parse_method: str,\n        split: str,\n        return_writer_id: bool = False,\n        only_lowercase: bool = False,\n        label_enc: Optional[LabelParser] = None,\n    ):\n        super().__init__()\n        _parse_methods = [\"form\", \"line\", \"word\"]\n        err_message = (\n            f\"{parse_method} is not a possible parsing method: {_parse_methods}\"\n        )\n        assert parse_method in _parse_methods, err_message\n\n        _splits = [\"train\", \"test\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n\n        self._split = split\n        self._return_writer_id = return_writer_id\n        self.only_lowercase = only_lowercase\n        self.root = Path(root)\n        self.label_enc = label_enc\n        self.parse_method = parse_method\n\n        # Process the data.\n        if not hasattr(self, \"data\"):\n            self.data = self._get_forms()\n\n        # Create the label encoder.\n        if self.label_enc is None:\n            vocab = []\n            s = \"\".join(self.data[\"target\"].tolist())\n\n            if self.only_lowercase:\n                s = s.lower()\n            vocab += sorted(list(set(s)))\n            self.label_enc = LabelParser().fit(vocab)\n        if not \"target_enc\" in self.data.columns:\n            self.data.insert(\n                2,\n                \"target_enc\",\n                self.data[\"target\"].apply(\n                    lambda s: np.array(\n                        self.label_enc.encode_labels(\n                            [c for c in (s.lower() if self.only_lowercase else s)]\n                        )\n                    )\n                ),\n            )\n\n        self.transforms = self._get_transforms(split)\n        self.id_to_idx = {\n            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n        }\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n            # Crop the image vertically.\n            img = img[data[\"bb_y_start\"] : data[\"bb_y_end\"], :]\n        assert isinstance(img, np.ndarray), (\n            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n            f\"Is there something wrong with this image?\"\n        )\n        if self.transforms is not None:\n            img = self.transforms(image=img)[\"image\"]\n        if self._return_writer_id:\n            return img, data[\"writer_id\"], data[\"target_enc\"]\n        return img, data[\"target_enc\"]\n\n    def get_max_height(self):\n        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n\n    @property\n    def vocab(self):\n        return self.label_enc.classes\n\n    @staticmethod\n    def collate_fn(\n        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n        dataset_returns_writer_id: bool = False,\n    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n        if dataset_returns_writer_id:\n            imgs, writer_ids, targets = zip(*batch)\n        else:\n            imgs, targets = zip(*batch)\n\n        img_sizes = [im.shape for im in imgs]\n        if (\n            not len(set(img_sizes)) == 1\n        ):  # images are of varying sizes, so pad them to the maximum size in the batch\n            hs, ws = zip(*img_sizes)\n            pad_fn = A.PadIfNeeded(\n                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n            )\n            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n        imgs = np.stack(imgs, axis=0)\n        imgs, targets = torch.Tensor(imgs), torch.Tensor(targets)\n        return imgs, targets\n\n    def set_transforms_for_split(self, split: str):\n        _splits = [\"train\", \"val\", \"test\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n        self.transforms = self._get_transforms(split)\n\n    def _get_transforms(self, split: str) -> A.Compose:\n        max_img_w = self.MAX_FORM_WIDTH\n\n        if self.parse_method == \"form\":\n            max_img_h = (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n        else:  # word or line\n            max_img_h = self.MAX_FORM_HEIGHT\n\n        transforms = ImageTransforms(\n            (max_img_h, max_img_w), (IAMDataset.MEAN, IAMDataset.STD)\n        )\n\n        if split == \"train\":\n            return transforms.train_trnsf\n        elif split == \"test\" or split == \"val\":\n            return transforms.test_trnsf\n\n    def statistics(self) -> Dict[str, float]:\n        assert len(self) > 0\n        tmp = self.transforms\n        self.transforms = None\n        mean, std, cnt = 0, 0, 0\n        for img, _ in self:\n            mean += np.mean(img)\n            std += np.var(img)\n            cnt += 1\n        mean /= cnt\n        std = np.sqrt(std / cnt)\n        self.transforms = tmp\n        return {\"mean\": mean, \"std\": std}\n    \n    def get_max_target_len(self):\n        return (self.data[\"target_len\"]).max()\n    \n    def _get_forms(self) -> pd.DataFrame:\n        \n        data = {\n            \"img_path\": [],\n            \"img_id\": [],\n            \"target\": [],\n            \"bb_y_start\": [],\n            \"bb_y_end\": [],\n            \"target_len\": [],\n        }\n        for form_dir in [\"formsA-D\", \"formsE-H\", \"formsI-Z\"]:\n            dr = self.root / form_dir\n            for img_path in dr.iterdir():\n                doc_id = img_path.stem\n                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n\n                bb_y_start = int(xml_root[1][0].get(\"asy\")) - 150\n                bb_y_end = int(xml_root[1][-1].get(\"dsy\")) + 150\n\n                form_text = []\n                for line in xml_root.iter(\"line\"):\n                    form_text.append(html.unescape(line.get(\"text\", \"\")))\n                \n                img_w, img_h = Image.open(str(img_path)).size\n                target = \" \".join(form_text)\n                target = target.replace(\"\\n\", \" \")\n                data[\"img_path\"].append(str(img_path))\n                data[\"img_id\"].append(doc_id)\n                data[\"target\"].append(target)\n                data[\"bb_y_start\"].append(bb_y_start)\n                data[\"bb_y_end\"].append(bb_y_end)\n                data[\"target_len\"].append(len(target))\n        return pd.DataFrame(data).sort_values(\n            \"target_len\"\n        )  # by default, sort by target length\n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-05-16T11:42:20.732251Z","start_time":"2024-05-16T11:42:20.717929Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:20:11.051618Z","iopub.execute_input":"2024-05-18T10:20:11.051962Z","iopub.status.idle":"2024-05-18T10:20:11.086517Z","shell.execute_reply.started":"2024-05-18T10:20:11.051931Z","shell.execute_reply":"2024-05-18T10:20:11.085563Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import re\nimport random\nclass RIMESDataset(Dataset):\n    MEAN = 0.8275\n    STD = 0.2314\n    \n    root: Path\n    data: pd.DataFrame\n    label_enc: LabelParser\n    transforms: Optional[A.Compose]\n    id_to_idx: Dict[str, int]\n    _split: str\n    _return_writer_id: Optional[bool]\n    \n    max_width: Optional[int]\n    max_height: Optional[int]\n    \n    @staticmethod\n    def process_target(target: str):\n        # Splitting the input string into lines\n        target = target.replace(\"\\\\n\", \"\\n\")\n        target = target.replace(\"\\n\", \" \")\n        \n        pattern = re.compile(r'¤?\\{?([^ ¤{}0-9]*)\\/([^ ¤{}0-9]*)\\}?¤?')\n        matches = pattern.findall(target)\n        match_pos = [a for a in pattern.finditer(target)]\n        new_target = \"\"\n        last_ind = 0\n\n        for i, (choices, position) in enumerate(zip(matches, match_pos)):\n            if position.start() > 0:\n                new_target += target[last_ind:position.start()-1]\n            if len(choices) != 0:\n                new_target += \" \" + choices[random.randint(0, len(choices) - 1)] + \" \"\n            last_ind = position.end() + 1\n        \n        new_target += target[last_ind:]\n        \n        return new_target\n    \n    def __init__(\n            self,\n            root: Union[Path, str],\n            split: str, \n            only_lowercase: bool = False,\n            label_enc: Optional[LabelParser] = None,):\n        super().__init__()\n        \n        _splits = [\"train\", \"test\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n        \n        self._split = split\n        self.only_lowercase = only_lowercase\n        self.root = Path(root)\n        self.label_enc = label_enc\n        \n        if not hasattr(self, \"data\"):\n            self.data = self._get_form_data()\n        \n        if self.label_enc is None:\n            vocab = []\n            s = \"\".join(self.data[\"target\"].tolist())\n            if self.only_lowercase:\n                s = s.lower()\n            vocab += sorted(list(set(s)))\n            self.label_enc = LabelParser().fit(vocab)\n        else:\n            vocab = []\n            s = \"\".join(self.data[\"target\"].tolist())\n            if self.only_lowercase:\n                s = s.lower()\n            vocab += sorted(list(set(s)))\n            self.label_enc.addClasses(vocab)\n        \n        if not \"target_enc\" in self.data.columns:\n            self.data.insert(\n                2,\n                \"target_enc\",\n                self.data[\"target\"].apply(\n                    lambda s: np.array(\n                        self.label_enc.ctc_encode_labels(\n                            [c for c in (s.lower() if self.only_lowercase else s)]\n                        )\n                    )\n                )\n            )\n        self.transforms = self._get_transforms(split)\n        self.id_to_idx = {\n            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n        }\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def set_transform_for_split(self, split):\n        _splits = [\"train\", \"val\", \"test\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n        self.transforms = self._get_transforms(split)\n    \n    def __getitem__(self, idx):\n        data = self.data.iloc[idx]\n        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n        \n        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n            img = img[data[\"bb_y_start\"]: data[\"bb_y_end\"], :]\n        assert isinstance(img, np.ndarray), (\n            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n            f\"Is there something wrong with this image?\"\n        )\n        if self.transforms is not None:\n            img = self.transforms(image=img)[\"image\"]\n        \n        return img, data[\"target_enc\"]\n    \n    def get_max_height(self):\n        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max() + 150\n    \n    def get_max_width(self):\n        return (self.data[\"bb_x_end\"] - self.data[\"bb_x_start\"]).max() + 150\n    \n    @property\n    def vocab(self):\n        return self.label_enc.classes\n        \n    @staticmethod\n    def collate_fn(\n        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n        \n        imgs, targets = zip(*batch)\n\n        img_sizes = [im.shape for im in imgs]\n        if not len(set(img_sizes)) == 1:\n            hs, ws = zip(*img_sizes)\n            pad_fn = A.PadIfNeeded(\n                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n            )\n            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n        imgs = np.stack(imgs, axis=0)\n        imgs, targets = torch.Tensor(imgs), torch.Tensor(targets)\n        return imgs, targets\n    \n    def _get_transforms(self, split: str) -> A.Compose:\n        max_img_w = self.max_width\n    \n        max_img_h = self.max_height\n    \n        transforms = ImageTransforms(\n            (max_img_h, max_img_w), (RIMESDataset.MEAN, RIMESDataset.STD)\n        )\n    \n        if split == \"train\":\n            return transforms.train_trnsf\n        elif split == \"test\" or split == \"val\":\n            return transforms.test_trnsf\n    \n    def get_max_target_len(self):\n        return (self.data[\"target_len\"]).max()\n\n    def _get_form_data(self):\n        data = {\n            \"img_path\": [],\n            \"img_id\": [],\n            \"target\": [],\n            \"bb_y_start\": [],\n            \"bb_y_end\": [],\n            \"bb_x_start\": [],\n            \"bb_x_end\": [],\n            \"target_len\": [],\n        }\n        \n        \n        def process_forms(paths: Tuple[str, str, Path]):\n            return_data = {\n                \"img_path\": [],\n                \"img_id\": [],\n                \"target\": [],\n                \"bb_y_start\": [],\n                \"bb_y_end\": [],\n                \"bb_x_start\": [],\n                \"bb_x_end\": [],\n                \"target_len\": []\n            }\n            img_path, xml_path, root = paths\n            img_path = root / img_path\n            xml_path = root / xml_path\n            doc_id = img_path.stem[:-2]\n            xml_root = read_xml(xml_path)\n            \n            target = \"\"\n            num_corps = 0\n            for box in xml_root.iter(\"box\"):\n                type_tag =  box.find(\"type\")\n                if type_tag.text == \"Corps de texte\":\n                    target = box.find(\"text\").text\n                    \n                    \n                    if target is None or target == \"\":\n                        continue\n                    words = target.split(\"\\\\n\")\n                    if len(words) <= 5:\n                        continue\n\n                    target = self.process_target(target)\n                    if target.find(\"¤\") != -1 or target.find(\"{\") != -1 or target.find(\"}\") != -1:\n                        continue #skip if the target sequence is not standard\n                        \n                    return_data[\"img_path\"].append(str(img_path.resolve()))\n                    return_data[\"img_id\"].append(doc_id)\n                    return_data[\"target\"].append(target)\n                    return_data[\"bb_y_start\"].append(int(box.get(\"top_left_y\")))\n                    return_data[\"bb_y_end\"].append(int(box.get(\"bottom_right_y\")))\n                    return_data[\"bb_x_start\"].append(int(box.get(\"top_left_x\")))\n                    return_data[\"bb_x_end\"].append(int(box.get(\"bottom_right_x\")))\n                    return_data[\"target_len\"].append(len(target))\n                    num_corps += 1\n            \n            return return_data\n        \n        image_pairs = []\n        for form_dir in [\"DVD1_TIF\", \"DVD2_TIF\", \"DVD3_TIF\"]:\n            dr = self.root / form_dir\n            for file in dr.iterdir():\n                name = file.stem\n                ext = file.suffix\n                if ext == \".tif\" and name[-1] == \"L\":\n                    image_pairs.append((name + \".tif\", name + \".xml\", dr))\n        \n        with ThreadPoolExecutor() as executor:\n            results = list(executor.map(process_forms, iter(image_pairs)))\n            \n            for single_results in results:\n                if single_results[\"img_path\"] == \"\":\n                    continue\n                data[\"img_path\"].extend(single_results[\"img_path\"])\n                data[\"img_id\"].extend(single_results[\"img_id\"])\n                data[\"target\"].extend(single_results[\"target\"])\n                data[\"bb_y_start\"].extend(single_results[\"bb_y_start\"])\n                data[\"bb_y_end\"].extend(single_results[\"bb_y_end\"])\n                data[\"bb_x_start\"].extend(single_results[\"bb_x_start\"])\n                data[\"bb_x_end\"].extend(single_results[\"bb_x_end\"])\n                data[\"target_len\"].extend(single_results[\"target_len\"])\n        \n        to_ret = pd.DataFrame(data)\n        self.max_height = (to_ret[\"bb_y_end\"] - to_ret[\"bb_y_start\"]).max() + 150\n        self.max_width = (to_ret[\"bb_x_end\"] - to_ret[\"bb_x_start\"]).max() + 150\n        \n        return to_ret","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-05-16T11:42:20.749373Z","start_time":"2024-05-16T11:42:20.733039Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:20:11.087750Z","iopub.execute_input":"2024-05-18T10:20:11.088021Z","iopub.status.idle":"2024-05-18T10:20:11.130037Z","shell.execute_reply.started":"2024-05-18T10:20:11.087999Z","shell.execute_reply":"2024-05-18T10:20:11.129235Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class AggregatedDataset(Dataset):\n    datasets: List[Dataset]\n    def __init__(self, rimes: RIMESDataset,\n                 iam: IAMDataset, \n                 split:str,\n                 only_lowercase: bool = False,\n                 label_enc: Optional[LabelParser] = None,):\n        super().__init__()\n        _splits = [\"train\", \"test\"]\n        err_message = f\"{split} is not a possible split: {_splits}\"\n        assert split in _splits, err_message\n        \n        self._split = split\n        self.rimes = rimes\n        self.iam = iam\n        self._only_lowercase = only_lowercase\n        self.label_enc = label_enc\n        \n        if self.label_enc is None:\n            iamLabelEncoder = iam.label_enc\n            rimesLabelEncoder = rimes.label_enc\n            self.label_enc = LabelParser()\n            self.label_enc.addClasses(iamLabelEncoder.classes)\n            self.label_enc.addClasses(rimesLabelEncoder.classes)\n        \n    def __len__(self):\n        return len(self.rimes) + len(self.iam)\n    \n    def get_max_target_len(self):\n        return max(self.iam.get_max_target_len(), self.rimes.get_max_target_len())\n    \n    @staticmethod\n    def unified_collate_fn(\n        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n        ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n        \n        imgs, targets = zip(*batch)\n        img_sizes = [im.shape for im in imgs]\n        if not len(set(img_sizes)) == 1:\n            hs, ws = zip(*img_sizes)\n            pad_fn = A.PadIfNeeded(\n                max(hs), max(ws), border_mode = cv.BORDER_CONSTANT, value=0\n            )\n            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n            \n        imgs = [np.expand_dims(im, axis=0) for im in imgs]\n        imgs = np.stack(imgs, axis=0)\n        \n        seq_lengths = [t.shape[0] for t in targets]\n        targets_padded = np.full((len(targets), max(seq_lengths) + 1), 0)\n        for i, t in enumerate(targets):\n            targets_padded[i, : seq_lengths[i]] = t\n        \n        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n        \n        return imgs, targets_padded\n    \n    def __getitem__(self, idx):\n        iam = self.iam \n        rimes = self.rimes\n        if idx < len(self.rimes):\n            img, target = rimes[idx]\n        if len(rimes) <= idx < len(rimes) + len(iam):\n            img, target = iam[idx - len(self.rimes)]\n        \n        if img is None :\n            raise ValueError(\"Image is None.\")\n        if target is None:\n            raise ValueError(\"Image is None.\")\n        \n        assert not np.any(np.isnan(img)), img\n        return img, target\n        \n        return img, target\n    def set_transforms_for_split(self, split):\n        self.iam.set_transforms_for_split(split)\n        self.rimes.set_transform_for_split(split)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-05-16T11:42:20.754035Z","start_time":"2024-05-16T11:42:20.750092Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:20:11.131159Z","iopub.execute_input":"2024-05-18T10:20:11.131463Z","iopub.status.idle":"2024-05-18T10:20:11.148447Z","shell.execute_reply.started":"2024-05-18T10:20:11.131439Z","shell.execute_reply":"2024-05-18T10:20:11.147574Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def forward(self, x):\n        return nn.functional.layer_norm(x, x.size()[1:], weight=None, bias=None, eps=1e-05)\n\ndef pCnv(inp,out,groups=1):\n  return nn.Sequential(\n      nn.Conv2d(inp,out,1,bias=False,groups=groups),\n      nn.InstanceNorm2d(out,affine=True)\n  )\n\ndef dsCnv(inp,k):\n  return nn.Sequential(\n      nn.Conv2d(inp,inp,k,groups=inp,bias=False,padding=(k - 1) // 2),\n      nn.InstanceNorm2d(inp,affine=True)\n  )\n\nclass InitBlock(nn.Module):\n    def __init__(self, fup, num_channels):\n        super().__init__()\n        \n        self.n1 = LayerNorm()\n        self.InitSeq = nn.Sequential(\n            pCnv(num_channels, fup),\n            nn.Softmax(dim=1),\n            dsCnv(fup, 11),\n            LayerNorm()\n        )\n        \n    def forward(self, x):\n        x  = self.n1(x)\n        xt = x\n        x = self.InitSeq(x)\n        x = torch.cat([x, xt], dim=1)\n        return x\n\nclass Gate(nn.Module):\n    def __init__(self, ifsz):\n        super().__init__()\n        self.ln = LayerNorm()\n        \n    def forward(self, x):\n        t0, t1 = torch.chunk(x, 2, dim=1)\n        t0 = torch.tanh(t0)\n        t1.sub(2)\n        t1 = torch.sigmoid(t1)\n        \n        return t1 * t0\n\nclass GateBlock(nn.Module):\n    def __init__(self, ifsz, ofsz, gt = True, ksz = 3):\n        super().__init__()\n        \n        cfsz = int(math.floor(ifsz / 2))\n        ifsz2 = ifsz + ifsz%2\n        \n        self.sq = nn.Sequential(\n            pCnv(ifsz, cfsz),\n            dsCnv(cfsz, ksz),\n            nn.ELU(),\n            \n            pCnv(cfsz, cfsz * 2),\n            dsCnv(cfsz * 2, ksz),\n            Gate(cfsz),\n            \n            pCnv(cfsz, ifsz),\n            dsCnv(ifsz, ksz),\n            nn.ELU(),\n        )\n        \n        self.gt = gt\n        \n    def forward(self, x):\n        y = self.sq(x)\n        \n        out = x + y\n        return out\n    \nclass OrigamiNet(nn.Module):\n    def __init__(self, \n                 n_channels: int, \n                 label_enc: LabelParser, \n                 mul_rate, \n                 layer_resizes, \n                 layer_sizes, \n                 num_layers, \n                 fup, \n                 reduceAxis=3 ):\n        super().__init__()\n        \n        self.layer_resizes = layer_resizes\n        self.Init_sequence = InitBlock(fup, 1)\n        self.label_enc = label_enc\n        \n        self.cer_metric = CharacterErrorRate(label_enc)\n        self.wer_metric = WordErrorRate(label_enc)\n        \n        layers = []\n        input_size = fup + n_channels\n        output_size = input_size\n        \n        for i in range(num_layers):\n            output_size = int(math.floor(layer_sizes[i] * mul_rate) ) if i in layer_sizes else input_size\n            layers.append(GateBlock(input_size, output_size, True, 3))\n            \n            if input_size != output_size:\n                layers.append(pCnv(input_size, output_size))\n                layers.append(nn.ELU())\n            input_size = output_size\n            \n            if i in layer_resizes:\n                layers.append(layer_resizes[i])\n        \n        layers.append(LayerNorm())\n        self.Gatesq = nn.Sequential(*layers)\n        self.Finsq = nn.Sequential(\n            pCnv(output_size, self.label_enc.vocab_size),\n            nn.ELU()\n        )\n        \n        self.n1 = LayerNorm()\n        self.it = 0\n        self.reduceAxis = reduceAxis\n        self.loss_fn = nn.CTCLoss(reduction=\"none\", zero_infinity=True)\n        \n    def forward(self, image, targets: Optional[torch.Tensor]):\n        x = self.Init_sequence(image)\n        x = self.Gatesq(x)\n        x = self.Finsq(x)\n        \n        x = torch.mean(x, self.reduceAxis, keepdim=False)\n        x = self.n1(x)\n        x = x.permute(0, 2, 1)\n        if targets is not None:\n            logits = x.detach().cpu()\n            logits = logits.permute(1, 0, 2).log_softmax(2)\n            logits_size = torch.IntTensor([logits.size(1)] * targets.size(0))\n            targets_size = torch.IntTensor([targets.size(1)] * targets.size(0))\n            targets = targets.cpu()\n            loss = self.loss_fn(logits, targets, logits_size, targets_size).mean() \n            return x, loss\n        return x\n    \n    def calculate_metrics(self, preds: torch.Tensor, targets: torch.Tensor):\n        self.cer_metric.reset()\n        self.wer_metric.reset()\n        \n        cer = self.cer_metric(preds, targets)\n        wer = self.wer_metric(preds, targets)\n        \n        return {\"CER\": cer, \"WER\":wer}","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-05-16T11:42:20.764031Z","start_time":"2024-05-16T11:42:20.754686Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:43:01.005391Z","iopub.execute_input":"2024-05-18T10:43:01.005761Z","iopub.status.idle":"2024-05-18T10:43:01.032601Z","shell.execute_reply.started":"2024-05-18T10:43:01.005730Z","shell.execute_reply":"2024-05-18T10:43:01.031657Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def compute_loss(loss_fn: torch.CTCLoss, preds, targets):\n    logits = preds.detach().cpu()\n    logits = logits.permute(1, 0, 2).log_softmax(2)\n    logits_size = torch.IntTensor([logits.size(1)] * targets.size(0))\n    targets_size = torch.IntTensor([targets.size(1)] * targets.size(0))\n    loss = self.loss_fn(logits, targets, logits_size, targets_size).mean() \n    \n    return loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pytorch_lightning as pl\n\nclass LitOrigamiNet(pl.LightningModule):\n    model: OrigamiNet\n\n    \"\"\"\n    Pytorch Lightning module that acting as a wrapper around the\n    FullPageHTREncoderDecoder class.\n\n    Using a PL module allows the model to be used in conjunction with a Pytorch\n    Lightning Trainer, and takes care of logging relevant metrics to Tensorboard.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_channels: int, \n        label_encoder: LabelParser,\n        mul_rate: int, \n        layer_resizes: dict,\n        layer_sizes: dict,\n        num_layers: int,\n        fup: int,\n        reduce_axis:int = 3,\n        learning_rate: float = 0.0002,\n        params_to_log: Optional[Dict[str, Union[str, float, int]]] = None,\n    ):\n        super().__init__()\n\n        # Save hyperparameters.\n        self.learning_rate = learning_rate\n        if params_to_log is not None:\n            self.save_hyperparameters(params_to_log)\n        self.save_hyperparameters(\n            \"learning_rate\",\n            \"n_channels\",\n            \"num_layers\",\n        )\n\n        # Initialize the model.\n        self.model = OrigamiNet(\n            n_channels,\n            label_encoder,\n            mul_rate,\n            layer_resizes,\n            layer_sizes,\n            fup,\n            reduce_axis\n        )\n\n    def forward(self, imgs: Tensor, targets: Optional[Tensor] = None):\n        return self.model(imgs, targets)\n\n    def training_step(self, batch, batch_idx):\n        imgs, targets = batch\n        logits, loss = self.model(imgs, targets)\n        self.log(\"train_loss\", loss, sync_dist=True, prog_bar=False)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        return self.val_or_test_step(batch)\n\n    def test_step(self, batch, batch_idx):\n        return self.val_or_test_step(batch)\n\n    def val_or_test_step(self, batch) -> Tensor:\n        imgs, targets = batch\n        logits, loss = self(imgs, targets)\n        _, preds = logits.max(-1)\n\n        # Update and log metrics.\n        self.model.cer_metric(preds, targets)\n        self.model.wer_metric(preds, targets)\n        self.log(\"char_error_rate\", self.model.cer_metric, on_step=True ,prog_bar=True)\n        self.log(\"word_error_rate\", self.model.wer_metric, on_step=True ,prog_bar=True)\n        self.log(\"val_loss\", loss, sync_dist=True, prog_bar=True, on_step=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=self.learning_rate)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:43:13.147345Z","iopub.execute_input":"2024-05-18T10:43:13.147725Z","iopub.status.idle":"2024-05-18T10:43:13.161986Z","shell.execute_reply.started":"2024-05-18T10:43:13.147684Z","shell.execute_reply":"2024-05-18T10:43:13.160861Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.callbacks import Callback, ModelCheckpoint\nimport matplotlib.pyplot as plt\n\nPREDICTIONS_TO_LOG = {\n    \"word\": 10,\n    \"line\": 6,\n    \"form\": 1,\n}\n\n\nclass LogWorstPredictions(Callback):\n    \"\"\"\n    At the end of training, log the worst image prediction, meaning the predictions\n    with the highest character error rates.\n    \"\"\"\n\n    def __init__(\n        self,\n        train_dataloader: Optional[DataLoader] = None,\n        val_dataloader: Optional[DataLoader] = None,\n        test_dataloader: Optional[DataLoader] = None,\n        training_skipped: bool = False,\n        data_format: str = \"word\",\n    ):\n        self.train_dataloader = train_dataloader\n        self.val_dataloader = val_dataloader\n        self.test_dataloader = test_dataloader\n        self.training_skipped = training_skipped\n        self.data_format = data_format\n\n    def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n        if self.training_skipped and self.val_dataloader is not None:\n            self.log_worst_predictions(\n                self.val_dataloader, trainer, pl_module, mode=\"val\"\n            )\n\n    def on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n        if self.test_dataloader is not None:\n            self.log_worst_predictions(\n                self.test_dataloader, trainer, pl_module, mode=\"test\"\n            )\n\n    def on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n        if self.train_dataloader is not None:\n            self.log_worst_predictions(\n                self.train_dataloader, trainer, pl_module, mode=\"train\"\n            )\n        if self.val_dataloader is not None:\n            self.log_worst_predictions(\n                self.val_dataloader, trainer, pl_module, mode=\"val\"\n            )\n\n    def log_worst_predictions(\n        self,\n        dataloader: DataLoader,\n        trainer: \"pl.Trainer\",\n        pl_module: \"pl.LightningModule\",\n        mode: str = \"train\",\n    ):\n        img_cers = []\n        device = \"cuda:0\" if pl_module.on_gpu else \"cpu\"\n        if not self.training_skipped:\n            self._load_best_model(trainer, pl_module)\n            pl_module = trainer.model\n\n        print(f\"Running {mode} inference on best model...\")\n\n        # Run inference on the validation set.\n        pl_module.eval()\n        for img, target in dataloader:\n            assert target.ndim == 2, target.ndim\n            cer_metric = pl_module.model.cer_metric\n            with torch.inference_mode():\n                preds, _ = pl_module(img.to(device), target.to(device))\n#                 preds = preds.log_softmax(2)\n                preds = torch.max(preds, dim=-1)[1] # extract the predicted characters\n                for prd, tgt, im in zip(preds, target, img):\n                    cer_metric.reset()\n                    cer = cer_metric(prd.unsqueeze(0), tgt.unsqueeze(0)).item()\n                    img_cers.append((im, cer, prd, tgt))\n\n        # Log the worst k predictions.\n        to_log = PREDICTIONS_TO_LOG[self.data_format] * 2\n        img_cers.sort(key=lambda x: x[1], reverse=True)  # sort by CER\n        img_cers = img_cers[:to_log]\n        fig = plt.figure(figsize=(5, 5))\n        for i, (im, cer, prd, tgt) in enumerate(img_cers):\n            pred_str, target_str = decode_prediction_and_target(\n                prd, tgt, pl_module.model.label_encoder\n            )\n\n            # Create plot.\n            ncols = 4 if self.data_format == \"word\" else 2\n            nrows = math.ceil(to_log / ncols)\n            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n            matplotlib_imshow(im, IAMDataset.MEAN, IAMDataset.STD)\n            ax.set_title(f\"Pred: {pred_str} (CER: {cer:.2f})\\nTarget: {target_str}\")\n\n        # # Log the results to Tensorboard.\n        # tensorboard = trainer.logger.experiment\n        # tensorboard.add_figure(f\"{mode}: worst predictions\", fig, trainer.global_step)\n        trainer.logger.experiment.log({f\"{mode}: predictions vs targets\": wandb.Image(fig)})\n        plt.close(fig)\n\n        print(\"Done.\")\n\n    @staticmethod\n    def _load_best_model(trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n        ckpt_callback = None\n        for cb in trainer.callbacks:\n            if isinstance(cb, ModelCheckpoint):\n                ckpt_callback = cb\n                break\n        assert ckpt_callback is not None, \"ModelCheckpoint not found in callbacks.\"\n        best_model_path = ckpt_callback.best_model_path\n\n        print(f\"Loading best model at {best_model_path}\")\n        label_encoder = pl_module.model.label_encoder\n        model = LitOrigamiNet.load_from_checkpoint(\n            best_model_path,\n            label_encoder=label_encoder,\n        )\n        trainer.model.load_state_dict(model.state_dict())\n\n\nclass LogModelPredictions(Callback):\n    \"\"\"\n    Use a fixed test batch to monitor model predictions at the end of every epoch.\n\n    Specifically: it generates matplotlib Figure using a trained network, along with images\n    and labels from a batch, that shows the network's prediction alongside the actual target.\n    \"\"\"\n\n    def __init__(\n        self,\n        label_encoder: LabelParser,\n        val_batch: Tuple[torch.Tensor, torch.Tensor],\n        use_gpu: bool = True,\n        data_format: str = \"word\",\n        train_batch: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n    ):\n        self.label_encoder = label_encoder\n        self.val_batch = val_batch\n        self.use_gpu = use_gpu\n        self.data_format = data_format\n        self.train_batch = train_batch\n\n    def on_validation_epoch_end(\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n    ):\n        self._predict_intermediate(trainer, pl_module, split=\"val\")\n\n    def on_train_epoch_end(\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n    ):\n        if self.train_batch is not None:\n            self._predict_intermediate(trainer, pl_module, split=\"train\")\n\n    def _predict_intermediate(\n        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", split=\"val\"\n    ):\n        \"\"\"Make predictions on a fixed batch of data and log the results to Tensorboard.\"\"\"\n\n        # Make predictions.\n        if split == \"train\":\n            imgs, targets = self.train_batch\n        else:  # split == \"val\"\n            imgs, targets = self.val_batch\n        with torch.inference_mode():\n            pl_module.eval()\n            preds = pl_module(imgs.cuda() if self.use_gpu else imgs)\n#             preds = preds.log_softmax(2)\n            preds = torch.max(preds, dim=-1)[1] # get the predicted outputs\n\n        # Decode predictions and generate a plot.\n        fig = plt.figure(figsize=(5, 5))\n        for i, (p, t) in enumerate(zip(preds, targets)):\n            print(p.size(), t.size())\n            pred_str, target_str = decode_prediction_and_target(\n                p, t, self.label_encoder\n            )\n\n            # Create plot.\n            ncols = 2 if self.data_format == \"word\" else 1\n            nrows = math.ceil(preds.size(0) / ncols)\n            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n            matplotlib_imshow(imgs[i], IAMDataset.MEAN, IAMDataset.STD)\n            ax.set_title(f\"Pred: {pred_str}\\nTarget: {target_str}\")\n\n\n        trainer.logger.experiment.log({f\"{split}: predictions vs targets\": wandb.Image(fig)})\n        plt.close(fig)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:21:32.911506Z","iopub.execute_input":"2024-05-18T10:21:32.911986Z","iopub.status.idle":"2024-05-18T10:21:32.951065Z","shell.execute_reply.started":"2024-05-18T10:21:32.911945Z","shell.execute_reply":"2024-05-18T10:21:32.950106Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from copy import copy\nfrom pytorch_lightning import seed_everything\n\nseed_everything(12345)\niam_ds = IAMDataset(root=\"/kaggle/input/iam-rimes/data/raw/IAM\", label_enc=None, parse_method=\"form\" ,split=\"train\")\nrimes_ds = RIMESDataset(root=\"/kaggle/input/iam-rimes/data/raw/RIMES\", label_enc=iam_ds.label_enc, split=\"train\")\nds = AggregatedDataset(rimes_ds, iam_ds, split=\"train\", label_enc=rimes_ds.label_enc)\nprint(len(ds.label_enc.ctc_classes))\nprint(ds.label_enc.ctc_decode_labels([7]))\nprint(f\"Maximum target length for Aggregated dataset: {ds.get_max_target_len()}\")\nds_orig, ds_compl = torch.utils.data.random_split(ds, [math.ceil(0.5 * len(ds)), math.floor(0.5 * len(ds))])\n\nds_train, ds_val = torch.utils.data.random_split(ds_orig, [math.ceil(0.8 * len(ds_orig)), math.floor(0.2 * len(ds_orig))])\n\nds_val.data = copy(ds)\nds_val.data.set_transforms_for_split(\"val\")\ntrain_len = len(ds_train)\nval_len = len(ds_val)\nprint(train_len, val_len)\n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-05-16T11:42:22.343984Z","start_time":"2024-05-16T11:42:20.764696Z"},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:20:11.226719Z","iopub.execute_input":"2024-05-18T10:20:11.226976Z","iopub.status.idle":"2024-05-18T10:20:23.505761Z","shell.execute_reply.started":"2024-05-18T10:20:11.226953Z","shell.execute_reply":"2024-05-18T10:20:23.504740Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"113\n[\"'\"]\nMaximum target length for Aggregated dataset: 2043\n2388 597\n","output_type":"stream"}]},{"cell_type":"code","source":"\nbatch_size = 1\n\ncollate_fn = partial(\n        AggregatedDataset.unified_collate_fn\n)\nnum_workers = 2\ndl_train = DataLoader(\n    ds_train,\n    batch_size=batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=num_workers,\n    pin_memory=True,\n)\ndl_val = DataLoader(\n    ds_val,\n    batch_size= batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=num_workers,\n    pin_memory=True,\n)\ntrain_len //= batch_size\nval_len //= batch_size","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:36:20.234958Z","iopub.execute_input":"2024-05-18T10:36:20.235684Z","iopub.status.idle":"2024-05-18T10:36:20.242395Z","shell.execute_reply.started":"2024-05-18T10:36:20.235647Z","shell.execute_reply":"2024-05-18T10:36:20.241382Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ModelSummary\nfrom torch.utils.data import DataLoader, Subset\nfrom pytorch_lightning.loggers import WandbLogger\nimport wandb\nimport os\n\nwandb.login(key=\"0350b0cc5bd9521bb37a798168d31b6b65e9caca\")\nwandb_logger = WandbLogger(project=\"bach_thesis\", log_model=\"all\")\ncallbacks = [\n        ModelCheckpoint(\n            save_top_k=(3),\n            mode=\"min\",\n            monitor=\"word_error_rate\",\n            filename=\"{epoch}-{char_error_rate:.4f}-{word_error_rate:.4f}\",\n        ),\n        ModelSummary(max_depth=2),\n        LitProgressBar(),\n        LogWorstPredictions(\n            dl_train,\n            dl_val,\n            training_skipped=False,\n            data_format=\"form\",\n        ),\n        LogModelPredictions(\n            ds.label_enc,\n            val_batch=next(\n                iter(\n                    DataLoader(\n                        Subset(\n                            ds_val,\n                            random.sample(\n                                range(len(ds_val)), 1\n                            ),\n                        ),\n                        batch_size=1,\n                        shuffle=False,\n                        collate_fn=collate_fn,\n                        num_workers=num_workers,\n                        pin_memory=True,\n                    )\n                )\n            ),\n            train_batch=next(\n                iter(\n                    DataLoader(\n                        Subset(\n                            ds_train,\n                            random.sample(\n                                range(len(ds_train)),\n                                1,\n                            ),\n                        ),\n                        batch_size=1,\n                        shuffle=False,\n                        collate_fn=collate_fn,\n                        num_workers=num_workers,\n                        pin_memory=True,\n                    )\n                )\n            ),\n            data_format=\"form\",\n            use_gpu=True,\n        ),\n        EarlyStopping(\n                    monitor=\"word_error_rate\",\n                    patience=50,\n                    verbose=True,\n                    mode=\"min\",\n                    check_on_train_epoch_end=False,\n                )\n    ]\n\ndef get_gpu_memory_map():\n    result = os.popen('nvidia-smi --query-gpu=memory.used --format=csv,nounits,noheader').read()\n    return int(result.strip())\n\ntrainer = Trainer(\n    max_epochs=3000,\n    accelerator=\"gpu\", \n    devices=1,\n    callbacks = callbacks,\n    logger=wandb_logger,\n    fast_dev_run=False,\n    precision=\"16-mixed\")\nmodel = LitOrigamiNet(\n    n_channels = 1, \n    label_encoder=ds.label_enc,\n    mul_rate= 1.0, \n    layer_resizes= {\n            0: nn.MaxPool2d(2, 2),\n            2: nn.MaxPool2d(2, 2),\n            4: nn.MaxPool2d(2,2),\n            6: nn.ZeroPad2d(1),\n            8: nn.ZeroPad2d(1),\n            10: nn.Upsample((450, 15), align_corners=True, mode=\"bilinear\"),\n            11: nn.Upsample((1100, 8), align_corners=True, mode=\"bilinear\")\n        },\n    layer_sizes= {\n            0:  128,\n            2:  256,\n            4:  512,\n            11: 256\n        }, \n    num_layers=12, \n    fup=16,\n    learning_rate=0.001,\n)\n# trainer.fit(model, dl_train, dl_val)\nprint(f\"Memory Used: {get_gpu_memory_map()} MB\")\n\ntry:\n    # Start training\n    trainer.fit(model, dl_train, dl_val)\nexcept Exception as e:\n    # Catch and print any exceptions during training\n    print(f\"An error occurred: {e}\")\nfinally:\n    # Ensure wandb is properly closed\n    wandb.finish()\n\n    # Cleanup to free GPU memory, if the objects are not needed anymore\n    del model\n    torch.cuda.empty_cache()\n    print(\"Training finished, resources cleared.\")","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-05-18T10:43:17.806276Z","iopub.execute_input":"2024-05-18T10:43:17.807095Z","iopub.status.idle":"2024-05-18T10:43:44.867026Z","shell.execute_reply.started":"2024-05-18T10:43:17.807061Z","shell.execute_reply":"2024-05-18T10:43:44.865784Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Memory Used: 402 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20240518_104319-b5b79m96</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/stefannastasa/bach_thesis/runs/b5b79m96' target=\"_blank\">hardy-donkey-45</a></strong> to <a href='https://wandb.ai/stefannastasa/bach_thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/stefannastasa/bach_thesis' target=\"_blank\">https://wandb.ai/stefannastasa/bach_thesis</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/stefannastasa/bach_thesis/runs/b5b79m96' target=\"_blank\">https://wandb.ai/stefannastasa/bach_thesis/runs/b5b79m96</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"torch.Size([1100]) torch.Size([450])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e039358fda14898bcae804525f207cb"}},"metadata":{}},{"name":"stdout","text":"An error occurred: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 176.12 MiB is free. Process 6839 has 15.72 GiB memory in use. Of the allocated memory 15.19 GiB is allocated by PyTorch, and 246.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.034 MB of 0.034 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">hardy-donkey-45</strong> at: <a href='https://wandb.ai/stefannastasa/bach_thesis/runs/b5b79m96' target=\"_blank\">https://wandb.ai/stefannastasa/bach_thesis/runs/b5b79m96</a><br/> View project at: <a href='https://wandb.ai/stefannastasa/bach_thesis' target=\"_blank\">https://wandb.ai/stefannastasa/bach_thesis</a><br/>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240518_104319-b5b79m96/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training finished, resources cleared.\n","output_type":"stream"}]}]}
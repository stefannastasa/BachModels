{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "sourceId": 8149148,
     "sourceType": "datasetVersion",
     "datasetId": 4819409
    }
   ],
   "dockerImageVersionId": 30684,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Utils "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install editdistance torchmetrics pytorch_lightning\n",
    "!pip install  torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import math\n",
    "from random import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import cv2 as cv\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from random import randint\n",
    "import html\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, Dict, Sequence, Optional, List, Any, Callable, Optional\n",
    "import pandas as pd\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchmetrics import Metric\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset\n",
    "import editdistance\n",
    "import wandb\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class LabelParser:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.vocab_size = None\n",
    "        self.class_to_idx = None\n",
    "        self.idx_to_class = None\n",
    "\n",
    "    def fit(self, classes: Sequence[str]):\n",
    "        self.classes = list(classes)\n",
    "        self.vocab_size = len(classes)\n",
    "        self.idx_to_class = dict(enumerate(classes))\n",
    "        self.class_to_idx = {cls: i for i, cls in self.idx_to_class.items()}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def addClasses(self, classes: List[str]):\n",
    "        all_classes = sorted(set(self.classes + classes))\n",
    "\n",
    "        self.fit(all_classes)\n",
    "\n",
    "    def encode_labels(self, sequence: Sequence[str]):\n",
    "        self._check_fitted()\n",
    "        return [self.class_to_idx[c] for c in sequence]\n",
    "\n",
    "    def decode_labels(self, sequence: Sequence[int]):\n",
    "        self._check_fitted()\n",
    "        return [self.idx_to_class[c] for c in sequence]\n",
    "\n",
    "    def _check_fitted(self):\n",
    "        if self.classes is None:\n",
    "            raise ValueError(\"LabelParser class was not fitted yet\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.266332Z",
     "start_time": "2024-04-17T18:00:09.262338Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def pickle_load(file) -> Any:\n",
    "    with open(file, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def pickle_save(obj, file):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def read_xml(file: Union[Path, str]) -> ET.Element:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    return root\n",
    "\n",
    "def find_child_by_tag(element: ET.Element, tag: str, value: str) -> Union[ET.Element, None]:\n",
    "    for child in element:\n",
    "        if child.get(tag) == value:\n",
    "            return child\n",
    "    return None\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def dpi_adjusting(img: np.ndarray, scale: float, **kwargs) -> np.ndarray:\n",
    "    height, width = img.shape[:2]\n",
    "    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n",
    "    return cv.resize(img, (new_width, new_height))\n",
    "\n",
    "class LitProgressBar(TQDMProgressBar):\n",
    "    def get_metrics(self, trainer, model):\n",
    "        # don't show the version number\n",
    "        items = super().get_metrics(trainer, model)\n",
    "        for k in list(items.keys()):\n",
    "            if k.startswith(\"grad\"):\n",
    "                items.pop(k, None)\n",
    "        items.pop(\"v_num\", None)\n",
    "        return items\n",
    "    \n",
    "def decode_prediction_and_target(\n",
    "    pred: Tensor, target: Tensor, label_encoder: LabelParser, eos_tkn_idx: int\n",
    ") -> Tuple[str, str]:\n",
    "    # Find padding and <EOS> positions in predictions and targets.\n",
    "    eos_idx_pred = (pred == eos_tkn_idx).float().argmax().item()\n",
    "    eos_idx_tgt = (target == eos_tkn_idx).float().argmax().item()\n",
    "\n",
    "    # Decode prediction and target.\n",
    "    p, t = pred.tolist(), target.tolist()\n",
    "    p = p[1:]  # skip the initial <SOS> token, which is added by default\n",
    "    p = p[:eos_idx_pred] if eos_idx_pred != 0 else p\n",
    "    t = t[:eos_idx_tgt] if eos_idx_tgt != 0 else t\n",
    "    pred_str = \"\".join(label_encoder.decode_labels(p))\n",
    "    target_str = \"\".join(label_encoder.decode_labels(t))\n",
    "    return pred_str, target_str\n",
    "\n",
    "def matplotlib_imshow(\n",
    "    img: torch.Tensor, mean: float = 0.5, std: float = 0.5, one_channel=True\n",
    "):\n",
    "    assert img.device.type == \"cpu\"\n",
    "    if one_channel and img.ndim == 3:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img * std + mean  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.235185Z",
     "start_time": "2024-04-17T18:00:09.229244Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image transformations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "class SafeRandomScale(A.RandomScale):\n",
    "    def apply(self, img, scale=0, interpolation=cv.INTER_LINEAR, **params):\n",
    "        height, width = img.shape[:2]\n",
    "        new_height, new_width = int(height * scale), int(width * scale)\n",
    "        if new_height <= 0 or new_width <= 0:\n",
    "            return img\n",
    "        return super().apply(img, scale, interpolation, **params)\n",
    "\n",
    "def adjust_dpi(img: np.ndarray, scale: float, **kwargs):\n",
    "    height, width = img.shape\n",
    "    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n",
    "    return cv.resize(img, (new_width, new_height))\n",
    "\n",
    "def randomly_displace_and_pad(\n",
    "    img: np.ndarray,\n",
    "    padded_size: Tuple[int, int],\n",
    "    crop_if_necessary: bool = False,\n",
    "    **kwargs,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly displace an image within a frame, and pad zeros around the image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): image to process\n",
    "        padded_size (Tuple[int, int]): (height, width) tuple indicating the size of the frame\n",
    "        crop_if_necessary (bool): whether to crop the image if its size exceeds that\n",
    "            of the frame\n",
    "    \"\"\"\n",
    "    frame_h, frame_w = padded_size\n",
    "    img_h, img_w = img.shape\n",
    "    if frame_h < img_h or frame_w < img_w:\n",
    "        if crop_if_necessary:\n",
    "            print(\n",
    "                \"WARNING (`randomly_displace_and_pad`): cropping input image before \"\n",
    "                \"padding because it exceeds the size of the frame.\"\n",
    "            )\n",
    "            img_h, img_w = min(img_h, frame_h), min(img_w, frame_w)\n",
    "            img = img[:img_h, :img_w]\n",
    "        else:\n",
    "            raise AssertionError(\n",
    "                f\"Frame is smaller than the image: ({frame_h}, {frame_w}) vs. ({img_h},\"\n",
    "                f\" {img_w})\"\n",
    "            )\n",
    "\n",
    "    res = np.zeros((frame_h, frame_w), dtype=img.dtype)\n",
    "\n",
    "    pad_top =  randint(0, frame_h - img_h)\n",
    "    pad_bottom = pad_top + img_h\n",
    "    pad_left = randint(0, frame_w - img_w)\n",
    "    pad_right = pad_left + img_w\n",
    "\n",
    "    res[pad_top:pad_bottom, pad_left:pad_right] = img\n",
    "    return res\n",
    "\n",
    "@dataclass\n",
    "class ImageTransforms:\n",
    "    max_img_size: Tuple[int, int]  # (h, w)\n",
    "    normalize_params: Tuple[float, float]  # (mean, std)\n",
    "    scale: float = (\n",
    "        0.5\n",
    "    )\n",
    "    random_scale_limit: float = 0.1\n",
    "    random_rotate_limit: int = 10\n",
    "\n",
    "    train_trnsf: A.Compose = field(init=False)\n",
    "    test_trnsf: A.Compose = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        scale, random_scale_limit, random_rotate_limit, normalize_params =(\n",
    "            self.scale,\n",
    "            self.random_scale_limit,\n",
    "            self.random_rotate_limit,\n",
    "            self.normalize_params\n",
    "        )\n",
    "\n",
    "        max_img_h, max_img_w = self.max_img_size\n",
    "        max_scale = scale + scale * random_scale_limit\n",
    "        padded_h, padded_w = math.ceil(max_scale * max_img_h), math.ceil(max_scale * max_img_w)\n",
    "\n",
    "        self.train_trnsf = A.Compose([\n",
    "            A.Lambda(partial(adjust_dpi, scale=scale)),\n",
    "            SafeRandomScale(scale_limit=random_scale_limit, p=0.5),\n",
    "            A.SafeRotate(\n",
    "                limit = random_rotate_limit,\n",
    "                border_mode = cv.BORDER_CONSTANT,\n",
    "                value = 0\n",
    "            ),\n",
    "            A.RandomBrightnessContrast(),\n",
    "            A.Perspective(scale=(0.01, 0.05)),\n",
    "            A.GaussNoise(),\n",
    "            A.Normalize(*normalize_params),\n",
    "            A.Lambda(\n",
    "                image=partial(\n",
    "                    randomly_displace_and_pad,\n",
    "                    padded_size=(padded_h, padded_w),\n",
    "                    crop_if_necessary=False,\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.test_trnsf = A.Compose([\n",
    "            A.Lambda(partial(adjust_dpi, scale=scale)),\n",
    "            A.Normalize(*normalize_params),\n",
    "            A.PadIfNeeded(\n",
    "                max_img_h, max_img_w, border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "        ])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.286749Z",
     "start_time": "2024-04-17T18:00:09.276042Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IAM Dataset and Synthetic Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class IAMDataset(Dataset):\n",
    "    MEAN = 0.8275\n",
    "    STD = 0.2314\n",
    "    MAX_FORM_HEIGHT = 3542\n",
    "    MAX_FORM_WIDTH = 2479\n",
    "\n",
    "    MAX_SEQ_LENS = {\n",
    "        \"word\": 55,\n",
    "        \"line\": 90,\n",
    "        \"form\": 700,\n",
    "    }  # based on the maximum seq lengths found in the dataset\n",
    "\n",
    "    _pad_token = \"<PAD>\"\n",
    "    _sos_token = \"<SOS>\"\n",
    "    _eos_token = \"<EOS>\"\n",
    "\n",
    "    root: Path\n",
    "    data: pd.DataFrame\n",
    "    label_enc: LabelParser\n",
    "    parse_method: str\n",
    "    only_lowercase: bool\n",
    "    transforms: Optional[A.Compose]\n",
    "    id_to_idx: Dict[str, int]\n",
    "    _split: str\n",
    "    _return_writer_id: Optional[bool]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[Path, str],\n",
    "        parse_method: str,\n",
    "        split: str,\n",
    "        return_writer_id: bool = False,\n",
    "        only_lowercase: bool = False,\n",
    "        label_enc: Optional[LabelParser] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _parse_methods = [\"form\", \"line\", \"word\"]\n",
    "        err_message = (\n",
    "            f\"{parse_method} is not a possible parsing method: {_parse_methods}\"\n",
    "        )\n",
    "        assert parse_method in _parse_methods, err_message\n",
    "\n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "\n",
    "        self._split = split\n",
    "        self._return_writer_id = return_writer_id\n",
    "        self.only_lowercase = only_lowercase\n",
    "        self.root = Path(root)\n",
    "        self.label_enc = label_enc\n",
    "        self.parse_method = parse_method\n",
    "\n",
    "        # Process the data.\n",
    "        if not hasattr(self, \"data\"):\n",
    "            if self.parse_method == \"form\":\n",
    "                self.data = self._get_forms()\n",
    "            elif self.parse_method == \"word\":\n",
    "                self.data = self._get_words(skip_bad_segmentation=True)\n",
    "            elif self.parse_method == \"line\":\n",
    "                self.data = self._get_lines()\n",
    "\n",
    "        # Create the label encoder.\n",
    "        if self.label_enc is None:\n",
    "            vocab = [self._pad_token, self._sos_token, self._eos_token]\n",
    "            s = \"\".join(self.data[\"target\"].tolist())\n",
    "            if self.only_lowercase:\n",
    "                s = s.lower()\n",
    "            vocab += sorted(list(set(s)))\n",
    "            self.label_enc = LabelParser().fit(vocab)\n",
    "        if not \"target_enc\" in self.data.columns:\n",
    "            self.data.insert(\n",
    "                2,\n",
    "                \"target_enc\",\n",
    "                self.data[\"target\"].apply(\n",
    "                    lambda s: np.array(\n",
    "                        self.label_enc.encode_labels(\n",
    "                            [c for c in (s.lower() if self.only_lowercase else s)]\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.transforms = self._get_transforms(split)\n",
    "        self.id_to_idx = {\n",
    "            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n",
    "        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n",
    "            # Crop the image vertically.\n",
    "            img = img[data[\"bb_y_start\"] : data[\"bb_y_end\"], :]\n",
    "        assert isinstance(img, np.ndarray), (\n",
    "            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n",
    "            f\"Is there something wrong with this image?\"\n",
    "        )\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        if self._return_writer_id:\n",
    "            return img, data[\"writer_id\"], data[\"target_enc\"]\n",
    "        return img, data[\"target_enc\"]\n",
    "\n",
    "    def get_max_height(self):\n",
    "        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.label_enc.classes\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(\n",
    "        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n",
    "        pad_val: int,\n",
    "        eos_tkn_idx: int,\n",
    "        dataset_returns_writer_id: bool = False,\n",
    "    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n",
    "        if dataset_returns_writer_id:\n",
    "            imgs, writer_ids, targets = zip(*batch)\n",
    "        else:\n",
    "            imgs, targets = zip(*batch)\n",
    "\n",
    "        img_sizes = [im.shape for im in imgs]\n",
    "        if (\n",
    "            not len(set(img_sizes)) == 1\n",
    "        ):  # images are of varying sizes, so pad them to the maximum size in the batch\n",
    "            hs, ws = zip(*img_sizes)\n",
    "            pad_fn = A.PadIfNeeded(\n",
    "                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "\n",
    "        seq_lengths = [t.shape[0] for t in targets]\n",
    "        targets_padded = np.full((len(targets), max(seq_lengths) + 1), pad_val)\n",
    "        for i, t in enumerate(targets):\n",
    "            targets_padded[i, : seq_lengths[i]] = t\n",
    "            targets_padded[i, seq_lengths[i]] = eos_tkn_idx\n",
    "\n",
    "        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n",
    "        if dataset_returns_writer_id:\n",
    "            return imgs, targets_padded, torch.tensor(writer_ids)\n",
    "        return imgs, targets_padded\n",
    "\n",
    "    def set_transforms_for_split(self, split: str):\n",
    "        _splits = [\"train\", \"val\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        self.transforms = self._get_transforms(split)\n",
    "\n",
    "    def _get_transforms(self, split: str) -> A.Compose:\n",
    "        max_img_w = self.MAX_FORM_WIDTH\n",
    "\n",
    "        if self.parse_method == \"form\":\n",
    "            max_img_h = (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n",
    "        else:  # word or line\n",
    "            max_img_h = self.MAX_FORM_HEIGHT\n",
    "\n",
    "        transforms = ImageTransforms(\n",
    "            (max_img_h, max_img_w), (IAMDataset.MEAN, IAMDataset.STD)\n",
    "        )\n",
    "\n",
    "        if split == \"train\":\n",
    "            return transforms.train_trnsf\n",
    "        elif split == \"test\" or split == \"val\":\n",
    "            return transforms.test_trnsf\n",
    "\n",
    "    def statistics(self) -> Dict[str, float]:\n",
    "        assert len(self) > 0\n",
    "        tmp = self.transforms\n",
    "        self.transforms = None\n",
    "        mean, std, cnt = 0, 0, 0\n",
    "        for img, _ in self:\n",
    "            mean += np.mean(img)\n",
    "            std += np.var(img)\n",
    "            cnt += 1\n",
    "        mean /= cnt\n",
    "        std = np.sqrt(std / cnt)\n",
    "        self.transforms = tmp\n",
    "        return {\"mean\": mean, \"std\": std}\n",
    "\n",
    "    def _get_forms(self) -> pd.DataFrame:\n",
    "        \"\"\"Read all form images from the IAM dataset.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame\n",
    "                A pandas dataframe containing the image path, image id, target, vertical\n",
    "                upper bound, vertical lower bound, and target length.\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"img_path\": [],\n",
    "            \"img_id\": [],\n",
    "            \"target\": [],\n",
    "            \"bb_y_start\": [],\n",
    "            \"bb_y_end\": [],\n",
    "            \"target_len\": [],\n",
    "        }\n",
    "        for form_dir in [\"formsA-D\", \"formsE-H\", \"formsI-Z\"]:\n",
    "            dr = self.root / form_dir\n",
    "            for img_path in dr.iterdir():\n",
    "                doc_id = img_path.stem\n",
    "                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n",
    "\n",
    "                # Based on some empiricial evaluation, the 'asy' and 'dsy'\n",
    "                # attributes of a line xml tag seem to correspond to its upper and\n",
    "                # lower bound, respectively. We add padding of 150 pixels.\n",
    "                bb_y_start = int(xml_root[1][0].get(\"asy\")) - 150\n",
    "                bb_y_end = int(xml_root[1][-1].get(\"dsy\")) + 150\n",
    "\n",
    "                form_text = []\n",
    "                for line in xml_root.iter(\"line\"):\n",
    "                    form_text.append(html.unescape(line.get(\"text\", \"\")))\n",
    "\n",
    "                img_w, img_h = Image.open(str(img_path)).size\n",
    "                data[\"img_path\"].append(str(img_path))\n",
    "                data[\"img_id\"].append(doc_id)\n",
    "                data[\"target\"].append(\"\\n\".join(form_text))\n",
    "                data[\"bb_y_start\"].append(bb_y_start)\n",
    "                data[\"bb_y_end\"].append(bb_y_end)\n",
    "                data[\"target_len\"].append(len(\"\\n\".join(form_text)))\n",
    "        return pd.DataFrame(data).sort_values(\n",
    "            \"target_len\"\n",
    "        )  # by default, sort by target length\n",
    "\n",
    "    def _get_lines(self, skip_bad_segmentation: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Read all line images from the IAM dataset.\n",
    "\n",
    "        Args:\n",
    "            skip_bad_segmentation (bool): skip lines that have the\n",
    "                segmentation='err' xml attribute\n",
    "        Returns:\n",
    "            List of 2-tuples, where each tuple contains the path to a line image\n",
    "            along with its ground truth text.\n",
    "        \"\"\"\n",
    "        data = {\"img_path\": [], \"img_id\": [], \"target\": []}\n",
    "        root = self.root / \"lines\"\n",
    "        for d1 in root.iterdir():\n",
    "            for d2 in d1.iterdir():\n",
    "                doc_id = d2.name\n",
    "                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n",
    "                for img_path in d2.iterdir():\n",
    "                    target = self._find_line(\n",
    "                        xml_root, img_path.stem, skip_bad_segmentation\n",
    "                    )\n",
    "                    if target is not None:\n",
    "                        data[\"img_path\"].append(str(img_path.resolve()))\n",
    "                        data[\"img_id\"].append(doc_id)\n",
    "                        data[\"target\"].append(target)\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def _get_words(self, skip_bad_segmentation: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Read all word images from the IAM dataset.\n",
    "\n",
    "        Args:\n",
    "            skip_bad_segmentation (bool): skip lines that have the\n",
    "                segmentation='err' xml attribute\n",
    "        Returns:\n",
    "            List of 2-tuples, where each tuple contains the path to a word image\n",
    "            along with its ground truth text.\n",
    "        \"\"\"\n",
    "        data = {\"img_path\": [], \"img_id\": [], \"writer_id\": [], \"target\": []}\n",
    "        root = self.root / \"words\"\n",
    "        parallel_inputs = []\n",
    "        for d1 in root.iterdir():\n",
    "            if d1.is_file():\n",
    "                continue\n",
    "            for d2 in d1.iterdir():\n",
    "                parallel_inputs.append(d2)\n",
    "\n",
    "        def process_dir(directory):\n",
    "            directory_results = {\"img_path\": [], \"img_id\": [], \"writer_id\": [], \"target\": []}\n",
    "            doc_id = directory.name\n",
    "            xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n",
    "            writer_id = int(xml_root.get(\"writer-id\"))\n",
    "            for img_path in directory.iterdir():\n",
    "                img = cv.imread(str(img_path.resolve()), cv.IMREAD_GRAYSCALE)\n",
    "\n",
    "                if isinstance(img, np.ndarray):\n",
    "                    target = self._find_word(\n",
    "                        xml_root, img_path.stem, skip_bad_segmentation\n",
    "                    )\n",
    "                    if target is not None:\n",
    "                        directory_results[\"img_path\"].append(str(img_path.resolve()))\n",
    "                        directory_results[\"img_id\"].append(doc_id)\n",
    "                        directory_results[\"writer_id\"].append(writer_id)\n",
    "                        directory_results[\"target\"].append(target)\n",
    "            return directory_results\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(process_dir, iter(parallel_inputs)), total=len(parallel_inputs)))\n",
    "            for single_results in results:\n",
    "                data[\"img_path\"].extend(single_results[\"img_path\"])\n",
    "                data[\"img_id\"].extend(single_results[\"img_id\"])\n",
    "                data[\"writer_id\"].extend(single_results[\"writer_id\"])\n",
    "                data[\"target\"].extend(single_results[\"target\"])\n",
    "                \n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def _find_line(\n",
    "        self,\n",
    "        xml_root: ET.Element,\n",
    "        line_id: str,\n",
    "        skip_bad_segmentation: bool = False,\n",
    "    ) -> Union[str, None]:\n",
    "        line = find_child_by_tag(xml_root[1].findall(\"line\"), \"id\", line_id)\n",
    "        if line is not None and not (\n",
    "            skip_bad_segmentation and line.get(\"segmentation\") == \"err\"\n",
    "        ):\n",
    "            return html.unescape(line.get(\"text\"))\n",
    "        return None\n",
    "\n",
    "    def _find_word(\n",
    "        self,\n",
    "        xml_root: ET.Element,\n",
    "        word_id: str,\n",
    "        skip_bad_segmentation: bool = False,\n",
    "    ) -> Union[str, None]:\n",
    "        line_id = \"-\".join(word_id.split(\"-\")[:-1])\n",
    "        line = find_child_by_tag(xml_root[1].findall(\"line\"), \"id\", line_id)\n",
    "        if line is not None and not (\n",
    "            skip_bad_segmentation and line.get(\"segmentation\") == \"err\"\n",
    "        ):\n",
    "            word = find_child_by_tag(line.findall(\"word\"), \"id\", word_id)\n",
    "            if word is not None:\n",
    "                return html.unescape(word.get(\"text\"))\n",
    "        return None\n",
    "\n",
    "class IAMSyntheticDataGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    Data generator that creates synthetic line/form images by stitching together word\n",
    "    images from the IAM dataset.\n",
    "    Calling `__getitem__()` samples a newly generated synthetic image every time\n",
    "    it is called.\n",
    "    \"\"\"\n",
    "\n",
    "    PUNCTUATION = [\",\", \".\", \";\", \":\", \"'\", '\"', \"!\", \"?\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        iam_root: Union[str, Path],\n",
    "        label_encoder: Optional[LabelParser] = None,\n",
    "        transforms: Optional[A.Compose] = None,\n",
    "        line_width: Tuple[int, int] = (1500, 2000),\n",
    "        lines_per_form: Tuple[int, int] = (1, 11),\n",
    "        words_per_line: Tuple[int, int] = (4, 10),\n",
    "        words_per_sequence: Tuple[int, int] = (7, 13),\n",
    "        px_between_lines: Tuple[int, int] = (25, 50),\n",
    "        px_between_words: int = 50,\n",
    "        px_around_image: Tuple[int, int] = (100, 200),\n",
    "        sample_form: bool = False,\n",
    "        only_lowercase: bool = False,\n",
    "        rng_seed: int = 0,\n",
    "        max_height: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.iam_root = iam_root\n",
    "        self.label_enc = label_encoder\n",
    "        self.transforms = transforms\n",
    "        self.line_width = line_width\n",
    "        self.lines_per_form = lines_per_form\n",
    "        self.words_per_line = words_per_line\n",
    "        self.words_per_sequence = words_per_sequence\n",
    "        self.px_between_lines = px_between_lines\n",
    "        self.px_between_words = px_between_words\n",
    "        self.px_around_image = px_around_image\n",
    "        self.sample_form = sample_form\n",
    "        self.only_lowercase = only_lowercase\n",
    "        self.rng_seed = rng_seed\n",
    "        self.max_height = max_height\n",
    "\n",
    "        self.iam_words = IAMDataset(\n",
    "            iam_root,\n",
    "            \"word\",\n",
    "            \"test\",\n",
    "            only_lowercase=only_lowercase,\n",
    "        )\n",
    "        if self.max_height is None:\n",
    "            self.max_height = IAMDataset.MAX_FORM_HEIGHT\n",
    "        if sample_form and \"\\n\" not in self.label_encoder.classes:\n",
    "            # Add the `\\n` token to the label encoder (since forms can contain newlines)\n",
    "            self.label_encoder.addClasses([\"\\n\"])\n",
    "        self.iam_words.transforms = None\n",
    "        self.rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        # This dataset does not have a finite length since it can generate random\n",
    "        # images at will, so return 1.\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def label_encoder(self):\n",
    "        if self.label_enc is not None:\n",
    "            return self.label_enc\n",
    "        return self.iam_words.label_enc\n",
    "\n",
    "    def __getitem__(self, *args, **kwargs):\n",
    "        \"\"\"By calling this method, a newly generated synthetic image is sampled.\"\"\"\n",
    "        if self.sample_form:\n",
    "            img, target = self.generate_form()\n",
    "        else:\n",
    "            img, target = self.generate_line()\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        # Encode the target sequence using the label encoder.\n",
    "        target_enc = np.array(self.label_encoder.encode_labels([c for c in target]))\n",
    "        return img, target_enc\n",
    "\n",
    "    def generate_line(self) -> Tuple[np.ndarray, str]:\n",
    "        words_to_sample = self.rng.integers(*self.words_per_line)\n",
    "        line_width = self.rng.integers(*self.line_width)\n",
    "        return self.sample_lines(words_to_sample, line_width, sample_one_line=True)\n",
    "\n",
    "    def generate_form(self) -> Tuple[np.ndarray, str]:\n",
    "        # Randomly pick the number of words and inter-line distance in the form.\n",
    "        words_to_sample = self.rng.integers(*self.lines_per_form) * 5  # 7 is handpicked\n",
    "        px_between_lines = self.rng.integers(*self.px_between_lines)\n",
    "\n",
    "        # Sample line images.\n",
    "        line_width = self.rng.integers(*self.line_width)\n",
    "        lines, target = self.sample_lines(words_to_sample, line_width)\n",
    "\n",
    "        # Concatenate the lines vertically.\n",
    "        form_w = max(l.shape[1] for l in lines)\n",
    "        form_h = sum(l.shape[0] + px_between_lines for l in lines)\n",
    "        if form_h > self.max_height:\n",
    "            print(\n",
    "                \"Generated form height exceeds maximum height. Generating a new form.\"\n",
    "            )\n",
    "            return self.generate_form()\n",
    "        form = np.ones((form_h, form_w), dtype=lines[0].dtype) * 255\n",
    "        curr_h = 0\n",
    "        for line_img in lines:\n",
    "            h, w = line_img.shape\n",
    "            if curr_h + h + px_between_lines > self.max_height:\n",
    "                break\n",
    "\n",
    "            form[curr_h : curr_h + h, :w] = line_img\n",
    "            curr_h += h + px_between_lines\n",
    "\n",
    "        # Add a random amount of padding around the image.\n",
    "        pad_px = self.rng.integers(*self.px_around_image)\n",
    "        new_h, new_w = form.shape[0] + pad_px * 2, form.shape[1] + pad_px * 2\n",
    "        form = A.PadIfNeeded(\n",
    "            new_h, new_w, border_mode=cv.BORDER_CONSTANT, value=255, always_apply=True\n",
    "        )(image=form)[\"image\"]\n",
    "\n",
    "        return form, target\n",
    "\n",
    "    def set_rng(self, seed: int):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def sample_word_image(self) -> Tuple[np.ndarray, str]:\n",
    "        idx = random.randint(0, len(self.iam_words) - 1)\n",
    "        img, target = self.iam_words[idx]\n",
    "        target = \"\".join(self.iam_words.label_enc.decode_labels(target))\n",
    "        return img, target\n",
    "\n",
    "    def sample_word_image_sequence(\n",
    "        self, words_to_sample: int\n",
    "    ) -> List[Tuple[np.ndarray, str]]:\n",
    "        \"\"\"Sample a sequence of contiguous words.\"\"\"\n",
    "        assert words_to_sample >= 1\n",
    "        start_idx = random.randint(0, len(self.iam_words) - 1)\n",
    "\n",
    "        img_idxs = [start_idx]\n",
    "        img_path = Path(self.iam_words.data.iloc[start_idx][\"img_path\"])\n",
    "        _, _, line_id, word_id = img_path.stem.split(\"-\")\n",
    "        sampled_words = 1\n",
    "        while sampled_words < words_to_sample:\n",
    "            word_id = f\"{int(word_id) + 1 :02}\"\n",
    "            img_name = (\n",
    "                \"-\".join(img_path.stem.split(\"-\")[:-2] + [line_id, word_id]) + \".png\"\n",
    "            )\n",
    "            if not (img_path.parent / img_name).is_file():\n",
    "                # Previous image was the last on its line. Go to the next line.\n",
    "                line_id = f\"{int(line_id) + 1 :02}\"\n",
    "                word_id = \"00\"\n",
    "                img_name = (\n",
    "                    \"-\".join(img_path.stem.split(\"-\")[:-2] + [line_id, word_id])\n",
    "                    + \".png\"\n",
    "                )\n",
    "            if not (img_path.parent / img_name).is_file():\n",
    "                # End of the document.\n",
    "                return self.sample_word_image_sequence(words_to_sample)\n",
    "            # Find the dataset index for the sampled word.\n",
    "            ix = self.iam_words.id_to_idx.get(Path(img_name).stem)\n",
    "            if ix is None:\n",
    "                # If the image has segmentation=err attribute, it will\n",
    "                # not be in the dataset. In this case try again.\n",
    "                return self.sample_word_image_sequence(words_to_sample)\n",
    "            img_idxs.append(ix)\n",
    "            sampled_words += 1\n",
    "\n",
    "        imgs, targets = zip(*[self.iam_words[idx] for idx in img_idxs])\n",
    "        targets = [\n",
    "            \"\".join(self.iam_words.label_enc.decode_labels(t)) for t in targets\n",
    "        ]\n",
    "        return list(zip(imgs, targets))\n",
    "\n",
    "    def sample_lines(\n",
    "        self, words_to_sample: int, max_line_width: int, sample_one_line: bool = False\n",
    "    ) -> Tuple[Union[List[np.ndarray], np.ndarray], str]:\n",
    "        \"\"\"\n",
    "        Calls `sample_word_image_sequence` several times, using some heuristics\n",
    "        to glue the sequences together.\n",
    "\n",
    "        Returns:\n",
    "            - list of line images\n",
    "            - transcription for all lines combined\n",
    "        \"\"\"\n",
    "        curr_pos, sampled_words = 0, 0\n",
    "        imgs, targets, lines = [], [], []\n",
    "        target_str, last_target = \"\", \"\"\n",
    "\n",
    "        # Sample images.\n",
    "        while sampled_words < words_to_sample:\n",
    "            words_per_seq = self.rng.integers(*self.words_per_sequence)\n",
    "            # Sample a sequence of contiguous words.\n",
    "            img_tgt_seq = self.sample_word_image_sequence(words_per_seq)\n",
    "            for i, (img, tgt) in enumerate(img_tgt_seq):\n",
    "                # Add the sequence to the sampled words so far.\n",
    "                if sampled_words >= words_to_sample:\n",
    "                    break\n",
    "                h, w = img.shape\n",
    "\n",
    "                if curr_pos + w > max_line_width:\n",
    "                    # Concatenate the sampled images into a line.\n",
    "                    line = self.concatenate_line(imgs, targets, max_line_width)\n",
    "\n",
    "                    if sample_one_line:\n",
    "                        return line, target_str\n",
    "\n",
    "                    lines.append(line)\n",
    "                    target_str += \"\\n\"\n",
    "                    last_target = \"\\n\"\n",
    "                    curr_pos = 0\n",
    "                    imgs, targets = [], []\n",
    "\n",
    "                # Basic heuristics to avoid some strange looking sentences.\n",
    "                if i == 0 and (\n",
    "                    (last_target in self.PUNCTUATION and tgt in self.PUNCTUATION)\n",
    "                    or (tgt in self.PUNCTUATION and sampled_words == 0)\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                if (\n",
    "                    sampled_words == 0\n",
    "                    or tgt in [c for c in self.PUNCTUATION if c not in [\"'\", '\"']]\n",
    "                    or last_target == \"\\n\"\n",
    "                ):\n",
    "                    target_str += tgt\n",
    "                else:\n",
    "                    target_str += \" \" + tgt\n",
    "\n",
    "                targets.append(tgt)\n",
    "                imgs.append(img)\n",
    "\n",
    "                sampled_words += 1\n",
    "                last_target = tgt\n",
    "                if tgt in self.PUNCTUATION:\n",
    "                    # Reduce horizontal spacing for punctuation tokens.\n",
    "                    curr_pos = max(0, curr_pos - self.px_between_words)\n",
    "                curr_pos += w + self.px_between_words\n",
    "        if imgs and targets:\n",
    "            # Concatenate the remaining images into a new line.\n",
    "            line = self.concatenate_line(imgs, targets, max_line_width)\n",
    "            lines.append(line)\n",
    "            if sample_one_line:\n",
    "                return line, target_str\n",
    "        return lines, target_str\n",
    "\n",
    "    def concatenate_line(\n",
    "        self, imgs: List[np.ndarray], targets: List[str], line_width: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Concatenate a series of (img, target) tuples into a line to create a line image.\n",
    "        \"\"\"\n",
    "        assert len(imgs) == len(targets)\n",
    "\n",
    "        line_height = max(im.shape[0] for im in imgs)\n",
    "        line = np.ones((line_height, line_width), dtype=imgs[0].dtype) * 255\n",
    "\n",
    "        curr_pos = 0\n",
    "        prev_lower_bound = line_height\n",
    "        for img, tgt in zip(imgs, targets):\n",
    "            h, w = img.shape\n",
    "            # Center the image in the middle of the line.\n",
    "            start_h = min(max(0, int((line_height - h) / 2)), line_height - h)\n",
    "\n",
    "            if tgt in [\",\", \".\"]:\n",
    "                # If sampled a comma or dot, place them at the bottom of the line.\n",
    "                start_h = min(max(0, prev_lower_bound - int(h / 2)), line_height - h)\n",
    "            elif tgt in ['\"', \"'\"]:\n",
    "                # If sampled a quote, place them at the top of the line.\n",
    "                start_h = 0\n",
    "            if tgt in self.PUNCTUATION:\n",
    "                # Reduce horizontal spacing for punctuation tokens.\n",
    "                curr_pos = max(0, curr_pos - self.px_between_words)\n",
    "\n",
    "            assert curr_pos + w <= line_width, f\"{curr_pos + w} > {line_width}\"\n",
    "            assert start_h + h <= line_height, f\"{start_h + h} > {line_height}\"\n",
    "\n",
    "            # Concatenate the word image to the line.\n",
    "            line[start_h : start_h + h, curr_pos : curr_pos + w] = img\n",
    "\n",
    "            curr_pos += w + self.px_between_words\n",
    "            prev_lower_bound = start_h + h\n",
    "        return line\n",
    "\n",
    "    @staticmethod\n",
    "    def get_worker_init_fn():\n",
    "        def worker_init_fn(worker_id: int):\n",
    "            set_seed(worker_id)\n",
    "            worker_info = torch.utils.data.get_worker_info()\n",
    "            dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "            if hasattr(dataset, \"set_rng\"):\n",
    "                dataset.set_rng(worker_id)\n",
    "            else:  # dataset is instance of `IAMDatasetSynthetic` class\n",
    "                dataset.synth_dataset.set_rng(worker_id)\n",
    "\n",
    "        return worker_init_fn\n",
    "\n",
    "\n",
    "\n",
    "class IAMDatasetSynthetic(Dataset):\n",
    "    \"\"\"\n",
    "    A Pytorch dataset combining the IAM dataset with the IAMSyntheticDataGenerator\n",
    "    dataset.\n",
    "\n",
    "    The distribution of real/synthetic images can be controlled by setting the\n",
    "    `synth_prob` argument.\n",
    "    \"\"\"\n",
    "\n",
    "    iam_dataset: IAMDataset\n",
    "    synth_dataset: IAMSyntheticDataGenerator\n",
    "\n",
    "    def __init__(self, iam_dataset: IAMDataset, synth_prob: float = 0.3, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            iam_dataset (Dataset): the IAM dataset to sample from\n",
    "            synth_prob (float): the probability of sampling a synthetic image when\n",
    "                calling `__getitem__()`.\n",
    "        \"\"\"\n",
    "        self.iam_dataset = iam_dataset\n",
    "        self.synth_prob = synth_prob\n",
    "        self.synth_dataset = IAMSyntheticDataGenerator(\n",
    "            iam_root=iam_dataset.root,\n",
    "            label_encoder=iam_dataset.label_enc,\n",
    "            transforms=iam_dataset.transforms,\n",
    "            sample_form=(True if iam_dataset.parse_method == \"form\" else False),\n",
    "            only_lowercase=iam_dataset.only_lowercase,\n",
    "            max_height=(\n",
    "                (iam_dataset.data[\"bb_y_end\"] - iam_dataset.data[\"bb_y_start\"]).max()\n",
    "                if iam_dataset.parse_method == \"form\"\n",
    "                else None\n",
    "            ),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        iam = self.iam_dataset\n",
    "        if random.random() > 1 - self.synth_prob:\n",
    "            # Sample from the synthetic dataset.\n",
    "            img, target = self.synth_dataset[0]\n",
    "        else:\n",
    "            # Index the IAM dataset.\n",
    "            img, target = iam[idx]\n",
    "        assert not np.any(np.isnan(img)), img\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.iam_dataset)\n",
    "    \n",
    "    \n",
    "import time\n",
    "class RIMESDataset(Dataset):\n",
    "    MEAN = 0.8275\n",
    "    STD = 0.2314\n",
    "    \n",
    "    root: Path\n",
    "    data: pd.DataFrame\n",
    "    label_enc: LabelParser\n",
    "    transforms: Optional[A.Compose]\n",
    "    id_to_idx: Dict[str, int]\n",
    "    _split: str\n",
    "    _return_writer_id: Optional[bool]\n",
    "    \n",
    "    _pad_token = \"<PAD>\"\n",
    "    _sos_token = \"<SOS>\"\n",
    "    _eos_token = \"<EOS>\"\n",
    "    \n",
    "    max_width: Optional[int]\n",
    "    max_height: Optional[int]\n",
    "    \n",
    "    @staticmethod\n",
    "    def process_target(target: str):\n",
    "    # Splitting the input string into lines\n",
    "        lines = target.split(\"\\\\n\")\n",
    "        \n",
    "        new_lines = []\n",
    "        for line in lines:\n",
    "            new_line = line\n",
    "            start_index = new_line.find(\"¤{\")\n",
    "            \n",
    "            while start_index != -1:\n",
    "                # Find the corresponding closing bracket\n",
    "                end_index = new_line.find(\"¤\", start_index + 1)\n",
    "                if end_index == -1:\n",
    "                    break  # Safety check\n",
    "    \n",
    "                seq = new_line[start_index + 2:end_index]\n",
    "                choices = seq.split(\"/\")\n",
    "                val = choices[randint(0, len(choices) - 1)]\n",
    "    \n",
    "                new_line = new_line[:start_index] + \" \" + val + \" \" + new_line[end_index + 1:]\n",
    "                \n",
    "                start_index = new_line.find(\"¤{\", start_index + 1)\n",
    "            \n",
    "            new_lines.append(new_line)\n",
    "        \n",
    "        return \"\\n\".join(new_lines)\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            root: Union[Path, str],\n",
    "            split: str, \n",
    "            only_lowercase: bool = False,\n",
    "            label_enc: Optional[LabelParser] = None,):\n",
    "        super().__init__()\n",
    "        \n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        \n",
    "        self._split = split\n",
    "        self.only_lowercase = only_lowercase\n",
    "        self.root = Path(root)\n",
    "        self.label_enc = label_enc\n",
    "        \n",
    "        if not hasattr(self, \"data\"):\n",
    "            self.data = self._get_form_data()\n",
    "        \n",
    "        if self.label_enc is None:\n",
    "            vocab = [self._pad_token, self._sos_token, self._eos_token]\n",
    "            s = \"\".join(self.data[\"target\"].tolist())\n",
    "            if self.only_lowercase:\n",
    "                s = s.lower()\n",
    "            vocab += sorted(list(set(s)))\n",
    "            self.label_enc = LabelParser().fit(vocab)\n",
    "            \n",
    "        if not \"target_enc\" in self.data.columns:\n",
    "            self.data.insert(\n",
    "                2,\n",
    "                \"target_enc\",\n",
    "                self.data[\"target\"].apply(\n",
    "                    lambda s: np.array(\n",
    "                        self.label_enc.encode_labels(\n",
    "                            [c for c in (s.lower() if self.only_lowercase else s)]\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self.transforms = self._get_transforms(split)\n",
    "        self.id_to_idx = {\n",
    "            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n",
    "            img = img[data[\"bb_y_start\"]: data[\"bb_y_end\"], :]\n",
    "        assert isinstance(img, np.ndarray), (\n",
    "            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n",
    "            f\"Is there something wrong with this image?\"\n",
    "        )\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        \n",
    "        return img, data[\"target_enc\"]\n",
    "    \n",
    "    def get_max_height(self):\n",
    "        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max() + 150\n",
    "    \n",
    "    def get_max_width(self):\n",
    "        return (self.data[\"bb_x_end\"] - self.data[\"bb_x_start\"]).max() + 150\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.label_enc.classes\n",
    "        \n",
    "    @staticmethod\n",
    "    def collate_fn(\n",
    "        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n",
    "        pad_val: int,\n",
    "        eos_tkn_idx: int,\n",
    "        dataset_returns_writer_id: bool = False,\n",
    "    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n",
    "        \n",
    "        imgs, targets = zip(*batch)\n",
    "\n",
    "        img_sizes = [im.shape for im in imgs]\n",
    "        if not len(set(img_sizes)) == 1:\n",
    "            hs, ws = zip(*img_sizes)\n",
    "            pad_fn = A.PadIfNeeded(\n",
    "                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "\n",
    "        seq_lengths = [t.shape[0] for t in targets]\n",
    "        targets_padded = np.full((len(targets), max(seq_lengths) + 1), pad_val)\n",
    "        for i, t in enumerate(targets):\n",
    "            targets_padded[i, : seq_lengths[i]] = t\n",
    "            targets_padded[i, seq_lengths[i]] = eos_tkn_idx\n",
    "\n",
    "        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n",
    "        \n",
    "        \n",
    "        return imgs, targets_padded\n",
    "    \n",
    "    def _get_transforms(self, split: str) -> A.Compose:\n",
    "        max_img_w = self.max_width\n",
    "    \n",
    "        max_img_h = self.max_height\n",
    "    \n",
    "        transforms = ImageTransforms(\n",
    "            (max_img_h, max_img_w), (RIMESDataset.MEAN, RIMESDataset.STD)\n",
    "        )\n",
    "    \n",
    "        if split == \"train\":\n",
    "            return transforms.train_trnsf\n",
    "        elif split == \"test\" or split == \"val\":\n",
    "            return transforms.test_trnsf\n",
    "    \n",
    "\n",
    "    def _get_form_data(self):\n",
    "        data = {\n",
    "            \"img_path\": [],\n",
    "            \"img_id\": [],\n",
    "            \"target\": [],\n",
    "            \"bb_y_start\": [],\n",
    "            \"bb_y_end\": [],\n",
    "            \"bb_x_start\": [],\n",
    "            \"bb_x_end\": [],\n",
    "            \"target_len\": [],\n",
    "        }\n",
    "        \n",
    "        \n",
    "        def process_forms(paths: Tuple[str, str, Path]):\n",
    "            return_data = {\n",
    "                \"img_path\": [],\n",
    "                \"img_id\": [],\n",
    "                \"target\": [],\n",
    "                \"bb_y_start\": [],\n",
    "                \"bb_y_end\": [],\n",
    "                \"bb_x_start\": [],\n",
    "                \"bb_x_end\": [],\n",
    "                \"target_len\": []\n",
    "            }\n",
    "            img_path, xml_path, root = paths\n",
    "            img_path = root / img_path\n",
    "            xml_path = root / xml_path\n",
    "            doc_id = img_path.stem[:-2]\n",
    "            xml_root = read_xml(xml_path)\n",
    "            \n",
    "            bb_y_start, bb_y_end, bb_x_start, bb_x_end = None, None, None, None\n",
    "            target = \"\"\n",
    "            num_corps = 0\n",
    "            for box in xml_root.iter(\"box\"):\n",
    "                type_tag =  box.find(\"type\")\n",
    "                if type_tag.text == \"Corps de texte\":\n",
    "                    target = box.find(\"text\").text\n",
    "                    if target is None or target == \"\":\n",
    "                        continue\n",
    "                    words = target.split(\"\\\\n\")\n",
    "                    if len(words) <= 5:\n",
    "                        continue\n",
    "                    bb_y_start = box.get(\"top_left_y\")\n",
    "                    bb_y_end   = box.get(\"bottom_right_y\")\n",
    "                    bb_x_start = box.get(\"top_left_x\")\n",
    "                    bb_x_end   = box.get(\"bottom_right_x\")\n",
    "                    \n",
    "                    return_data[\"img_path\"].append(str(img_path.resolve()))\n",
    "                    return_data[\"img_id\"].append(doc_id)\n",
    "                    return_data[\"target\"].append(self.process_target(target))\n",
    "                    return_data[\"bb_y_start\"].append(int(bb_y_start))\n",
    "                    return_data[\"bb_y_end\"].append(int(bb_y_end))\n",
    "                    return_data[\"bb_x_start\"].append(int(bb_x_start))\n",
    "                    return_data[\"bb_x_end\"].append(int(bb_x_end))\n",
    "                    return_data[\"target_len\"].append(len(target))\n",
    "                    num_corps += 1\n",
    "            \n",
    "            \n",
    "            # print(return_data[\"img_path\"])\n",
    "            \n",
    "            return return_data\n",
    "        \n",
    "        image_pairs = []\n",
    "        for form_dir in [\"DVD1_TIF\", \"DVD2_TIF\", \"DVD3_TIF\"]:\n",
    "            dr = self.root / form_dir\n",
    "            for file in dr.iterdir():\n",
    "                name = file.stem\n",
    "                ext = file.suffix\n",
    "                if ext == \".tif\" and name[-1] == \"L\":\n",
    "                    image_pairs.append((name + \".tif\", name + \".xml\", dr))\n",
    "        \n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(executor.map(process_forms, iter(image_pairs)))\n",
    "            \n",
    "            for single_results in results:\n",
    "                if single_results[\"img_path\"] == \"\":\n",
    "                    continue\n",
    "                data[\"img_path\"].extend(single_results[\"img_path\"])\n",
    "                data[\"img_id\"].extend(single_results[\"img_id\"])\n",
    "                data[\"target\"].extend(single_results[\"target\"])\n",
    "                data[\"bb_y_start\"].extend(single_results[\"bb_y_start\"])\n",
    "                data[\"bb_y_end\"].extend(single_results[\"bb_y_end\"])\n",
    "                data[\"bb_x_start\"].extend(single_results[\"bb_x_start\"])\n",
    "                data[\"bb_x_end\"].extend(single_results[\"bb_x_end\"])\n",
    "                data[\"target_len\"].extend(single_results[\"target_len\"])\n",
    "        \n",
    "        to_ret = pd.DataFrame(data)\n",
    "        self.max_height = (to_ret[\"bb_y_end\"] - to_ret[\"bb_y_start\"]).max() + 150\n",
    "        self.max_width = (to_ret[\"bb_x_end\"] - to_ret[\"bb_x_start\"]).max() + 150\n",
    "        \n",
    "        return to_ret\n",
    "    \n",
    "\n",
    "class AggregatedDataset(Dataset):\n",
    "    datasets: List[Dataset]\n",
    "    def __init__(self, rimes: RIMESDataset,\n",
    "                 iam: IAMDataset, \n",
    "                 split:str,\n",
    "                 only_lowercase: bool = False,\n",
    "                 label_enc: Optional[LabelParser] = None,):\n",
    "        super().__init__()\n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        \n",
    "        self._split = split\n",
    "        self.rimes = rimes\n",
    "        self.iam = iam\n",
    "        self._only_lowercase = only_lowercase\n",
    "        self.label_enc = label_enc\n",
    "        \n",
    "        if self.label_enc is None:\n",
    "            iamLabelEncoder = iam.label_enc\n",
    "            rimesLabelEncoder = rimes.label_enc\n",
    "            self.label_enc = LabelParser()\n",
    "            self.label_enc.addClasses(iamLabelEncoder.classes)\n",
    "            self.label_enc.addClasses(rimesLabelEncoder.classes)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.rimes) + len(self.iam)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.rimes):\n",
    "            img, target = self.rimes[idx]\n",
    "        if len(self.rimes) <= idx < len(self.iam):\n",
    "            img, target = self.iam[idx - len(self.rimes)]\n",
    "        \n",
    "        return img, target\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.341978Z",
     "start_time": "2024-04-17T18:00:09.291501Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class CharacterErrorRate(Metric):\n",
    "\n",
    "    def __init__(self, label_encoder: LabelParser):\n",
    "        super().__init__()\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        self.add_state(\"cer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"nr_samples\",default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.ndim == target.ndim\n",
    "        eos_tkn_idx, sos_tkn_idx = list(\n",
    "            self.label_encoder.encode_labels([\"<EOS>\", \"<SOS>\"])\n",
    "        )\n",
    "\n",
    "        if (preds[:, 0] == sos_tkn_idx).all():  # this should normally be the case\n",
    "            preds = preds[:, 1:]\n",
    "\n",
    "        eos_idxs_prd = (preds == eos_tkn_idx).float().argmax(1).tolist()\n",
    "        eos_idxs_tgt = (target == eos_tkn_idx).float().argmax(1).tolist()\n",
    "\n",
    "        for i, (p, t) in enumerate(zip(preds, target)):\n",
    "            eos_idx_p, eos_idx_t = eos_idxs_prd[i], eos_idxs_tgt[i]\n",
    "            p = p[:eos_idx_p] if eos_idx_p else p\n",
    "            t = t[:eos_idx_t] if eos_idx_t else t\n",
    "            p_str, t_str = map(tensor_to_str, (p, t))\n",
    "            editd = editdistance.eval(p_str, t_str)\n",
    "\n",
    "            self.cer_sum += editd/t.numel()\n",
    "            self.nr_samples +=1\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.cer_sum / self.nr_samples.float()\n",
    "\n",
    "class WordErrorRate(Metric):\n",
    "    def __init__(self, label_encoder: LabelParser):\n",
    "        super().__init__()\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        self.add_state(\"wer_sum\", default=torch.zeros(1, dtype=torch.float), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"nr_samples\", default=torch.zeros(1, dtype=torch.int64), dist_reduce_fx=\"sum\")\n",
    "\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.ndim == target.ndim\n",
    "\n",
    "        eos_tkn_idx, sos_tkn_idx = self.label_encoder.encode_labels([\"<EOS>\", \"<SOS>\"])\n",
    "\n",
    "        if (preds[:, 0] == sos_tkn_idx).all():\n",
    "            preds = preds[:, 1:]\n",
    "\n",
    "        eos_idxs_prd = (preds == eos_tkn_idx).float().argmax(1).tolist()\n",
    "        eos_idxs_tgt = (target == eos_tkn_idx).float().argmax(1).tolist()\n",
    "\n",
    "        for i, (p, t) in enumerate(zip(preds, target)):\n",
    "            eos_idx_p, eos_idx_t = eos_idxs_prd[i], eos_idxs_tgt[i]\n",
    "            p = (p[:eos_idx_p] if eos_idx_p else p).flatten().tolist()\n",
    "            t = (t[:eos_idx_t] if eos_idx_t else t).flatten().tolist()\n",
    "            if not t:\n",
    "                continue\n",
    "            \n",
    "            p_words = \"\".join(self.label_encoder.decode_labels(p)).split()\n",
    "            t_words = \"\".join(self.label_encoder.decode_labels(t)).split()\n",
    "            editd = editdistance.eval(p_words, t_words)\n",
    "            \n",
    "            \n",
    "            self.wer_sum += editd / len(t_words)\n",
    "            self.nr_samples += 1\n",
    "            \n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Compute Word Error Rate.\"\"\"\n",
    "        return self.wer_sum / self.nr_samples.float()\n",
    "\n",
    "def tensor_to_str(t: torch.Tensor) -> str:\n",
    "    return \"\".join(map(str, t.flatten().tolist()))"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.353845Z",
     "start_time": "2024-04-17T18:00:09.343701Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "class PosEmbedding1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements 1D sinusoidal embeddings.\n",
    "\n",
    "    Adapted from 'The Annotated Transformer', http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros((max_len, d_model), requires_grad=False)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add a 1D positional embedding to an input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): tensor of shape (B, T, d_model) to add positional\n",
    "                embedding to\n",
    "        \"\"\"\n",
    "        _, T, _ = x.shape\n",
    "        # assert T <= self.pe.size(0) \\\n",
    "        assert T <= self.pe.size(1), (\n",
    "            f\"Stored 1D positional embedding does not have enough dimensions for the current feature map. \"\n",
    "            f\"Currently max_len={self.pe.size(1)}, T={T}. Consider increasing max_len such that max_len >= T.\"\n",
    "        )\n",
    "        return x + self.pe[:, :T]\n",
    "\n",
    "\n",
    "\n",
    "class PosEmbedding2D(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe_x = torch.zeros((max_len, d_model // 2), requires_grad=False)\n",
    "        pe_y = torch.zeros((max_len, d_model // 2), requires_grad=False)\n",
    "\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            -math.log(10000.0) * torch.arange(0, d_model // 2, 2) / d_model\n",
    "        )\n",
    "\n",
    "        pe_y[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe_y[:, 1::2] = torch.cos(pos * div_term)\n",
    "        pe_x[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe_x[:, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "        self.register_buffer(\"pe_x\", pe_x)\n",
    "        self.register_buffer(\"pe_y\", pe_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, w, h, _ = x.shape\n",
    "\n",
    "        pe_x_ = self.pe_x[:w, :].unsqueeze(1).expand(-1, h, -1)\n",
    "        pe_y_ = self.pe_y[:h, :].unsqueeze(0).expand(w, -1, -1)\n",
    "\n",
    "        pe = torch.cat([pe_y_, pe_x_], -1)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        return x + pe\n",
    "\n",
    "\n",
    "class encoderHTR(nn.Module):\n",
    "    def __init__(self, d_model: int, encoder_type: str, dropout=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        assert encoder_type in [\"resnet18\", \"resnet34\", \"resnet50\"], \"Model not found\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pos_embd = PosEmbedding2D(d_model)\n",
    "\n",
    "        resnet = getattr(models, encoder_type)(pretrained=False)\n",
    "\n",
    "        modules = list(resnet.children())\n",
    "        cnv_1 = modules[0]\n",
    "        cnv_1 = nn.Conv2d(\n",
    "            1,\n",
    "            cnv_1.out_channels,\n",
    "            cnv_1.kernel_size,\n",
    "            cnv_1.stride,\n",
    "            cnv_1.padding,\n",
    "            bias=cnv_1.bias\n",
    "        )\n",
    "        self.encoder = nn.Sequential(cnv_1, *modules[1:-2])\n",
    "        self.linear = nn.Conv2d(resnet.fc.in_features, d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        x = self.encoder(imgs.unsqueeze(1))\n",
    "        x = self.linear(x).transpose(1, 2).transpose(2, 3)\n",
    "        x = self.pos_embd(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.flatten(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class decoderHTR(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_length,\n",
    "                 max_seq_len,\n",
    "                 eos_tkn_idx,\n",
    "                 sos_tkn_idx,\n",
    "                 pad_tkn_idx,\n",
    "                 d_model,\n",
    "                 num_layers,\n",
    "                 nhead,\n",
    "                 dim_ffn,\n",
    "                 dropout,\n",
    "                 activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.vocab_length = vocab_length\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.eos_idx = eos_tkn_idx\n",
    "        self.sos_idx = sos_tkn_idx\n",
    "        self.pad_idx = pad_tkn_idx\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.nhead = nhead\n",
    "        self.dim_ffn = dim_ffn\n",
    "        self.pos_emb = PosEmbedding1D(d_model)\n",
    "        self.emb = nn.Embedding(vocab_length, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model,\n",
    "            nhead,\n",
    "            dim_ffn,\n",
    "            dropout,\n",
    "            activation,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.clf = nn.Linear(d_model, vocab_length)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, memory: torch.Tensor):\n",
    "        B, _, _ = memory.shape\n",
    "        all_logits = []\n",
    "        sampled_ids = [torch.full([B], self.sos_idx).to(memory.device)]\n",
    "        tgt = self.pos_emb(\n",
    "            self.emb(sampled_ids[0]).unsqueeze(1) * math.sqrt(self.d_model)\n",
    "        )\n",
    "        tgt = self.drop(tgt)\n",
    "        eos_sampled = torch.zeros(B).bool()\n",
    "\n",
    "        for t in range(self.max_seq_len):\n",
    "            tgt_mask = self.subsequent_mask(len(sampled_ids)).to(memory.device)\n",
    "            out = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "            logits = self.clf(out[:, -1, :])\n",
    "            _, pred = torch.max(logits, -1)\n",
    "            all_logits.append(logits)\n",
    "            sampled_ids.append(pred)\n",
    "            for i, pr in enumerate(pred):\n",
    "                if pr == self.eos_idx:\n",
    "                    eos_sampled[i] = True\n",
    "            if eos_sampled.all():\n",
    "                break\n",
    "\n",
    "            tgt_ext = self.drop(\n",
    "                self.pos_emb.pe[:, len(sampled_ids)]\n",
    "                + self.emb(pred) * math.sqrt(self.d_model)\n",
    "            ).unsqueeze(1)\n",
    "            tgt = torch.cat([tgt, tgt_ext], 1)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)\n",
    "        all_logits = torch.stack(all_logits, 1)\n",
    "\n",
    "        eos_idxs = (sampled_ids == self.eos_idx).float().argmax(1)\n",
    "        for i in range(B):\n",
    "            if eos_idxs[i] != 0:\n",
    "                sampled_ids[i, eos_idxs[i] + 1:] = self.pad_idx\n",
    "\n",
    "        return all_logits, sampled_ids\n",
    "\n",
    "    def forward_teacher_forcing(self, memory: torch.Tensor, tgt: torch.Tensor):\n",
    "        B, T = tgt.shape\n",
    "        tgt = torch.cat(\n",
    "            [\n",
    "                torch.full([B], self.sos_idx).unsqueeze(1).to(memory.device),\n",
    "                tgt[:, :-1]\n",
    "            ],\n",
    "            1\n",
    "        )\n",
    "\n",
    "        tgt_key_masking = tgt == self.pad_idx\n",
    "        tgt_mask = self.subsequent_mask(T).to(tgt.device)\n",
    "\n",
    "        tgt = self.pos_emb(self.emb(tgt) * math.sqrt(self.d_model))\n",
    "        tgt = self.drop(tgt)\n",
    "        out = self.decoder(\n",
    "            tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_masking\n",
    "        )\n",
    "        logits = self.clf(out)\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def subsequent_mask(size: int):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        return mask == 1\n",
    "\n",
    "\n",
    "class FullPageHTR(nn.Module):\n",
    "    encoder: encoderHTR\n",
    "    decoder: decoderHTR\n",
    "    cer_metric: CharacterErrorRate\n",
    "    wer_metric: WordErrorRate\n",
    "    loss_fn: Callable\n",
    "    label_encoder: LabelParser\n",
    "\n",
    "    def __init__(self, label_encoder: LabelParser,\n",
    "                 max_seq_len=500,\n",
    "                 d_model=1024,\n",
    "                 num_layers=6,\n",
    "                 nhead=4,\n",
    "                 dim_feedforward=1024,\n",
    "                 encoder_name=\"resnet18\",\n",
    "                 drop_enc=0.1,\n",
    "                 drop_dec=0.1,\n",
    "                 activ_dec=\"gelu\",\n",
    "                 loss_type=\"cross_entropy\",\n",
    "                 label_smoothing=0.0,\n",
    "                 vocab_len: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.eos_token_idx, self.sos_token_idx, self.pad_token_idx = label_encoder.encode_labels(\n",
    "            [\"<EOS>\", \"<SOS>\", \"<PAD>\"]\n",
    "        )\n",
    "\n",
    "        self.encoder = encoderHTR(d_model, encoder_type=encoder_name, dropout=drop_enc)\n",
    "        self.decoder = decoderHTR(vocab_length=(vocab_len or len(label_encoder.classes)),\n",
    "                                  max_seq_len=max_seq_len,\n",
    "                                  eos_tkn_idx=self.eos_token_idx,\n",
    "                                  sos_tkn_idx=self.sos_token_idx,\n",
    "                                  pad_tkn_idx=self.pad_token_idx,\n",
    "                                  d_model=d_model,\n",
    "                                  num_layers=num_layers,\n",
    "                                  nhead=nhead,\n",
    "                                  dim_ffn=dim_feedforward,\n",
    "                                  dropout=drop_dec,\n",
    "                                  activation=activ_dec)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.cer_metric = CharacterErrorRate(label_encoder)\n",
    "        self.wer_metric = WordErrorRate(label_encoder)\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "        \n",
    "        assert loss_type in [\"cross_entropy\", \"ctc_loss\"]\n",
    "        self.loss_type = loss_type\n",
    "        if loss_type == \"cross_entropy\":\n",
    "            self.loss_fn = nn.CrossEntropyLoss(\n",
    "                ignore_index=self.pad_token_idx,\n",
    "                label_smoothing=label_smoothing\n",
    "            )\n",
    "        elif loss_type == \"ctc_loss\":\n",
    "            self.loss_fn = nn.CTCLoss(\n",
    "                blank=self.pad_token_idx\n",
    "            )\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor, targets: Optional[torch.Tensor] = None):\n",
    "        logits, sampled_ids = self.decoder(self.encoder(imgs))\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            if self.loss_type == \"cross_entropy\":\n",
    "                loss = self.loss_fn(\n",
    "                    logits[:, : targets.size(1), :].transpose(1, 2),\n",
    "                    targets[:, : logits.size(1)],\n",
    "                )\n",
    "            elif self.loss_type == \"ctc_loss\":\n",
    "                logits = self.log_softmax(logits)\n",
    "                _, preds = logits[:,: targets.size(1)].max(-1)\n",
    "                lengths = preds == self.eos_token_idx\n",
    "                _, target_lengths = (targets == self.eos_token_idx).max(-1)\n",
    "\n",
    "                # Calculate predicted lengths\n",
    "                pred_length = []\n",
    "                for batch in lengths:\n",
    "                    val, eos_pos = batch.max(-1)\n",
    "                    if val == 0:\n",
    "                        pred_length.append(batch.size(-1))\n",
    "                    else:\n",
    "                        pred_length.append(eos_pos.item())\n",
    "\n",
    "                # Optimize memory usage by avoiding temporary list\n",
    "                pred_length_tensor = torch.tensor(pred_length, dtype=torch.long)\n",
    "\n",
    "                logits = logits[:,: targets.size(1)].permute(1, 0, 2)\n",
    "                loss = self.loss_fn(logits, targets, tuple(pred_length), target_lengths.tolist())\n",
    "            \n",
    "        return logits, sampled_ids, loss\n",
    "\n",
    "    def forward_teacher_forcing(self, imgs: torch.Tensor, targets: torch.Tensor):\n",
    "        memory = self.encoder(imgs)\n",
    "        logits = self.decoder.forward_teacher_forcing(memory, targets)\n",
    "        if self.loss_type == \"cross_entropy\":\n",
    "            loss = self.loss_fn(logits.transpose(1, 2), targets)\n",
    "        elif self.loss_type == \"ctc_loss\":\n",
    "            logits = self.log_softmax(logits)\n",
    "            _, preds = logits.max(-1)\n",
    "            lengths = preds == self.eos_token_idx\n",
    "            _, target_lengths = (targets == self.eos_token_idx).max(-1)\n",
    "\n",
    "            # Calculate predicted lengths\n",
    "            pred_length = []\n",
    "            for batch in lengths:\n",
    "                val, eos_pos = batch.max(-1)\n",
    "                if val == 0:\n",
    "                    pred_length.append(batch.size(-1))\n",
    "                else:\n",
    "                    pred_length.append(eos_pos.item())\n",
    "            \n",
    "            logits = logits.permute(1, 0, 2)\n",
    "            loss = self.loss_fn(logits, targets, tuple(pred_length), tuple(target_lengths.tolist()) )\n",
    "            print(loss.item())\n",
    "        return logits, loss\n",
    "\n",
    "    def calculate_metrics(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        self.cer_metric.reset()\n",
    "        self.wer_metric.reset()\n",
    "\n",
    "        cer = self.cer_metric(preds, targets)\n",
    "        wer = self.wer_metric(preds, targets)\n",
    "        return {\"CER\": cer, \"WER\": wer}\n",
    "\n",
    "    def set_num_output_classes(self, n_classes: int):\n",
    "        old_vocab_len = self.decoder.vocab_length\n",
    "        self.decoder.vocab_length = n_classes\n",
    "        self.decoder.clf = nn.Linear(self.decoder.d_model, n_classes)\n",
    "\n",
    "        new_embs = nn.Embedding(n_classes, self.decoder.d_model)\n",
    "        with torch.no_grad():\n",
    "            new_embs.weight[:old_vocab_len] = self.decoder.emb.weight\n",
    "            self.decoder.emb = new_embs\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.379441Z",
     "start_time": "2024-04-17T18:00:09.354931Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Trainer Class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from copy import copy\n",
    "from pytorch_lightning import seed_everything\n",
    "!ls /kaggle/input/iam-dataset/raw/\n",
    "seed_everything(12345)\n",
    "ds = IAMDataset(root=\"../input/iam-dataset/raw\", label_enc=None, parse_method=\"form\" ,split=\"train\")\n",
    "ds_train, ds_val = torch.utils.data.random_split(ds, [math.ceil(0.8 * len(ds)), math.floor(0.2 * len(ds))])\n",
    "\n",
    "ds_val.data = copy(ds)\n",
    "ds_val.data.set_transforms_for_split(\"val\")\n",
    "train_len = len(ds_train)\n",
    "val_len = len(ds_val)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:10.966306Z",
     "start_time": "2024-04-17T18:00:09.397368Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "batch_size = 2\n",
    "pad_tkn_idx, eos_tkn_idx = ds.label_enc.encode_labels([\"<PAD>\", \"<EOS>\"])\n",
    "collate_fn = partial(\n",
    "        IAMDataset.collate_fn, pad_val=pad_tkn_idx, eos_tkn_idx=eos_tkn_idx\n",
    ")\n",
    "num_workers = 4\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=2 * batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_len //= batch_size\n",
    "val_len //= 2 * batch_size"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:10.971166Z",
     "start_time": "2024-04-17T18:00:10.967554Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    " import gc\n",
    "\n",
    " try:\n",
    "   device = \"cuda\"\n",
    "   torch.cuda.empty_cache()\n",
    "   gc.collect()\n",
    "\n",
    "   model = FullPageHTR(ds.label_enc).to(device)\n",
    "   optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "   trainer = ModelTrainer(\"Testing_run\", model, ds_name=\"IAM_forms\" , train_data=dl_train, val_data=dl_val, optimizer=optimizer, num_epochs=100, device=device, normalization_steps=56)\n",
    "\n",
    "   wandb.finish()\n",
    "   trainer.train(train_len, val_len, wanda=True)\n",
    " except RuntimeError:\n",
    "   del model\n",
    "   print(\"Error time!!\")"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:35.444932Z",
     "start_time": "2024-04-17T18:00:34.988633Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-28T11:31:16.198702Z",
     "iopub.status.idle": "2024-04-28T11:31:16.199348Z",
     "shell.execute_reply.started": "2024-04-28T11:31:16.199103Z",
     "shell.execute_reply": "2024-04-28T11:31:16.199124Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "synth_ds = IAMDatasetSynthetic(iam_dataset=ds, synth_prob=0.3)\n",
    "print(\"Initialized synthetic dataset\")\n",
    "ds_synth_train, ds_synth_val = torch.utils.data.random_split(synth_ds, [math.ceil(0.8 * len(synth_ds)), math.floor(0.2 * len(synth_ds))])\n",
    "\n",
    "ds_synth_val.data = copy(synth_ds)\n",
    "ds_synth_val.data.iam_dataset.set_transforms_for_split(\"val\")\n",
    "train_len = len(ds_synth_train)\n",
    "val_len = len(ds_synth_val)\n",
    "print(train_len, val_len)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 56\n",
    "pad_tkn_idx, eos_tkn_idx = ds.label_enc.encode_labels([\"<PAD>\", \"<EOS>\"])\n",
    "collate_fn = partial(\n",
    "        IAMDataset.collate_fn, pad_val=pad_tkn_idx, eos_tkn_idx=eos_tkn_idx\n",
    ")\n",
    "num_workers = 4\n",
    "dl_synth_train = DataLoader(\n",
    "    ds_synth_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dl_synth_val = DataLoader(\n",
    "    ds_synth_val,\n",
    "    batch_size=2 * batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_len //= batch_size\n",
    "val_len //= 2 * batch_size"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "import gc\n",
    "\n",
    "try:\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    model = FullPageHTR(ds.label_enc, loss_type=\"cross_entropy\", encoder_name=\"resnet34\").to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "    trainer = ModelTrainer(\"Synth_0.3_ctc_loss\", model, ds_name=\"IAM_forms_synthetic\" , train_data=dl_train_synth, val_data=dl_val_synth, optimizer=optimizer, num_epochs=100, device=device, normalization_steps=56)\n",
    "\n",
    "    wandb.finish()\n",
    "    trainer.train(train_len, val_len, wanda=True)\n",
    "finally:\n",
    "    del model\n",
    " try:\n",
    "  \n",
    " except RuntimeError:\n",
    "   del model\n",
    "   print(\"Error time!!\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-28T11:35:30.046380Z",
     "iopub.execute_input": "2024-04-28T11:35:30.046811Z",
     "iopub.status.idle": "2024-04-28T11:35:31.975548Z",
     "shell.execute_reply.started": "2024-04-28T11:35:30.046783Z",
     "shell.execute_reply": "2024-04-28T11:35:31.974113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitFullPageHTREncoderDecoder(pl.LightningModule):\n",
    "    model: FullPageHTR\n",
    "\n",
    "    \"\"\"\n",
    "    Pytorch Lightning module that acting as a wrapper around the\n",
    "    FullPageHTREncoderDecoder class.\n",
    "\n",
    "    Using a PL module allows the model to be used in conjunction with a Pytorch\n",
    "    Lightning Trainer, and takes care of logging relevant metrics to Tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_encoder: LabelParser,\n",
    "        learning_rate: float = 0.0002,\n",
    "        label_smoothing: float = 0.0,\n",
    "        max_seq_len: int = 500,\n",
    "        d_model: int = 260,\n",
    "        num_layers: int = 6,\n",
    "        nhead: int = 4,\n",
    "        dim_feedforward: int = 1024,\n",
    "        encoder_name: str = \"resnet18\",\n",
    "        drop_enc: int = 0.1,\n",
    "        drop_dec: int = 0.1,\n",
    "        activ_dec: str = \"gelu\",\n",
    "        loss_function: str = \"cross_entropy\",\n",
    "        vocab_len: Optional[int] = None,  # if not specified len(label_encoder) is used\n",
    "        params_to_log: Optional[Dict[str, Union[str, float, int]]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Save hyperparameters.\n",
    "        self.learning_rate = learning_rate\n",
    "        if params_to_log is not None:\n",
    "            self.save_hyperparameters(params_to_log)\n",
    "        self.save_hyperparameters(\n",
    "            \"learning_rate\",\n",
    "            \"d_model\",\n",
    "            \"num_layers\",\n",
    "            \"nhead\",\n",
    "            \"dim_feedforward\",\n",
    "            \"max_seq_len\",\n",
    "            \"encoder_name\",\n",
    "            \"drop_enc\",\n",
    "            \"drop_dec\",\n",
    "            \"activ_dec\",\n",
    "        )\n",
    "\n",
    "        # Initialize the model.\n",
    "        self.model = FullPageHTR(\n",
    "            label_encoder=label_encoder,\n",
    "            max_seq_len=max_seq_len,\n",
    "            d_model=d_model,\n",
    "            num_layers=num_layers,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            encoder_name=encoder_name,\n",
    "            drop_enc=drop_enc,\n",
    "            drop_dec=drop_dec,\n",
    "            activ_dec=activ_dec,\n",
    "            vocab_len=vocab_len,\n",
    "            label_smoothing=label_smoothing,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def encoder(self):\n",
    "        return self.model.encoder\n",
    "\n",
    "    @property\n",
    "    def decoder(self):\n",
    "        return self.model.decoder\n",
    "\n",
    "    def forward(self, imgs: Tensor, targets: Optional[Tensor] = None):\n",
    "        return self.model(imgs, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, targets = batch\n",
    "        logits, loss = self.model.forward_teacher_forcing(imgs, targets)\n",
    "        self.log(\"train_loss\", loss, sync_dist=True, prog_bar=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.val_or_test_step(batch)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.val_or_test_step(batch)\n",
    "\n",
    "    def val_or_test_step(self, batch) -> Tensor:\n",
    "        imgs, targets = batch\n",
    "        logits, _, loss = self(imgs, targets)\n",
    "        _, preds = logits.max(-1)\n",
    "\n",
    "        # Update and log metrics.\n",
    "        self.model.cer_metric(preds, targets)\n",
    "        self.model.wer_metric(preds, targets)\n",
    "        self.log(\"char_error_rate\", self.model.cer_metric, on_step=True ,prog_bar=True)\n",
    "        self.log(\"word_error_rate\", self.model.wer_metric, on_step=True ,prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, sync_dist=True, prog_bar=True, on_step=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pytorch_lightning.callbacks import Callback, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PREDICTIONS_TO_LOG = {\n",
    "    \"word\": 10,\n",
    "    \"line\": 6,\n",
    "    \"form\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "class LogWorstPredictions(Callback):\n",
    "    \"\"\"\n",
    "    At the end of training, log the worst image prediction, meaning the predictions\n",
    "    with the highest character error rates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_dataloader: Optional[DataLoader] = None,\n",
    "        val_dataloader: Optional[DataLoader] = None,\n",
    "        test_dataloader: Optional[DataLoader] = None,\n",
    "        training_skipped: bool = False,\n",
    "        data_format: str = \"word\",\n",
    "    ):\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.training_skipped = training_skipped\n",
    "        self.data_format = data_format\n",
    "\n",
    "    def on_validation_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if self.training_skipped and self.val_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.val_dataloader, trainer, pl_module, mode=\"val\"\n",
    "            )\n",
    "\n",
    "    def on_test_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if self.test_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.test_dataloader, trainer, pl_module, mode=\"test\"\n",
    "            )\n",
    "\n",
    "    def on_fit_end(self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        if self.train_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.train_dataloader, trainer, pl_module, mode=\"train\"\n",
    "            )\n",
    "        if self.val_dataloader is not None:\n",
    "            self.log_worst_predictions(\n",
    "                self.val_dataloader, trainer, pl_module, mode=\"val\"\n",
    "            )\n",
    "\n",
    "    def log_worst_predictions(\n",
    "        self,\n",
    "        dataloader: DataLoader,\n",
    "        trainer: \"pl.Trainer\",\n",
    "        pl_module: \"pl.LightningModule\",\n",
    "        mode: str = \"train\",\n",
    "    ):\n",
    "        img_cers = []\n",
    "        device = \"cuda:0\" if pl_module.on_gpu else \"cpu\"\n",
    "        if not self.training_skipped:\n",
    "            self._load_best_model(trainer, pl_module)\n",
    "            pl_module = trainer.model\n",
    "\n",
    "        print(f\"Running {mode} inference on best model...\")\n",
    "\n",
    "        # Run inference on the validation set.\n",
    "        pl_module.eval()\n",
    "        for img, target in dataloader:\n",
    "            assert target.ndim == 2, target.ndim\n",
    "            cer_metric = pl_module.model.cer_metric\n",
    "            with torch.inference_mode():\n",
    "                logits, preds, _ = pl_module(img.to(device), target.to(device))\n",
    "                for prd, tgt, im in zip(preds, target, img):\n",
    "                    cer_metric.reset()\n",
    "                    cer = cer_metric(prd.unsqueeze(0), tgt.unsqueeze(0)).item()\n",
    "                    img_cers.append((im, cer, prd, tgt))\n",
    "\n",
    "        # Log the worst k predictions.\n",
    "        to_log = PREDICTIONS_TO_LOG[self.data_format] * 2\n",
    "        img_cers.sort(key=lambda x: x[1], reverse=True)  # sort by CER\n",
    "        img_cers = img_cers[:to_log]\n",
    "        fig = plt.figure(figsize=(24, 16))\n",
    "        for i, (im, cer, prd, tgt) in enumerate(img_cers):\n",
    "            pred_str, target_str = decode_prediction_and_target(\n",
    "                prd, tgt, pl_module.model.label_encoder, pl_module.decoder.eos_tkn_idx\n",
    "            )\n",
    "\n",
    "            # Create plot.\n",
    "            ncols = 4 if self.data_format == \"word\" else 2\n",
    "            nrows = math.ceil(to_log / ncols)\n",
    "            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n",
    "            matplotlib_imshow(im, IAMDataset.MEAN, IAMDataset.STD)\n",
    "            ax.set_title(f\"Pred: {pred_str} (CER: {cer:.2f})\\nTarget: {target_str}\")\n",
    "\n",
    "        # # Log the results to Tensorboard.\n",
    "        # tensorboard = trainer.logger.experiment\n",
    "        # tensorboard.add_figure(f\"{mode}: worst predictions\", fig, trainer.global_step)\n",
    "        trainer.logger.experiment.log({f\"{mode}: predictions vs targets\": wandb.Image(fig)})\n",
    "\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(\"Done.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_best_model(trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"):\n",
    "        ckpt_callback = None\n",
    "        for cb in trainer.callbacks:\n",
    "            if isinstance(cb, ModelCheckpoint):\n",
    "                ckpt_callback = cb\n",
    "                break\n",
    "        assert ckpt_callback is not None, \"ModelCheckpoint not found in callbacks.\"\n",
    "        best_model_path = ckpt_callback.best_model_path\n",
    "\n",
    "        print(f\"Loading best model at {best_model_path}\")\n",
    "        label_encoder = pl_module.model.label_encoder\n",
    "        model = LitFullPageHTREncoderDecoder.load_from_checkpoint(\n",
    "            best_model_path,\n",
    "            label_encoder=label_encoder,\n",
    "        )\n",
    "        trainer.model.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "class LogModelPredictions(Callback):\n",
    "    \"\"\"\n",
    "    Use a fixed test batch to monitor model predictions at the end of every epoch.\n",
    "\n",
    "    Specifically: it generates matplotlib Figure using a trained network, along with images\n",
    "    and labels from a batch, that shows the network's prediction alongside the actual target.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        label_encoder: LabelParser,\n",
    "        val_batch: Tuple[torch.Tensor, torch.Tensor],\n",
    "        use_gpu: bool = True,\n",
    "        data_format: str = \"word\",\n",
    "        train_batch: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n",
    "    ):\n",
    "        self.label_encoder = label_encoder\n",
    "        self.val_batch = val_batch\n",
    "        self.use_gpu = use_gpu\n",
    "        self.data_format = data_format\n",
    "        self.train_batch = train_batch\n",
    "\n",
    "    def on_validation_epoch_end(\n",
    "        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n",
    "    ):\n",
    "        self._predict_intermediate(trainer, pl_module, split=\"val\")\n",
    "\n",
    "    def on_train_epoch_end(\n",
    "        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\"\n",
    "    ):\n",
    "        if self.train_batch is not None:\n",
    "            self._predict_intermediate(trainer, pl_module, split=\"train\")\n",
    "\n",
    "    def _predict_intermediate(\n",
    "        self, trainer: \"pl.Trainer\", pl_module: \"pl.LightningModule\", split=\"val\"\n",
    "    ):\n",
    "        \"\"\"Make predictions on a fixed batch of data and log the results to Tensorboard.\"\"\"\n",
    "\n",
    "        # Make predictions.\n",
    "        if split == \"train\":\n",
    "            imgs, targets = self.train_batch\n",
    "        else:  # split == \"val\"\n",
    "            imgs, targets = self.val_batch\n",
    "        with torch.inference_mode():\n",
    "            pl_module.eval()\n",
    "            _, preds, _ = pl_module(imgs.cuda() if self.use_gpu else imgs)\n",
    "\n",
    "        # Decode predictions and generate a plot.\n",
    "        fig = plt.figure(figsize=(12, 16))\n",
    "        for i, (p, t) in enumerate(zip(preds, targets)):\n",
    "            pred_str, target_str = decode_prediction_and_target(\n",
    "                p, t, self.label_encoder, pl_module.decoder.eos_idx\n",
    "            )\n",
    "\n",
    "            # Create plot.\n",
    "            ncols = 2 if self.data_format == \"word\" else 1\n",
    "            nrows = math.ceil(preds.size(0) / ncols)\n",
    "            ax = fig.add_subplot(nrows, ncols, i + 1, xticks=[], yticks=[])\n",
    "            matplotlib_imshow(imgs[i], IAMDataset.MEAN, IAMDataset.STD)\n",
    "            ax.set_title(f\"Pred: {pred_str}\\nTarget: {target_str}\")\n",
    "\n",
    "        # Log the results to Tensorboard.\n",
    "#         tensorboard = trainer.logger.experiment\n",
    "#         tensorboard.add_figure(\n",
    "#             f\"{split}: predictions vs targets\", fig, trainer.global_step\n",
    "#         )\n",
    "        trainer.logger.experiment.log({f\"{split}: predictions vs targets\": wandb.Image(fig)})\n",
    "\n",
    "        plt.close(fig)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install wandb --upgrade\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, ModelSummary\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "wandb.login(key=\"0350b0cc5bd9521bb37a798168d31b6b65e9caca\")\n",
    "wandb_logger = WandbLogger(project=\"bach_thesis\", log_model=\"all\")\n",
    "callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            save_top_k=(3),\n",
    "            mode=\"min\",\n",
    "            monitor=\"word_error_rate\",\n",
    "            filename=\"{epoch}-{char_error_rate:.4f}-{word_error_rate:.4f}\",\n",
    "        ),\n",
    "        ModelSummary(max_depth=2),\n",
    "        LitProgressBar(),\n",
    "        LogWorstPredictions(\n",
    "            dl_synth_train,\n",
    "            dl_synth_val,\n",
    "            training_skipped=False,\n",
    "            data_format=\"form\",\n",
    "        ),\n",
    "        LogModelPredictions(\n",
    "            ds.label_enc,\n",
    "            val_batch=next(\n",
    "                iter(\n",
    "                    DataLoader(\n",
    "                        Subset(\n",
    "                            ds_val,\n",
    "                            random.sample(\n",
    "                                range(len(ds_val)), 1\n",
    "                            ),\n",
    "                        ),\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_fn,\n",
    "                        num_workers=2,\n",
    "                        pin_memory=True,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            train_batch=next(\n",
    "                iter(\n",
    "                    DataLoader(\n",
    "                        Subset(\n",
    "                            ds_train,\n",
    "                            random.sample(\n",
    "                                range(len(ds_train)),\n",
    "                                1,\n",
    "                            ),\n",
    "                        ),\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate_fn,\n",
    "                        num_workers=2,\n",
    "                        pin_memory=True,\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            data_format=\"form\",\n",
    "            use_gpu=True,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "                    monitor=\"word_error_rate\",\n",
    "                    patience=50,\n",
    "                    verbose=True,\n",
    "                    mode=\"min\",\n",
    "                    check_on_train_epoch_end=False,\n",
    "                )\n",
    "    ]\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=3000,\n",
    "    accelerator=\"tpu\", \n",
    "    devices=4,\n",
    "    callbacks = callbacks,\n",
    "    logger=wandb_logger,\n",
    "    fast_dev_run=False)\n",
    "model = LitFullPageHTREncoderDecoder(ds.label_enc,\n",
    "        learning_rate = 0.0001,\n",
    "        label_smoothing = 0.0,\n",
    "        max_seq_len = 500,\n",
    "        d_model = 260,\n",
    "        num_layers = 6,\n",
    "        nhead = 4,\n",
    "        dim_feedforward = 1024,\n",
    "        encoder_name = \"resnet34\",\n",
    "        drop_enc = 0.1,\n",
    "        drop_dec = 0.1,\n",
    "        activ_dec = \"gelu\",\n",
    "        loss_function = \"cross_entropy\",\n",
    "        vocab_len = None, \n",
    "    )\n",
    "trainer.fit(model, dl_synth_train, dl_synth_val)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Utils "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6871496eafc0097"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "from random import random\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import cv2 as cv\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from random import randint\n",
    "import html\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, Dict, Sequence, Optional, List, Any, Callable, Optional\n",
    "import pandas as pd\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchmetrics import Metric\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset\n",
    "import editdistance\n",
    "import wandb\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.optim import Optimizer\n",
    "import tqdm\n",
    "\n",
    "def pickle_load(file) -> Any:\n",
    "    with open(file, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def pickle_save(obj, file):\n",
    "    with open(file, \"wb\") as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def read_xml(file: Union[Path, str]) -> ET.Element:\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    return root\n",
    "\n",
    "def find_child_by_tag(element: ET.Element, tag: str, value: str) -> Union[ET.Element, None]:\n",
    "    for child in element:\n",
    "        if child.get(tag) == value:\n",
    "            return child\n",
    "    return None\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def dpi_adjusting(img: np.ndarray, scale: float, **kwargs) -> np.ndarray:\n",
    "    height, width = img.shape[:2]\n",
    "    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n",
    "    return cv.resize(img, (new_width, new_height))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.235185Z",
     "start_time": "2024-04-17T18:00:09.229244Z"
    }
   },
   "id": "99c3987e96451b69",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LabelParser:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.vocab_size = None\n",
    "        self.class_to_idx = None\n",
    "        self.idx_to_class = None\n",
    "\n",
    "    def fit(self, classes: Sequence[str]):\n",
    "        self.classes = list(classes)\n",
    "        self.vocab_size = len(classes)\n",
    "        self.idx_to_class = dict(enumerate(classes))\n",
    "        self.class_to_idx = {cls: i for i, cls in self.idx_to_class.items()}\n",
    "\n",
    "        return self\n",
    "\n",
    "    def addClasses(self, classes: List[str]):\n",
    "        all_classes = sorted(set(self.classes + classes))\n",
    "\n",
    "        self.fit(all_classes)\n",
    "\n",
    "    def encode_labels(self, sequence: Sequence[str]):\n",
    "        self._check_fitted()\n",
    "        return [self.class_to_idx[c] for c in sequence]\n",
    "\n",
    "    def decode_labels(self, sequence: Sequence[int]):\n",
    "        self._check_fitted()\n",
    "        return [self.idx_to_class[c] for c in sequence]\n",
    "\n",
    "    def _check_fitted(self):\n",
    "        if self.classes is None:\n",
    "            raise ValueError(\"LabelParser class was not fitted yet\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.266332Z",
     "start_time": "2024-04-17T18:00:09.262338Z"
    }
   },
   "id": "84fcc3e65a6865e",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Image transformations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2a49033f8460546"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SafeRandomScale(A.RandomScale):\n",
    "    def apply(self, img, scale=0, interpolation=cv.INTER_LINEAR, **params):\n",
    "        height, width = img.shape[:2]\n",
    "        new_height, new_width = int(height * scale), int(width * scale)\n",
    "        if new_height <= 0 or new_width <= 0:\n",
    "            return img\n",
    "        return super().apply(img, scale, interpolation, **params)\n",
    "\n",
    "def adjust_dpi(img: np.ndarray, scale: float, **kwargs):\n",
    "    height, width = img.shape\n",
    "    new_height, new_width = math.ceil(height * scale), math.ceil(width * scale)\n",
    "    return cv.resize(img, (new_width, new_height))\n",
    "\n",
    "def randomly_displace_and_pad(\n",
    "    img: np.ndarray,\n",
    "    padded_size: Tuple[int, int],\n",
    "    crop_if_necessary: bool = False,\n",
    "    **kwargs,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly displace an image within a frame, and pad zeros around the image.\n",
    "\n",
    "    Args:\n",
    "        img (np.ndarray): image to process\n",
    "        padded_size (Tuple[int, int]): (height, width) tuple indicating the size of the frame\n",
    "        crop_if_necessary (bool): whether to crop the image if its size exceeds that\n",
    "            of the frame\n",
    "    \"\"\"\n",
    "    frame_h, frame_w = padded_size\n",
    "    img_h, img_w = img.shape\n",
    "    if frame_h < img_h or frame_w < img_w:\n",
    "        if crop_if_necessary:\n",
    "            print(\n",
    "                \"WARNING (`randomly_displace_and_pad`): cropping input image before \"\n",
    "                \"padding because it exceeds the size of the frame.\"\n",
    "            )\n",
    "            img_h, img_w = min(img_h, frame_h), min(img_w, frame_w)\n",
    "            img = img[:img_h, :img_w]\n",
    "        else:\n",
    "            raise AssertionError(\n",
    "                f\"Frame is smaller than the image: ({frame_h}, {frame_w}) vs. ({img_h},\"\n",
    "                f\" {img_w})\"\n",
    "            )\n",
    "\n",
    "    res = np.zeros((frame_h, frame_w), dtype=img.dtype)\n",
    "\n",
    "    pad_top =  randint(0, frame_h - img_h)\n",
    "    pad_bottom = pad_top + img_h\n",
    "    pad_left = randint(0, frame_w - img_w)\n",
    "    pad_right = pad_left + img_w\n",
    "\n",
    "    res[pad_top:pad_bottom, pad_left:pad_right] = img\n",
    "    return res\n",
    "\n",
    "@dataclass\n",
    "class ImageTransforms:\n",
    "    max_img_size: Tuple[int, int]  # (h, w)\n",
    "    normalize_params: Tuple[float, float]  # (mean, std)\n",
    "    scale: float = (\n",
    "        0.5\n",
    "    )\n",
    "    random_scale_limit: float = 0.1\n",
    "    random_rotate_limit: int = 10\n",
    "\n",
    "    train_trnsf: A.Compose = field(init=False)\n",
    "    test_trnsf: A.Compose = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        scale, random_scale_limit, random_rotate_limit, normalize_params =(\n",
    "            self.scale,\n",
    "            self.random_scale_limit,\n",
    "            self.random_rotate_limit,\n",
    "            self.normalize_params\n",
    "        )\n",
    "\n",
    "        max_img_h, max_img_w = self.max_img_size\n",
    "        max_scale = scale + scale * random_scale_limit\n",
    "        padded_h, padded_w = math.ceil(max_scale * max_img_h), math.ceil(max_scale * max_img_w)\n",
    "\n",
    "        self.train_trnsf = A.Compose([\n",
    "            A.Lambda(partial(adjust_dpi, scale=scale)),\n",
    "            SafeRandomScale(scale_limit=random_scale_limit, p=0.5),\n",
    "            A.SafeRotate(\n",
    "                limit = random_rotate_limit,\n",
    "                border_mode = cv.BORDER_CONSTANT,\n",
    "                value = 0\n",
    "            ),\n",
    "            A.RandomBrightnessContrast(),\n",
    "            A.Perspective(scale=(0.01, 0.05)),\n",
    "            A.GaussNoise(),\n",
    "            A.Normalize(*normalize_params),\n",
    "            A.Lambda(\n",
    "                image=partial(\n",
    "                    randomly_displace_and_pad,\n",
    "                    padded_size=(padded_h, padded_w),\n",
    "                    crop_if_necessary=False,\n",
    "                )\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.test_trnsf = A.Compose([\n",
    "            A.Lambda(partial(adjust_dpi, scale=scale)),\n",
    "            A.Normalize(*normalize_params),\n",
    "            A.PadIfNeeded(\n",
    "                max_img_h, max_img_w, border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "        ])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.286749Z",
     "start_time": "2024-04-17T18:00:09.276042Z"
    }
   },
   "id": "20ee595f0c62e83",
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IAM Dataset and Synthetic Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57dcf31d3faa61db"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class IAMDataset(Dataset):\n",
    "    MEAN = 0.8275\n",
    "    STD = 0.2314\n",
    "    MAX_FORM_HEIGHT = 3542\n",
    "    MAX_FORM_WIDTH = 2479\n",
    "\n",
    "    MAX_SEQ_LENS = {\n",
    "        \"word\": 55,\n",
    "        \"line\": 90,\n",
    "        \"form\": 700,\n",
    "    }  # based on the maximum seq lengths found in the dataset\n",
    "\n",
    "    _pad_token = \"<PAD>\"\n",
    "    _sos_token = \"<SOS>\"\n",
    "    _eos_token = \"<EOS>\"\n",
    "\n",
    "    root: Path\n",
    "    data: pd.DataFrame\n",
    "    label_enc: LabelParser\n",
    "    parse_method: str\n",
    "    only_lowercase: bool\n",
    "    transforms: Optional[A.Compose]\n",
    "    id_to_idx: Dict[str, int]\n",
    "    _split: str\n",
    "    _return_writer_id: Optional[bool]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: Union[Path, str],\n",
    "        parse_method: str,\n",
    "        split: str,\n",
    "        return_writer_id: bool = False,\n",
    "        only_lowercase: bool = False,\n",
    "        label_enc: Optional[LabelParser] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        _parse_methods = [\"form\", \"line\", \"word\"]\n",
    "        err_message = (\n",
    "            f\"{parse_method} is not a possible parsing method: {_parse_methods}\"\n",
    "        )\n",
    "        assert parse_method in _parse_methods, err_message\n",
    "\n",
    "        _splits = [\"train\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "\n",
    "        self._split = split\n",
    "        self._return_writer_id = return_writer_id\n",
    "        self.only_lowercase = only_lowercase\n",
    "        self.root = Path(root)\n",
    "        self.label_enc = label_enc\n",
    "        self.parse_method = parse_method\n",
    "\n",
    "        # Process the data.\n",
    "        if not hasattr(self, \"data\"):\n",
    "            if self.parse_method == \"form\":\n",
    "                self.data = self._get_forms()\n",
    "            elif self.parse_method == \"word\":\n",
    "                self.data = self._get_words(skip_bad_segmentation=True)\n",
    "            elif self.parse_method == \"line\":\n",
    "                self.data = self._get_lines()\n",
    "\n",
    "        # Create the label encoder.\n",
    "        if self.label_enc is None:\n",
    "            vocab = [self._pad_token, self._sos_token, self._eos_token]\n",
    "            s = \"\".join(self.data[\"target\"].tolist())\n",
    "            if self.only_lowercase:\n",
    "                s = s.lower()\n",
    "            vocab += sorted(list(set(s)))\n",
    "            self.label_enc = LabelParser().fit(vocab)\n",
    "        if not \"target_enc\" in self.data.columns:\n",
    "            self.data.insert(\n",
    "                2,\n",
    "                \"target_enc\",\n",
    "                self.data[\"target\"].apply(\n",
    "                    lambda s: np.array(\n",
    "                        self.label_enc.encode_labels(\n",
    "                            [c for c in (s.lower() if self.only_lowercase else s)]\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.transforms = self._get_transforms(split)\n",
    "        self.id_to_idx = {\n",
    "            Path(self.data.iloc[i][\"img_path\"]).stem: i for i in range(len(self))\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        img = cv.imread(data[\"img_path\"], cv.IMREAD_GRAYSCALE)\n",
    "        if all(col in data.keys() for col in [\"bb_y_start\", \"bb_y_end\"]):\n",
    "            # Crop the image vertically.\n",
    "            img = img[data[\"bb_y_start\"] : data[\"bb_y_end\"], :]\n",
    "        assert isinstance(img, np.ndarray), (\n",
    "            f\"Error: image at path {data['img_path']} is not properly loaded. \"\n",
    "            f\"Is there something wrong with this image?\"\n",
    "        )\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        if self._return_writer_id:\n",
    "            return img, data[\"writer_id\"], data[\"target_enc\"]\n",
    "        return img, data[\"target_enc\"]\n",
    "\n",
    "    def get_max_height(self):\n",
    "        return (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n",
    "\n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.label_enc.classes\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(\n",
    "        batch: Sequence[Tuple[np.ndarray, np.ndarray]],\n",
    "        pad_val: int,\n",
    "        eos_tkn_idx: int,\n",
    "        dataset_returns_writer_id: bool = False,\n",
    "    ) -> Union[Tuple[Tensor, Tensor], Tuple[Tensor, Tensor, Tensor]]:\n",
    "        if dataset_returns_writer_id:\n",
    "            imgs, writer_ids, targets = zip(*batch)\n",
    "        else:\n",
    "            imgs, targets = zip(*batch)\n",
    "\n",
    "        img_sizes = [im.shape for im in imgs]\n",
    "        if (\n",
    "            not len(set(img_sizes)) == 1\n",
    "        ):  # images are of varying sizes, so pad them to the maximum size in the batch\n",
    "            hs, ws = zip(*img_sizes)\n",
    "            pad_fn = A.PadIfNeeded(\n",
    "                max(hs), max(ws), border_mode=cv.BORDER_CONSTANT, value=0\n",
    "            )\n",
    "            imgs = [pad_fn(image=im)[\"image\"] for im in imgs]\n",
    "        imgs = np.stack(imgs, axis=0)\n",
    "\n",
    "        seq_lengths = [t.shape[0] for t in targets]\n",
    "        targets_padded = np.full((len(targets), max(seq_lengths) + 1), pad_val)\n",
    "        for i, t in enumerate(targets):\n",
    "            targets_padded[i, : seq_lengths[i]] = t\n",
    "            targets_padded[i, seq_lengths[i]] = eos_tkn_idx\n",
    "\n",
    "        imgs, targets_padded = torch.tensor(imgs), torch.tensor(targets_padded)\n",
    "        if dataset_returns_writer_id:\n",
    "            return imgs, targets_padded, torch.tensor(writer_ids)\n",
    "        return imgs, targets_padded\n",
    "\n",
    "    def set_transforms_for_split(self, split: str):\n",
    "        _splits = [\"train\", \"val\", \"test\"]\n",
    "        err_message = f\"{split} is not a possible split: {_splits}\"\n",
    "        assert split in _splits, err_message\n",
    "        self.transforms = self._get_transforms(split)\n",
    "\n",
    "    def _get_transforms(self, split: str) -> A.Compose:\n",
    "        max_img_w = self.MAX_FORM_WIDTH\n",
    "\n",
    "        if self.parse_method == \"form\":\n",
    "            max_img_h = (self.data[\"bb_y_end\"] - self.data[\"bb_y_start\"]).max()\n",
    "        else:  # word or line\n",
    "            max_img_h = self.MAX_FORM_HEIGHT\n",
    "\n",
    "        transforms = ImageTransforms(\n",
    "            (max_img_h, max_img_w), (IAMDataset.MEAN, IAMDataset.STD)\n",
    "        )\n",
    "\n",
    "        if split == \"train\":\n",
    "            return transforms.train_trnsf\n",
    "        elif split == \"test\" or split == \"val\":\n",
    "            return transforms.test_trnsf\n",
    "\n",
    "    def statistics(self) -> Dict[str, float]:\n",
    "        assert len(self) > 0\n",
    "        tmp = self.transforms\n",
    "        self.transforms = None\n",
    "        mean, std, cnt = 0, 0, 0\n",
    "        for img, _ in self:\n",
    "            mean += np.mean(img)\n",
    "            std += np.var(img)\n",
    "            cnt += 1\n",
    "        mean /= cnt\n",
    "        std = np.sqrt(std / cnt)\n",
    "        self.transforms = tmp\n",
    "        return {\"mean\": mean, \"std\": std}\n",
    "\n",
    "    def _get_forms(self) -> pd.DataFrame:\n",
    "        \"\"\"Read all form images from the IAM dataset.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame\n",
    "                A pandas dataframe containing the image path, image id, target, vertical\n",
    "                upper bound, vertical lower bound, and target length.\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"img_path\": [],\n",
    "            \"img_id\": [],\n",
    "            \"target\": [],\n",
    "            \"bb_y_start\": [],\n",
    "            \"bb_y_end\": [],\n",
    "            \"target_len\": [],\n",
    "        }\n",
    "        for form_dir in [\"formsA-D\", \"formsE-H\", \"formsI-Z\"]:\n",
    "            dr = self.root / form_dir\n",
    "            for img_path in dr.iterdir():\n",
    "                doc_id = img_path.stem\n",
    "                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n",
    "\n",
    "                # Based on some empiricial evaluation, the 'asy' and 'dsy'\n",
    "                # attributes of a line xml tag seem to correspond to its upper and\n",
    "                # lower bound, respectively. We add padding of 150 pixels.\n",
    "                bb_y_start = int(xml_root[1][0].get(\"asy\")) - 150\n",
    "                bb_y_end = int(xml_root[1][-1].get(\"dsy\")) + 150\n",
    "\n",
    "                form_text = []\n",
    "                for line in xml_root.iter(\"line\"):\n",
    "                    form_text.append(html.unescape(line.get(\"text\", \"\")))\n",
    "\n",
    "                img_w, img_h = Image.open(str(img_path)).size\n",
    "                data[\"img_path\"].append(str(img_path))\n",
    "                data[\"img_id\"].append(doc_id)\n",
    "                data[\"target\"].append(\"\\n\".join(form_text))\n",
    "                data[\"bb_y_start\"].append(bb_y_start)\n",
    "                data[\"bb_y_end\"].append(bb_y_end)\n",
    "                data[\"target_len\"].append(len(\"\\n\".join(form_text)))\n",
    "        return pd.DataFrame(data).sort_values(\n",
    "            \"target_len\"\n",
    "        )  # by default, sort by target length\n",
    "\n",
    "    def _get_lines(self, skip_bad_segmentation: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Read all line images from the IAM dataset.\n",
    "\n",
    "        Args:\n",
    "            skip_bad_segmentation (bool): skip lines that have the\n",
    "                segmentation='err' xml attribute\n",
    "        Returns:\n",
    "            List of 2-tuples, where each tuple contains the path to a line image\n",
    "            along with its ground truth text.\n",
    "        \"\"\"\n",
    "        data = {\"img_path\": [], \"img_id\": [], \"target\": []}\n",
    "        root = self.root / \"lines\"\n",
    "        for d1 in root.iterdir():\n",
    "            for d2 in d1.iterdir():\n",
    "                doc_id = d2.name\n",
    "                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n",
    "                for img_path in d2.iterdir():\n",
    "                    target = self._find_line(\n",
    "                        xml_root, img_path.stem, skip_bad_segmentation\n",
    "                    )\n",
    "                    if target is not None:\n",
    "                        data[\"img_path\"].append(str(img_path.resolve()))\n",
    "                        data[\"img_id\"].append(doc_id)\n",
    "                        data[\"target\"].append(target)\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def _get_words(self, skip_bad_segmentation: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Read all word images from the IAM dataset.\n",
    "\n",
    "        Args:\n",
    "            skip_bad_segmentation (bool): skip lines that have the\n",
    "                segmentation='err' xml attribute\n",
    "        Returns:\n",
    "            List of 2-tuples, where each tuple contains the path to a word image\n",
    "            along with its ground truth text.\n",
    "        \"\"\"\n",
    "        data = {\"img_path\": [], \"img_id\": [], \"writer_id\": [], \"target\": []}\n",
    "        root = self.root / \"words\"\n",
    "        for d1 in root.iterdir():\n",
    "            if d1.is_file():\n",
    "                continue\n",
    "\n",
    "            for d2 in d1.iterdir():\n",
    "\n",
    "                doc_id = d2.name\n",
    "                xml_root = read_xml(self.root / \"xml\" / (doc_id + \".xml\"))\n",
    "                writer_id = int(xml_root.get(\"writer-id\"))\n",
    "                for img_path in d2.iterdir():\n",
    "                    img = cv.imread(str(img_path.resolve()), cv.IMREAD_GRAYSCALE)\n",
    "                    if isinstance(img, np.ndarray):\n",
    "                        target = self._find_word(\n",
    "                            xml_root, img_path.stem, skip_bad_segmentation\n",
    "                        )\n",
    "                        if target is not None:\n",
    "                            data[\"img_path\"].append(str(img_path.resolve()))\n",
    "                            data[\"img_id\"].append(doc_id)\n",
    "                            data[\"writer_id\"].append(writer_id)\n",
    "                            data[\"target\"].append(target)\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def _find_line(\n",
    "        self,\n",
    "        xml_root: ET.Element,\n",
    "        line_id: str,\n",
    "        skip_bad_segmentation: bool = False,\n",
    "    ) -> Union[str, None]:\n",
    "        line = find_child_by_tag(xml_root[1].findall(\"line\"), \"id\", line_id)\n",
    "        if line is not None and not (\n",
    "            skip_bad_segmentation and line.get(\"segmentation\") == \"err\"\n",
    "        ):\n",
    "            return html.unescape(line.get(\"text\"))\n",
    "        return None\n",
    "\n",
    "    def _find_word(\n",
    "        self,\n",
    "        xml_root: ET.Element,\n",
    "        word_id: str,\n",
    "        skip_bad_segmentation: bool = False,\n",
    "    ) -> Union[str, None]:\n",
    "        line_id = \"-\".join(word_id.split(\"-\")[:-1])\n",
    "        line = find_child_by_tag(xml_root[1].findall(\"line\"), \"id\", line_id)\n",
    "        if line is not None and not (\n",
    "            skip_bad_segmentation and line.get(\"segmentation\") == \"err\"\n",
    "        ):\n",
    "            word = find_child_by_tag(line.findall(\"word\"), \"id\", word_id)\n",
    "            if word is not None:\n",
    "                return html.unescape(word.get(\"text\"))\n",
    "        return None\n",
    "\n",
    "class IAMSyntheticDataGenerator(Dataset):\n",
    "    \"\"\"\n",
    "    Data generator that creates synthetic line/form images by stitching together word\n",
    "    images from the IAM dataset.\n",
    "    Calling `__getitem__()` samples a newly generated synthetic image every time\n",
    "    it is called.\n",
    "    \"\"\"\n",
    "\n",
    "    PUNCTUATION = [\",\", \".\", \";\", \":\", \"'\", '\"', \"!\", \"?\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        iam_root: Union[str, Path],\n",
    "        label_encoder: Optional[LabelParser] = None,\n",
    "        transforms: Optional[A.Compose] = None,\n",
    "        line_width: Tuple[int, int] = (1500, 2000),\n",
    "        lines_per_form: Tuple[int, int] = (1, 11),\n",
    "        words_per_line: Tuple[int, int] = (4, 10),\n",
    "        words_per_sequence: Tuple[int, int] = (7, 13),\n",
    "        px_between_lines: Tuple[int, int] = (25, 50),\n",
    "        px_between_words: int = 50,\n",
    "        px_around_image: Tuple[int, int] = (100, 200),\n",
    "        sample_form: bool = False,\n",
    "        only_lowercase: bool = False,\n",
    "        rng_seed: int = 0,\n",
    "        max_height: Optional[int] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.iam_root = iam_root\n",
    "        self.label_enc = label_encoder\n",
    "        self.transforms = transforms\n",
    "        self.line_width = line_width\n",
    "        self.lines_per_form = lines_per_form\n",
    "        self.words_per_line = words_per_line\n",
    "        self.words_per_sequence = words_per_sequence\n",
    "        self.px_between_lines = px_between_lines\n",
    "        self.px_between_words = px_between_words\n",
    "        self.px_around_image = px_around_image\n",
    "        self.sample_form = sample_form\n",
    "        self.only_lowercase = only_lowercase\n",
    "        self.rng_seed = rng_seed\n",
    "        self.max_height = max_height\n",
    "\n",
    "        self.iam_words = IAMDataset(\n",
    "            iam_root,\n",
    "            \"word\",\n",
    "            \"test\",\n",
    "            only_lowercase=only_lowercase,\n",
    "        )\n",
    "        if self.max_height is None:\n",
    "            self.max_height = IAMDataset.MAX_FORM_HEIGHT\n",
    "        if sample_form and \"\\n\" not in self.label_encoder.classes:\n",
    "            # Add the `\\n` token to the label encoder (since forms can contain newlines)\n",
    "            self.label_encoder.addClasses([\"\\n\"])\n",
    "        self.iam_words.transforms = None\n",
    "        self.rng = np.random.default_rng(rng_seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        # This dataset does not have a finite length since it can generate random\n",
    "        # images at will, so return 1.\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def label_encoder(self):\n",
    "        if self.label_enc is not None:\n",
    "            return self.label_enc\n",
    "        return self.iam_words.label_enc\n",
    "\n",
    "    def __getitem__(self, *args, **kwargs):\n",
    "        \"\"\"By calling this method, a newly generated synthetic image is sampled.\"\"\"\n",
    "        if self.sample_form:\n",
    "            img, target = self.generate_form()\n",
    "        else:\n",
    "            img, target = self.generate_line()\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "        # Encode the target sequence using the label encoder.\n",
    "        target_enc = np.array(self.label_encoder.encode_labels([c for c in target]))\n",
    "        return img, target_enc\n",
    "\n",
    "    def generate_line(self) -> Tuple[np.ndarray, str]:\n",
    "        words_to_sample = self.rng.integers(*self.words_per_line)\n",
    "        line_width = self.rng.integers(*self.line_width)\n",
    "        return self.sample_lines(words_to_sample, line_width, sample_one_line=True)\n",
    "\n",
    "    def generate_form(self) -> Tuple[np.ndarray, str]:\n",
    "        # Randomly pick the number of words and inter-line distance in the form.\n",
    "        words_to_sample = self.rng.integers(*self.lines_per_form) * 5  # 7 is handpicked\n",
    "        px_between_lines = self.rng.integers(*self.px_between_lines)\n",
    "\n",
    "        # Sample line images.\n",
    "        line_width = self.rng.integers(*self.line_width)\n",
    "        lines, target = self.sample_lines(words_to_sample, line_width)\n",
    "\n",
    "        # Concatenate the lines vertically.\n",
    "        form_w = max(l.shape[1] for l in lines)\n",
    "        form_h = sum(l.shape[0] + px_between_lines for l in lines)\n",
    "        if form_h > self.max_height:\n",
    "            print(\n",
    "                \"Generated form height exceeds maximum height. Generating a new form.\"\n",
    "            )\n",
    "            return self.generate_form()\n",
    "        form = np.ones((form_h, form_w), dtype=lines[0].dtype) * 255\n",
    "        curr_h = 0\n",
    "        for line_img in lines:\n",
    "            h, w = line_img.shape\n",
    "            if curr_h + h + px_between_lines > self.max_height:\n",
    "                break\n",
    "\n",
    "            form[curr_h : curr_h + h, :w] = line_img\n",
    "            curr_h += h + px_between_lines\n",
    "\n",
    "        # Add a random amount of padding around the image.\n",
    "        pad_px = self.rng.integers(*self.px_around_image)\n",
    "        new_h, new_w = form.shape[0] + pad_px * 2, form.shape[1] + pad_px * 2\n",
    "        form = A.PadIfNeeded(\n",
    "            new_h, new_w, border_mode=cv.BORDER_CONSTANT, value=255, always_apply=True\n",
    "        )(image=form)[\"image\"]\n",
    "\n",
    "        return form, target\n",
    "\n",
    "    def set_rng(self, seed: int):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def sample_word_image(self) -> Tuple[np.ndarray, str]:\n",
    "        idx = random.randint(0, len(self.iam_words) - 1)\n",
    "        img, target = self.iam_words[idx]\n",
    "        target = \"\".join(self.iam_words.label_enc.decode_labels(target))\n",
    "        return img, target\n",
    "\n",
    "    def sample_word_image_sequence(\n",
    "        self, words_to_sample: int\n",
    "    ) -> List[Tuple[np.ndarray, str]]:\n",
    "        \"\"\"Sample a sequence of contiguous words.\"\"\"\n",
    "        assert words_to_sample >= 1\n",
    "        start_idx = random.randint(0, len(self.iam_words) - 1)\n",
    "\n",
    "        img_idxs = [start_idx]\n",
    "        img_path = Path(self.iam_words.data.iloc[start_idx][\"img_path\"])\n",
    "        _, _, line_id, word_id = img_path.stem.split(\"-\")\n",
    "        sampled_words = 1\n",
    "        while sampled_words < words_to_sample:\n",
    "            word_id = f\"{int(word_id) + 1 :02}\"\n",
    "            img_name = (\n",
    "                \"-\".join(img_path.stem.split(\"-\")[:-2] + [line_id, word_id]) + \".png\"\n",
    "            )\n",
    "            if not (img_path.parent / img_name).is_file():\n",
    "                # Previous image was the last on its line. Go to the next line.\n",
    "                line_id = f\"{int(line_id) + 1 :02}\"\n",
    "                word_id = \"00\"\n",
    "                img_name = (\n",
    "                    \"-\".join(img_path.stem.split(\"-\")[:-2] + [line_id, word_id])\n",
    "                    + \".png\"\n",
    "                )\n",
    "            if not (img_path.parent / img_name).is_file():\n",
    "                # End of the document.\n",
    "                return self.sample_word_image_sequence(words_to_sample)\n",
    "            # Find the dataset index for the sampled word.\n",
    "            ix = self.iam_words.id_to_idx.get(Path(img_name).stem)\n",
    "            if ix is None:\n",
    "                # If the image has segmentation=err attribute, it will\n",
    "                # not be in the dataset. In this case try again.\n",
    "                return self.sample_word_image_sequence(words_to_sample)\n",
    "            img_idxs.append(ix)\n",
    "            sampled_words += 1\n",
    "\n",
    "        imgs, targets = zip(*[self.iam_words[idx] for idx in img_idxs])\n",
    "        targets = [\n",
    "            \"\".join(self.iam_words.label_enc.decode_labels(t)) for t in targets\n",
    "        ]\n",
    "        return list(zip(imgs, targets))\n",
    "\n",
    "    def sample_lines(\n",
    "        self, words_to_sample: int, max_line_width: int, sample_one_line: bool = False\n",
    "    ) -> Tuple[Union[List[np.ndarray], np.ndarray], str]:\n",
    "        \"\"\"\n",
    "        Calls `sample_word_image_sequence` several times, using some heuristics\n",
    "        to glue the sequences together.\n",
    "\n",
    "        Returns:\n",
    "            - list of line images\n",
    "            - transcription for all lines combined\n",
    "        \"\"\"\n",
    "        curr_pos, sampled_words = 0, 0\n",
    "        imgs, targets, lines = [], [], []\n",
    "        target_str, last_target = \"\", \"\"\n",
    "\n",
    "        # Sample images.\n",
    "        while sampled_words < words_to_sample:\n",
    "            words_per_seq = self.rng.integers(*self.words_per_sequence)\n",
    "            # Sample a sequence of contiguous words.\n",
    "            img_tgt_seq = self.sample_word_image_sequence(words_per_seq)\n",
    "            for i, (img, tgt) in enumerate(img_tgt_seq):\n",
    "                # Add the sequence to the sampled words so far.\n",
    "                if sampled_words >= words_to_sample:\n",
    "                    break\n",
    "                h, w = img.shape\n",
    "\n",
    "                if curr_pos + w > max_line_width:\n",
    "                    # Concatenate the sampled images into a line.\n",
    "                    line = self.concatenate_line(imgs, targets, max_line_width)\n",
    "\n",
    "                    if sample_one_line:\n",
    "                        return line, target_str\n",
    "\n",
    "                    lines.append(line)\n",
    "                    target_str += \"\\n\"\n",
    "                    last_target = \"\\n\"\n",
    "                    curr_pos = 0\n",
    "                    imgs, targets = [], []\n",
    "\n",
    "                # Basic heuristics to avoid some strange looking sentences.\n",
    "                if i == 0 and (\n",
    "                    (last_target in self.PUNCTUATION and tgt in self.PUNCTUATION)\n",
    "                    or (tgt in self.PUNCTUATION and sampled_words == 0)\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                if (\n",
    "                    sampled_words == 0\n",
    "                    or tgt in [c for c in self.PUNCTUATION if c not in [\"'\", '\"']]\n",
    "                    or last_target == \"\\n\"\n",
    "                ):\n",
    "                    target_str += tgt\n",
    "                else:\n",
    "                    target_str += \" \" + tgt\n",
    "\n",
    "                targets.append(tgt)\n",
    "                imgs.append(img)\n",
    "\n",
    "                sampled_words += 1\n",
    "                last_target = tgt\n",
    "                if tgt in self.PUNCTUATION:\n",
    "                    # Reduce horizontal spacing for punctuation tokens.\n",
    "                    curr_pos = max(0, curr_pos - self.px_between_words)\n",
    "                curr_pos += w + self.px_between_words\n",
    "        if imgs and targets:\n",
    "            # Concatenate the remaining images into a new line.\n",
    "            line = self.concatenate_line(imgs, targets, max_line_width)\n",
    "            lines.append(line)\n",
    "            if sample_one_line:\n",
    "                return line, target_str\n",
    "        return lines, target_str\n",
    "\n",
    "    def concatenate_line(\n",
    "        self, imgs: List[np.ndarray], targets: List[str], line_width: int\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Concatenate a series of (img, target) tuples into a line to create a line image.\n",
    "        \"\"\"\n",
    "        assert len(imgs) == len(targets)\n",
    "\n",
    "        line_height = max(im.shape[0] for im in imgs)\n",
    "        line = np.ones((line_height, line_width), dtype=imgs[0].dtype) * 255\n",
    "\n",
    "        curr_pos = 0\n",
    "        prev_lower_bound = line_height\n",
    "        for img, tgt in zip(imgs, targets):\n",
    "            h, w = img.shape\n",
    "            # Center the image in the middle of the line.\n",
    "            start_h = min(max(0, int((line_height - h) / 2)), line_height - h)\n",
    "\n",
    "            if tgt in [\",\", \".\"]:\n",
    "                # If sampled a comma or dot, place them at the bottom of the line.\n",
    "                start_h = min(max(0, prev_lower_bound - int(h / 2)), line_height - h)\n",
    "            elif tgt in ['\"', \"'\"]:\n",
    "                # If sampled a quote, place them at the top of the line.\n",
    "                start_h = 0\n",
    "            if tgt in self.PUNCTUATION:\n",
    "                # Reduce horizontal spacing for punctuation tokens.\n",
    "                curr_pos = max(0, curr_pos - self.px_between_words)\n",
    "\n",
    "            assert curr_pos + w <= line_width, f\"{curr_pos + w} > {line_width}\"\n",
    "            assert start_h + h <= line_height, f\"{start_h + h} > {line_height}\"\n",
    "\n",
    "            # Concatenate the word image to the line.\n",
    "            line[start_h : start_h + h, curr_pos : curr_pos + w] = img\n",
    "\n",
    "            curr_pos += w + self.px_between_words\n",
    "            prev_lower_bound = start_h + h\n",
    "        return line\n",
    "\n",
    "    @staticmethod\n",
    "    def get_worker_init_fn():\n",
    "        def worker_init_fn(worker_id: int):\n",
    "            set_seed(worker_id)\n",
    "            worker_info = torch.utils.data.get_worker_info()\n",
    "            dataset = worker_info.dataset  # the dataset copy in this worker process\n",
    "            if hasattr(dataset, \"set_rng\"):\n",
    "                dataset.set_rng(worker_id)\n",
    "            else:  # dataset is instance of `IAMDatasetSynthetic` class\n",
    "                dataset.synth_dataset.set_rng(worker_id)\n",
    "\n",
    "        return worker_init_fn\n",
    "\n",
    "\n",
    "\n",
    "class IAMDatasetSynthetic(Dataset):\n",
    "    \"\"\"\n",
    "    A Pytorch dataset combining the IAM dataset with the IAMSyntheticDataGenerator\n",
    "    dataset.\n",
    "\n",
    "    The distribution of real/synthetic images can be controlled by setting the\n",
    "    `synth_prob` argument.\n",
    "    \"\"\"\n",
    "\n",
    "    iam_dataset: IAMDataset\n",
    "    synth_dataset: IAMSyntheticDataGenerator\n",
    "\n",
    "    def __init__(self, iam_dataset: IAMDataset, nr_of_samples: int, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            iam_dataset (Dataset): the IAM dataset to sample from\n",
    "            synth_prob (float): the probability of sampling a synthetic image when\n",
    "                calling `__getitem__()`.\n",
    "        \"\"\"\n",
    "        self.iam_dataset = iam_dataset\n",
    "        self.nr_of_samples = nr_of_samples\n",
    "        self.synth_dataset = IAMSyntheticDataGenerator(\n",
    "            iam_root=iam_dataset.root,\n",
    "            label_encoder=iam_dataset.label_enc,\n",
    "            transforms=iam_dataset.transforms,\n",
    "            sample_form=(True if iam_dataset.parse_method == \"form\" else False),\n",
    "            only_lowercase=iam_dataset.only_lowercase,\n",
    "            max_height=(\n",
    "                (iam_dataset.data[\"bb_y_end\"] - iam_dataset.data[\"bb_y_start\"]).max()\n",
    "                if iam_dataset.parse_method == \"form\"\n",
    "                else None\n",
    "            ),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        iam = self.iam_dataset\n",
    "        if len(self.iam_dataset) <= idx < self.nr_of_samples:\n",
    "            # Sample from the synthetic dataset.\n",
    "            img, target = self.synth_dataset[0]\n",
    "        else:\n",
    "            # Index the IAM dataset.\n",
    "            img, target = iam[idx]\n",
    "        assert not np.any(np.isnan(img)), img\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nr_of_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.341978Z",
     "start_time": "2024-04-17T18:00:09.291501Z"
    }
   },
   "id": "ad28e414623ed935",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9df553f544f369e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class CharacterErrorRate(Metric):\n",
    "\n",
    "    def __init__(self, label_encoder: LabelParser):\n",
    "        super().__init__()\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "        self.add_state(\"edits\", default=torch.Tensor([0]), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total_chars\", default=torch.Tensor([0]), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.ndim == target.ndim\n",
    "\n",
    "        eos_tkn_idx, sos_tkn_idx = list(\n",
    "            self.label_encoder.encode_labels([\"<EOS>\", \"<SOS>\"])\n",
    "        )\n",
    "\n",
    "        if (preds[:, 0] == sos_tkn_idx).all():  # this should normally be the case\n",
    "            preds = preds[:, 1:]\n",
    "\n",
    "        eos_idxs_prd = (preds == eos_tkn_idx).float().argmax(1).tolist()\n",
    "        eos_idxs_tgt = (target == eos_tkn_idx).float().argmax(1).tolist()\n",
    "\n",
    "        for i, (p, t) in enumerate(zip(preds, target)):\n",
    "            eos_idx_p, eos_idx_t = eos_idxs_prd[i], eos_idxs_tgt[i]\n",
    "            p = p[:eos_idx_p] if eos_idx_p else p\n",
    "            t = t[:eos_idx_t] if eos_idx_t else t\n",
    "            p_str, t_str = map(tensor_to_str, (p, t))\n",
    "            editd = editdistance.eval(p_str, t_str)\n",
    "\n",
    "            self.edits += editd\n",
    "            self.total_chars += t.numel()\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        return self.edits.float() / self.total_chars\n",
    "\n",
    "class WordErrorRate(Metric):\n",
    "    def __init__(self, label_encoder: LabelParser):\n",
    "        super().__init__()\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "        self.add_state(\"edits\", default=torch.Tensor([0]), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total_words\", default=torch.Tensor([0]), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.ndim == target.ndim\n",
    "\n",
    "        eos_tkn_idx, sos_tkn_idx = self.label_encoder.encode_labels([\"<EOS>\", \"<SOS>\"])\n",
    "\n",
    "        if (preds[:, 0] == sos_tkn_idx).all():\n",
    "            preds = preds[:, 1:]\n",
    "\n",
    "        eos_idxs_prd = (preds == eos_tkn_idx).float().argmax(1).tolist()\n",
    "        eos_idxs_tgt = (target == eos_tkn_idx).float().argmax(1).tolist()\n",
    "\n",
    "        for i, (p, t) in enumerate(zip(preds, target)):\n",
    "            eos_idx_p, eos_idx_t = eos_idxs_prd[i], eos_idxs_tgt[i]\n",
    "            p = (p[:eos_idx_p] if eos_idx_p else p).flatten().tolist()\n",
    "            t = (t[:eos_idx_t] if eos_idx_t else t).flatten().tolist()\n",
    "            p_words = \"\".join(self.label_encoder.decode_labels(p)).split()\n",
    "            t_words = \"\".join(self.label_encoder.decode_labels(t)).split()\n",
    "            editd = editdistance.eval(p_words, t_words)\n",
    "\n",
    "            self.edits += editd\n",
    "            self.total_words += len(t_words)\n",
    "\n",
    "    def compute(self) -> torch.Tensor:\n",
    "        \"\"\"Compute Word Error Rate.\"\"\"\n",
    "        return self.edits.float() / self.total_words\n",
    "\n",
    "def tensor_to_str(t: torch.Tensor) -> str:\n",
    "    return \"\".join(map(str, t.flatten().tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.353845Z",
     "start_time": "2024-04-17T18:00:09.343701Z"
    }
   },
   "id": "31810000d109013c",
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b217de73eb942e10"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class PosEmbedding1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements 1D sinusoidal embeddings.\n",
    "\n",
    "    Adapted from 'The Annotated Transformer', http://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros((max_len, d_model), requires_grad=False)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Add a 1D positional embedding to an input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): tensor of shape (B, T, d_model) to add positional\n",
    "                embedding to\n",
    "        \"\"\"\n",
    "        _, T, _ = x.shape\n",
    "        # assert T <= self.pe.size(0) \\\n",
    "        assert T <= self.pe.size(1), (\n",
    "            f\"Stored 1D positional embedding does not have enough dimensions for the current feature map. \"\n",
    "            f\"Currently max_len={self.pe.size(1)}, T={T}. Consider increasing max_len such that max_len >= T.\"\n",
    "        )\n",
    "        return x + self.pe[:, :T]\n",
    "\n",
    "\n",
    "\n",
    "class PosEmbedding2D(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe_x = torch.zeros((max_len, d_model // 2), requires_grad=False)\n",
    "        pe_y = torch.zeros((max_len, d_model // 2), requires_grad=False)\n",
    "\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(\n",
    "            -math.log(10000.0) * torch.arange(0, d_model // 2, 2) / d_model\n",
    "        )\n",
    "\n",
    "        pe_y[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe_y[:, 1::2] = torch.cos(pos * div_term)\n",
    "        pe_x[:, 0::2] = torch.sin(pos * div_term)\n",
    "        pe_x[:, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "        self.register_buffer(\"pe_x\", pe_x)\n",
    "        self.register_buffer(\"pe_y\", pe_y)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, w, h, _ = x.shape\n",
    "\n",
    "        pe_x_ = self.pe_x[:w, :].unsqueeze(1).expand(-1, h, -1)\n",
    "        pe_y_ = self.pe_y[:h, :].unsqueeze(0).expand(w, -1, -1)\n",
    "\n",
    "        pe = torch.cat([pe_y_, pe_x_], -1)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        return x + pe\n",
    "\n",
    "\n",
    "class encoderHTR(nn.Module):\n",
    "    def __init__(self, d_model: int, encoder_type: str, dropout=0.1, bias=True):\n",
    "        super().__init__()\n",
    "        assert encoder_type in [\"resnet18\", \"resnet34\", \"resnet50\"], \"Model not found\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.pos_embd = PosEmbedding2D(d_model)\n",
    "\n",
    "        resnet = getattr(models, encoder_type)(pretrained=False)\n",
    "\n",
    "        modules = list(resnet.children())\n",
    "        cnv_1 = modules[0]\n",
    "        cnv_1 = nn.Conv2d(\n",
    "            1,\n",
    "            cnv_1.out_channels,\n",
    "            cnv_1.kernel_size,\n",
    "            cnv_1.stride,\n",
    "            cnv_1.padding,\n",
    "            bias=cnv_1.bias\n",
    "        )\n",
    "        self.encoder = nn.Sequential(cnv_1, *modules[1:-2])\n",
    "        self.linear = nn.Conv2d(resnet.fc.in_features, d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        x = self.encoder(imgs.unsqueeze(1))\n",
    "        x = self.linear(x).transpose(1, 2).transpose(2, 3)\n",
    "        x = self.pos_embd(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.flatten(1, 2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class decoderHTR(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_length,\n",
    "                 max_seq_len,\n",
    "                 eos_tkn_idx,\n",
    "                 sos_tkn_idx,\n",
    "                 pad_tkn_idx,\n",
    "                 d_model,\n",
    "                 num_layers,\n",
    "                 nhead,\n",
    "                 dim_ffn,\n",
    "                 dropout,\n",
    "                 activation=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.vocab_length = vocab_length\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.eos_idx = eos_tkn_idx\n",
    "        self.sos_idx = sos_tkn_idx\n",
    "        self.pad_idx = pad_tkn_idx\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.nhead = nhead\n",
    "        self.dim_ffn = dim_ffn\n",
    "        self.pos_emb = PosEmbedding1D(d_model)\n",
    "        self.emb = nn.Embedding(vocab_length, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model,\n",
    "            nhead,\n",
    "            dim_ffn,\n",
    "            dropout,\n",
    "            activation,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.clf = nn.Linear(d_model, vocab_length)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, memory: torch.Tensor):\n",
    "        B, _, _ = memory.shape\n",
    "        all_logits = []\n",
    "        sampled_ids = [torch.full([B], self.sos_idx).to(memory.device)]\n",
    "        tgt = self.pos_emb(\n",
    "            self.emb(sampled_ids[0]).unsqueeze(1) * math.sqrt(self.d_model)\n",
    "        )\n",
    "        tgt = self.drop(tgt)\n",
    "        eos_sampled = torch.zeros(B).bool()\n",
    "\n",
    "        for t in range(self.max_seq_len):\n",
    "            tgt_mask = self.subsequent_mask(len(sampled_ids)).to(memory.device)\n",
    "            out = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n",
    "            logits = self.clf(out[:, -1, :])\n",
    "            _, pred = torch.max(logits, -1)\n",
    "            all_logits.append(logits)\n",
    "            sampled_ids.append(pred)\n",
    "            for i, pr in enumerate(pred):\n",
    "                if pr == self.eos_idx:\n",
    "                    eos_sampled[i] = True\n",
    "            if eos_sampled.all():\n",
    "                break\n",
    "\n",
    "            tgt_ext = self.drop(\n",
    "                self.pos_emb.pe[:, len(sampled_ids)]\n",
    "                + self.emb(pred) * math.sqrt(self.d_model)\n",
    "            ).unsqueeze(1)\n",
    "            tgt = torch.cat([tgt, tgt_ext], 1)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)\n",
    "        all_logits = torch.stack(all_logits, 1)\n",
    "\n",
    "        eos_idxs = (sampled_ids == self.eos_idx).float().argmax(1)\n",
    "        for i in range(B):\n",
    "            if eos_idxs[i] != 0:\n",
    "                sampled_ids[i, eos_idxs[i] + 1:] = self.pad_idx\n",
    "\n",
    "        return all_logits, sampled_ids\n",
    "\n",
    "    def forward_teacher_forcing(self, memory: torch.Tensor, tgt: torch.Tensor):\n",
    "        B, T = tgt.shape\n",
    "        tgt = torch.cat(\n",
    "            [\n",
    "                torch.full([B], self.sos_idx).unsqueeze(1).to(memory.device),\n",
    "                tgt[:, :-1]\n",
    "            ],\n",
    "            1\n",
    "        )\n",
    "\n",
    "        tgt_key_masking = tgt == self.pad_idx\n",
    "        tgt_mask = self.subsequent_mask(T).to(tgt.device)\n",
    "\n",
    "        tgt = self.pos_emb(self.emb(tgt) * math.sqrt(self.d_model))\n",
    "        tgt = self.drop(tgt)\n",
    "        out = self.decoder(\n",
    "            tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_key_masking\n",
    "        )\n",
    "        logits = self.clf(out)\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def subsequent_mask(size: int):\n",
    "        mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "        return mask == 1\n",
    "\n",
    "\n",
    "class FullPageHTR(nn.Module):\n",
    "    encoder: encoderHTR\n",
    "    decoder: decoderHTR\n",
    "    cer_metric: CharacterErrorRate\n",
    "    wer_metric: WordErrorRate\n",
    "    loss_fn: Callable\n",
    "    label_encoder: LabelParser\n",
    "\n",
    "    def __init__(self, label_encoder: LabelParser,\n",
    "                 max_seq_len=500,\n",
    "                 d_model=260,\n",
    "                 num_layers=6,\n",
    "                 nhead=4,\n",
    "                 dim_feedforward=1024,\n",
    "                 encoder_name=\"resnet18\",\n",
    "                 drop_enc=0.5,\n",
    "                 drop_dec=0.5,\n",
    "                 activ_dec=\"gelu\",\n",
    "                 label_smoothing=0.1,\n",
    "                 vocab_len: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.eos_token_idx, self.sos_token_idx, self.pad_token_idx = label_encoder.encode_labels(\n",
    "            [\"<EOS>\", \"<SOS>\", \"<PAD>\"]\n",
    "        )\n",
    "\n",
    "        self.encoder = encoderHTR(d_model, encoder_type=encoder_name, dropout=drop_enc)\n",
    "        self.decoder = decoderHTR(vocab_length=(vocab_len or len(label_encoder.classes)),\n",
    "                                  max_seq_len=max_seq_len,\n",
    "                                  eos_tkn_idx=self.eos_token_idx,\n",
    "                                  sos_tkn_idx=self.sos_token_idx,\n",
    "                                  pad_tkn_idx=self.pad_token_idx,\n",
    "                                  d_model=d_model,\n",
    "                                  num_layers=num_layers,\n",
    "                                  nhead=nhead,\n",
    "                                  dim_ffn=dim_feedforward,\n",
    "                                  dropout=drop_dec,\n",
    "                                  activation=activ_dec)\n",
    "        self.label_encoder = label_encoder\n",
    "        self.cer_metric = CharacterErrorRate(label_encoder)\n",
    "        self.wer_metric = WordErrorRate(label_encoder)\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=self.pad_token_idx,\n",
    "            label_smoothing=label_smoothing\n",
    "        )\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor, targets: Optional[torch.Tensor] = None):\n",
    "        logits, sampled_ids = self.decoder(self.encoder(imgs))\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = self.loss_fn(\n",
    "                logits[:, : targets.size(1), :].transpose(1, 2),\n",
    "                targets[:, : logits.size(1)],\n",
    "            )\n",
    "        return logits, sampled_ids, loss\n",
    "\n",
    "    def forward_teacher_forcing(self, imgs: torch.Tensor, targets: torch.Tensor):\n",
    "        memory = self.encoder(imgs)\n",
    "        logits = self.decoder.forward_teacher_forcing(memory, targets)\n",
    "        loss = self.loss_fn(logits.transpose(1, 2), targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def calculate_metrics(self, preds: torch.Tensor, targets: torch.Tensor):\n",
    "        self.cer_metric.reset()\n",
    "        self.wer_metric.reset()\n",
    "\n",
    "        cer = self.cer_metric(preds, targets)\n",
    "        wer = self.wer_metric(preds, targets)\n",
    "        return {\"CER\": cer, \"WER\": wer}\n",
    "\n",
    "    def set_num_output_classes(self, n_classes: int):\n",
    "        old_vocab_len = self.decoder.vocab_length\n",
    "        self.decoder.vocab_length = n_classes\n",
    "        self.decoder.clf = nn.Linear(self.decoder.d_model, n_classes)\n",
    "\n",
    "        new_embs = nn.Embedding(n_classes, self.decoder.d_model)\n",
    "        with torch.no_grad():\n",
    "            new_embs.weight[:old_vocab_len] = self.decoder.emb.weight\n",
    "            self.decoder.emb = new_embs\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.379441Z",
     "start_time": "2024-04-17T18:00:09.354931Z"
    }
   },
   "id": "1f202b947323068f",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1e574a335cd03a24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Trainer Class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "519d65cf8c1f839e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "class ModelTrainer:\n",
    "\n",
    "    def __init__(self, run_name: str,\n",
    "                 model: FullPageHTR,\n",
    "                 ds_name: str,\n",
    "                 train_data: DataLoader,\n",
    "                 val_data: DataLoader,\n",
    "                 optimizer: Optimizer,\n",
    "                 num_epochs: int,\n",
    "                 device: torch.device,\n",
    "                 normalization_steps: int):\n",
    "\n",
    "        self.normalization_steps = normalization_steps\n",
    "\n",
    "        self.model = model\n",
    "        self.train_data, self.val_data = train_data, val_data\n",
    "        self.num_epochs = num_epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.ds_name = ds_name\n",
    "        self.run_name = run_name\n",
    "        self.device = device\n",
    "\n",
    "    def _init_wandb(self):\n",
    "\n",
    "        wandb.init(project=\"fullpage-htr-base\",\n",
    "                   config={\n",
    "                       \"run_name\": self.run_name,\n",
    "                       \"learning_rate\": self.optimizer.param_groups[0][\"lr\"],\n",
    "                       \"epochs\": self.num_epochs,\n",
    "                       \"dataset\": self.ds_name\n",
    "                   })\n",
    "        wandb.define_metric('Train')\n",
    "        wandb.define_metric('Val')\n",
    "\n",
    "    def train_epoch_ga(self, epoch_nr, ds_size):\n",
    "      self.model.train()\n",
    "      total_loss = 0.0\n",
    "      total_cer = 0.0\n",
    "      total_wer = 0.0\n",
    "\n",
    "      nr_batches = 0\n",
    "      b_loss = 0.0\n",
    "      b_cer = 0.0\n",
    "      b_wer = 0.0\n",
    "      for idx, batch in enumerate(tqdm(self.train_data)):\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        outputs, loss = self.model.forward_teacher_forcing(inputs, labels)\n",
    "        loss = loss / (self.normalization_steps * labels.size(0))\n",
    "        loss.backward()\n",
    "        b_loss += loss.item()\n",
    "\n",
    "        _, preds = outputs.max(-1)\n",
    "        res = self.model.calculate_metrics(preds, labels)\n",
    "\n",
    "        b_cer += res[\"CER\"] / (self.normalization_steps * labels.size(0))\n",
    "        b_wer += res[\"WER\"] / (self.normalization_steps * labels.size(0))\n",
    "\n",
    "        if idx > 0 and (idx % self.normalization_steps == 0 or idx + 1 == len(self.train_data)):\n",
    "\n",
    "          self.optimizer.step()\n",
    "          self.optimizer.zero_grad()\n",
    "          if self.wanda:\n",
    "            wandb.log({\n",
    "              'Train Loss': b_loss ,\n",
    "              'Train CER' : b_cer ,\n",
    "              'Train WER' : b_wer ,\n",
    "              'Train': idx + ds_size * epoch_nr\n",
    "            })\n",
    "\n",
    "          total_loss += b_loss\n",
    "          total_cer  += b_cer\n",
    "          total_wer  += b_wer\n",
    "          b_cer = 0.0\n",
    "          b_wer = 0.0\n",
    "          b_loss = 0.0\n",
    "\n",
    "      total_loss /= (ds_size // self.normalization_steps)\n",
    "      total_cer  /= (ds_size // self.normalization_steps)\n",
    "      total_wer  /= (ds_size // self.normalization_steps)\n",
    "\n",
    "      return total_loss, total_cer, total_wer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def train_epoch(self, epoch_nr, ds_size):\n",
    "      self.model.train()\n",
    "      total_loss = 0.0\n",
    "      total_cer = 0.0\n",
    "      total_wer = 0.0\n",
    "\n",
    "      nr_batches = 0\n",
    "      b_cer = 0.0\n",
    "      b_wer = 0.0\n",
    "      for i, mb in enumerate(tqdm(self.train_data)):\n",
    "\n",
    "        inputs, labels = mb\n",
    "        inputs = inputs.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "\n",
    "        output_logits, loss = self.model.forward_teacher_forcing(inputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        _, preds = output_logits.max(-1)\n",
    "        res = self.model.calculate_metrics(preds, labels)\n",
    "\n",
    "\n",
    "        b_cer = res[\"CER\"]\n",
    "        b_wer = res[\"WER\"]\n",
    "        if self.wanda:\n",
    "          wandb.log({\n",
    "              'Train Loss': loss.item() / labels.size(0),\n",
    "              'Train CER' : b_cer / labels.size(0),\n",
    "              'Train WER' : b_wer / labels.size(0),\n",
    "              'Train': i + ds_size * epoch_nr\n",
    "            })\n",
    "\n",
    "        total_loss += loss.item() / labels.size(0)\n",
    "        total_cer += b_cer / labels.size(0)\n",
    "        total_wer += b_wer / labels.size(0)\n",
    "\n",
    "\n",
    "\n",
    "      total_loss /= ds_size\n",
    "      total_cer /= ds_size\n",
    "      total_wer /= ds_size\n",
    "\n",
    "      return total_loss, total_cer, total_wer\n",
    "\n",
    "    def val_epoch(self, epoch_nr, ds_size):\n",
    "      self.model.eval()\n",
    "      total_loss = 0.0\n",
    "      total_cer = 0.0\n",
    "      total_wer = 0.0\n",
    "      nr_batches = 0\n",
    "      b_loss = 0.0\n",
    "      b_cer = 0.0\n",
    "      b_wer = 0.0\n",
    "      with torch.no_grad():\n",
    "        for i, mb in enumerate(tqdm(self.val_data)):\n",
    "\n",
    "          inputs, labels = mb\n",
    "          inputs = inputs.to(self.device)\n",
    "          labels = labels.to(self.device)\n",
    "          self.optimizer.zero_grad()\n",
    "\n",
    "          output_logits, _, loss = self.model.forward(inputs, labels)\n",
    "\n",
    "\n",
    "          b_loss = loss.item()\n",
    "          _, preds = output_logits.max(-1)\n",
    "          res = self.model.calculate_metrics(preds, labels)\n",
    "          b_cer = res[\"CER\"]\n",
    "          b_wer = res[\"WER\"]\n",
    "          if self.wanda:\n",
    "            wandb.log({\n",
    "                'Val Loss': b_loss / labels.size(0),\n",
    "                'Val CER' : b_cer / labels.size(0),\n",
    "                'Val WER' : b_wer / labels.size(0),\n",
    "                'Val' : i  + ds_size * epoch_nr})\n",
    "\n",
    "          total_loss += b_loss / labels.size(0)\n",
    "          total_cer += b_cer / labels.size(0)\n",
    "          total_wer += b_wer / labels.size(0)\n",
    "\n",
    "\n",
    "          torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        total_loss /= ds_size\n",
    "        total_cer /= ds_size\n",
    "        total_wer /= ds_size\n",
    "      return total_loss, total_cer, total_wer\n",
    "\n",
    "    def train(self, train_len, val_len, wanda=True):\n",
    "        self.wanda = wanda\n",
    "        if wanda:\n",
    "          self._init_wandb()\n",
    "        for i in range(self.num_epochs):\n",
    "            print(f'#.Epoch {i}')\n",
    "            torch.cuda.empty_cache()\n",
    "            train_loss, train_cer, train_wer = self.train_epoch_ga(i, train_len)\n",
    "            val_loss, val_cer, val_wer = self.val_epoch(i, val_len)\n",
    "            print(f\"Train Loss avg: {train_loss}, Train CER avg: {train_cer}, Train WER avg: {train_wer}\")\n",
    "            print(f\"Val Loss avg: {val_loss}, Val CER avg: {val_cer}, Val WER avg: {val_wer}\")\n",
    "        if wanda:\n",
    "          wandb.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:09.396645Z",
     "start_time": "2024-04-17T18:00:09.382271Z"
    }
   },
   "id": "f6be0a0954c27110",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "ds = IAMDataset(root=\"/Users/tefannastasa/BachelorsWorkspace/BachModels/BachModels/data/raw\", label_enc=None, parse_method=\"form\" ,split=\"train\")\n",
    "ds_train, ds_val = torch.utils.data.random_split(ds, [math.ceil(0.8 * len(ds)), math.floor(0.2 * len(ds))])\n",
    "\n",
    "ds_val.data = copy(ds)\n",
    "ds_val.data.set_transforms_for_split(\"val\")\n",
    "train_len = len(ds_train)\n",
    "val_len = len(ds_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:10.966306Z",
     "start_time": "2024-04-17T18:00:09.397368Z"
    }
   },
   "id": "5e76a0ecf4f4c0ed",
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 2\n",
    "pad_tkn_idx, eos_tkn_idx = ds.label_enc.encode_labels([\"<PAD>\", \"<EOS>\"])\n",
    "collate_fn = partial(\n",
    "        IAMDataset.collate_fn, pad_val=pad_tkn_idx, eos_tkn_idx=eos_tkn_idx\n",
    ")\n",
    "num_workers = 1\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=2 * batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_len //= batch_size\n",
    "val_len //= 2 * batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:10.971166Z",
     "start_time": "2024-04-17T18:00:10.967554Z"
    }
   },
   "id": "b49f69c5bf30b77d",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tefannastasa/BachelorsWorkspace/BachModels/BachModels/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/tefannastasa/BachelorsWorkspace/BachModels/BachModels/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[1;32m      6\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[0;32m----> 8\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mFullPageHTR\u001B[49m\u001B[43m(\u001B[49m\u001B[43mds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_enc\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0002\u001B[39m)\n\u001B[1;32m     10\u001B[0m trainer \u001B[38;5;241m=\u001B[39m ModelTrainer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTesting_run\u001B[39m\u001B[38;5;124m\"\u001B[39m, model, ds_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIAM_forms\u001B[39m\u001B[38;5;124m\"\u001B[39m , train_data\u001B[38;5;241m=\u001B[39mdl_train, val_data\u001B[38;5;241m=\u001B[39mdl_val, optimizer\u001B[38;5;241m=\u001B[39moptimizer, num_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, device\u001B[38;5;241m=\u001B[39mdevice, normalization_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m56\u001B[39m)\n",
      "File \u001B[0;32m~/BachelorsWorkspace/BachModels/BachModels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1152\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1148\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1149\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m   1150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[0;32m-> 1152\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/BachelorsWorkspace/BachModels/BachModels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    800\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    801\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 802\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    805\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    806\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    807\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    812\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    813\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/BachelorsWorkspace/BachModels/BachModels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    800\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    801\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 802\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    805\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    806\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    807\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    812\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    813\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/BachelorsWorkspace/BachModels/BachModels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:849\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    847\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, buf \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffers\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    848\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buf \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 849\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffers[key] \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbuf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[0;32m~/BachelorsWorkspace/BachModels/BachModels/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1150\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1147\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1148\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1149\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m-> 1150\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/BachelorsWorkspace/BachModels/BachModels/venv/lib/python3.9/site-packages/torch/cuda/__init__.py:293\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    291\u001B[0m     )\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 293\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    296\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    297\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "try:\n",
    "  device = \"cuda\"\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "\n",
    "  model = FullPageHTR(ds.label_enc).to(device)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
    "  trainer = ModelTrainer(\"Testing_run\", model, ds_name=\"IAM_forms\" , train_data=dl_train, val_data=dl_val, optimizer=optimizer, num_epochs=100, device=device, normalization_steps=56)\n",
    "\n",
    "  wandb.finish()\n",
    "  trainer.train(train_len, val_len, wanda=True)\n",
    "except RuntimeError:\n",
    "  del model\n",
    "  print(\"Error time!!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-17T18:00:35.444932Z",
     "start_time": "2024-04-17T18:00:34.988633Z"
    }
   },
   "id": "5c6e4dfbc92c413",
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
